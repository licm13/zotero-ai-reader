#!/usr/bin/env python3
"""
Zotero å…³é”®è¯åˆ†ç±»åˆ†æå·¥å…·
============================

ä»Zoteroåº“ä¸­æå–æ‰€æœ‰æ–‡çŒ®çš„AIç¬”è®°å…³é”®è¯ï¼Œè¿›è¡Œæ™ºèƒ½èšç±»å’Œåˆ†ç±»åˆ†æã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- è‡ªåŠ¨æ£€ç´¢æ‰€æœ‰æ–‡çŒ®çš„"AI æ·±åº¦é˜…è¯»æŠ¥å‘Š"ç¬”è®°
- æå–Keywordså…³é”®è¯éƒ¨åˆ†
- å…³é”®è¯å½’ä¸€åŒ–ï¼ˆå»é‡ã€åŒä¹‰è¯è¯†åˆ«ã€ç¼©å†™å±•å¼€ï¼‰
- åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…±ç°å…³ç³»çš„æ™ºèƒ½èšç±»
- æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»ï¼ˆä¸€ä¸ªå…³é”®è¯å¯å±äºå¤šä¸ªç±»ç›®ï¼‰
- ç”Ÿæˆå±‚æ¬¡åŒ–åˆ†ç±»ç»“æ„å’Œç»Ÿè®¡æŠ¥å‘Š

ä½œè€…ï¼šProf. Chengming Li (SCUT)
"""

import os
import re
import json
import math
import time
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple
from pyzotero import zotero

try:
    import config
    LIBRARY_ID = config.LIBRARY_ID
    API_KEY = config.API_KEY
    LIBRARY_TYPE = config.LIBRARY_TYPE
    print("âœ… å·²ä» config.py åŠ è½½é…ç½®")
except ImportError:
    print("âš ï¸  æœªæ‰¾åˆ° config.py æ–‡ä»¶ï¼")
    print("ğŸ“‹ è¯·å¤åˆ¶ config.example.py ä¸º config.py å¹¶å¡«å…¥æ‚¨çš„é…ç½®ä¿¡æ¯")
    exit(1)

# ================= é…ç½®å‚æ•° =================

NOTE_TITLE = "AI æ·±åº¦é˜…è¯»æŠ¥å‘Š"  # ç›®æ ‡ç¬”è®°æ ‡é¢˜
NOTE_TITLE_KEYWORDS = ["AI", "æ·±åº¦é˜…è¯»", "é˜…è¯»æŠ¥å‘Š"]  # ç”¨äºæ¨¡ç³ŠåŒ¹é…çš„å…³é”®è¯
OUTPUT_DIR = "keyword_analysis"  # è¾“å‡ºç›®å½•
OUTPUT_JSON = os.path.join(OUTPUT_DIR, "keyword_categories.json")  # JSONè¾“å‡º
OUTPUT_REPORT = os.path.join(OUTPUT_DIR, "keyword_report.txt")  # æ–‡æœ¬æŠ¥å‘Š
OUTPUT_STATS = os.path.join(OUTPUT_DIR, "keyword_statistics.json")  # ç»Ÿè®¡ä¿¡æ¯

# ç›¸ä¼¼åº¦é˜ˆå€¼
SEMANTIC_SIMILARITY_THRESHOLD = 0.6  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
STRING_SIMILARITY_THRESHOLD = 0.8    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
COOCCURRENCE_WEIGHT = 0.3             # å…±ç°å…³ç³»æƒé‡
MIN_CLUSTER_SIZE = 2                  # æœ€å°èšç±»å¤§å°

# è°ƒè¯•æ¨¡å¼
DEBUG_MODE = True  # æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯

# ================= è¾…åŠ©å‡½æ•° =================

def normalize_keyword(keyword: str) -> str:
    """
    å…³é”®è¯å½’ä¸€åŒ–ï¼š
    - å»é™¤é¦–å°¾ç©ºæ ¼
    - å°å†™åŒ–
    - å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ï¼‰
    """
    # å»é™¤HTMLæ ‡ç­¾
    keyword = re.sub(r'<[^>]+>', '', keyword)
    # è½¬æ¢ä¸ºå°å†™
    keyword = keyword.lower().strip()
    # ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ã€ä¸­æ–‡
    keyword = re.sub(r'[^\w\s\-ä¸€-é¾¥]', '', keyword)
    # è§„èŒƒåŒ–ç©ºæ ¼
    keyword = re.sub(r'\s+', ' ', keyword).strip()
    return keyword

def split_keywords(keywords_text: str) -> List[str]:
    """
    ä»å…³é”®è¯æ–‡æœ¬ä¸­æå–å•ä¸ªå…³é”®è¯åˆ—è¡¨
    æ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€æ¢è¡Œã€ç«–çº¿ç­‰
    """
    # æ›¿æ¢å„ç§åˆ†éš”ç¬¦ä¸ºç»Ÿä¸€çš„åˆ†éš”ç¬¦
    text = re.sub(r'[ï¼Œï¼›ã€\n\r|]', ',', keywords_text)
    # æŒ‰é€—å·åˆ†å‰²
    keywords = [normalize_keyword(kw.strip()) for kw in text.split(',')]
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œå¤ªçŸ­çš„å…³é”®è¯
    keywords = [kw for kw in keywords if kw and len(kw) >= 2]
    return keywords

def extract_keywords_from_note(note_content: str) -> List[str]:
    """
    ä»ç¬”è®°å†…å®¹ä¸­æå–Keywordséƒ¨åˆ†
    """
    if not note_content:
        return []
    
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', note_content)
    
    # æŸ¥æ‰¾Keywordséƒ¨åˆ†ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼ŒåŒ…æ‹¬æ›´å®½æ¾çš„åŒ¹é…ï¼‰
    patterns = [
        r'(?:Keywords|å…³é”®è¯|è®ºæ–‡å…³é”®è¯|å…³é”®è¯ï¼š|Keywords:|Key\s*words)[ï¼š:\s]*\n?\s*(.+?)(?:\n\n|\n(?:Summary|æ€»ç»“|Abstract|æ‘˜è¦|è¦ç‚¹|æ ¸å¿ƒ|ä¸»è¦)|$)',
        r'(?:Keywords|å…³é”®è¯|è®ºæ–‡å…³é”®è¯)[ï¼š:\s]+(.+?)(?:\n\n|\n(?:Summary|æ€»ç»“|Abstract|æ‘˜è¦)|$)',
        r'(?:Keywords|å…³é”®è¯)[ï¼š:\s]*(.+?)(?:\n{2,}|$)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
        if match:
            keywords_text = match.group(1).strip()
            # å»é™¤å¯èƒ½çš„åç»­æ ‡é¢˜ï¼ˆå¦‚"Summary"ã€"Abstract"ç­‰ï¼‰
            keywords_text = re.split(r'\n(?:Summary|æ€»ç»“|Abstract|æ‘˜è¦|è¦ç‚¹|æ ¸å¿ƒ|ä¸»è¦|ç ”ç©¶|æ–¹æ³•)', keywords_text, flags=re.IGNORECASE)[0]
            keywords = split_keywords(keywords_text)
            if keywords:
                if DEBUG_MODE:
                    print(f"      âœ¨ æå–åˆ°å…³é”®è¯: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
                return keywords
    
    # å¦‚æœæ‰¾ä¸åˆ°æ˜ç¡®çš„å…³é”®è¯éƒ¨åˆ†ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«"å…³é”®è¯"çš„è¡Œ
    if "å…³é”®è¯" in text or "Keywords" in text or "keywords" in text.lower():
        lines = text.split('\n')
        for i, line in enumerate(lines):
            if re.search(r'(?:å…³é”®è¯|Keywords)', line, re.IGNORECASE):
                # å–è¯¥è¡ŒåŠåç»­å‡ è¡Œä½œä¸ºå…³é”®è¯
                keywords_text = '\n'.join(lines[i:i+3])
                keywords = split_keywords(keywords_text)
                if keywords:
                    if DEBUG_MODE:
                        print(f"      âœ¨ ä»è¡Œä¸­æå–åˆ°å…³é”®è¯: {', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")
                    return keywords
    
    if DEBUG_MODE:
        print(f"      âš ï¸  æœªæ‰¾åˆ°å…³é”®è¯éƒ¨åˆ†ï¼ˆå†…å®¹å‰100å­—ç¬¦: {text[:100]}...ï¼‰")
    
    return []

def levenshtein_distance(s1: str, s2: str) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç¼–è¾‘è·ç¦»ï¼ˆLevenshteinè·ç¦»ï¼‰
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def string_similarity(s1: str, s2: str) -> float:
    """
    è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰
    è¿”å›å€¼ï¼š0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ
    """
    if not s1 or not s2:
        return 0.0
    if s1 == s2:
        return 1.0
    
    max_len = max(len(s1), len(s2))
    if max_len == 0:
        return 1.0
    
    distance = levenshtein_distance(s1, s2)
    return 1.0 - (distance / max_len)

def jaccard_similarity(set1: Set, set2: Set) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªé›†åˆçš„Jaccardç›¸ä¼¼åº¦
    """
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    if union == 0:
        return 0.0
    return intersection / union

def tfidf_vectorize(keywords_list: List[List[str]]) -> Tuple[Dict[str, int], List[Dict[str, float]]]:
    """
    å°†å…³é”®è¯åˆ—è¡¨è½¬æ¢ä¸ºTF-IDFå‘é‡
    è¿”å›ï¼š(è¯æ±‡è¡¨å­—å…¸, TF-IDFå‘é‡åˆ—è¡¨)
    """
    # æ„å»ºè¯æ±‡è¡¨
    vocab = {}
    doc_freq = defaultdict(int)  # æ–‡æ¡£é¢‘ç‡ï¼ˆåŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°ï¼‰
    
    for doc_keywords in keywords_list:
        unique_keywords = set(doc_keywords)
        for kw in unique_keywords:
            if kw not in vocab:
                vocab[kw] = len(vocab)
            doc_freq[kw] += 1
    
    total_docs = len(keywords_list)
    vocab_size = len(vocab)
    
    # è®¡ç®—TF-IDFå‘é‡
    tfidf_vectors = []
    for doc_keywords in keywords_list:
        vector = {}
        kw_count = Counter(doc_keywords)
        max_count = max(kw_count.values()) if kw_count else 1
        
        for kw, count in kw_count.items():
            # TF (Term Frequency)
            tf = count / max_count
            # IDF (Inverse Document Frequency)
            idf = math.log(total_docs / (doc_freq[kw] + 1))
            # TF-IDF
            vector[kw] = tf * idf
        
        tfidf_vectors.append(vector)
    
    return vocab, tfidf_vectors

def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªTF-IDFå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    # è·å–æ‰€æœ‰å”¯ä¸€çš„é”®
    all_keys = set(vec1.keys()) | set(vec2.keys())
    
    if not all_keys:
        return 0.0
    
    dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in all_keys)
    norm1 = math.sqrt(sum(v * v for v in vec1.values()))
    norm2 = math.sqrt(sum(v * v for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def build_cooccurrence_matrix(keywords_list: List[List[str]]) -> Dict[Tuple[str, str], int]:
    """
    æ„å»ºå…³é”®è¯å…±ç°çŸ©é˜µ
    è¿”å›ï¼š{(keyword1, keyword2): count, ...}
    """
    cooccurrence = defaultdict(int)
    
    for doc_keywords in keywords_list:
        unique_keywords = list(set(doc_keywords))
        # æ¯å¯¹å…³é”®è¯åœ¨åŒä¸€æ–‡æ¡£ä¸­å‡ºç°ï¼Œå…±ç°æ¬¡æ•°+1
        for i, kw1 in enumerate(unique_keywords):
            for kw2 in unique_keywords[i+1:]:
                # ç¡®ä¿æœ‰åºï¼ˆé¿å…é‡å¤ï¼‰
                pair = tuple(sorted([kw1, kw2]))
                cooccurrence[pair] += 1
    
    return dict(cooccurrence)

def calculate_keyword_similarity(kw1: str, kw2: str, 
                                  vocab: Dict[str, int],
                                  tfidf_vectors: List[Dict[str, float]],
                                  keywords_list: List[List[str]],
                                  cooccurrence: Dict[Tuple[str, str], int]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå…³é”®è¯çš„ç»¼åˆç›¸ä¼¼åº¦
    ç»“åˆï¼šå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ã€TF-IDFè¯­ä¹‰ç›¸ä¼¼åº¦ã€å…±ç°å…³ç³»
    """
    # 1. å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    str_sim = string_similarity(kw1, kw2)
    
    # 2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºTF-IDFï¼‰
    # æ‰¾åˆ°åŒ…å«è¿™ä¸¤ä¸ªå…³é”®è¯çš„æ–‡æ¡£ï¼Œè®¡ç®—å®ƒä»¬çš„å‘é‡ç›¸ä¼¼åº¦
    semantic_sim = 0.0
    docs_with_kw1 = [i for i, doc in enumerate(keywords_list) if kw1 in doc]
    docs_with_kw2 = [i for i, doc in enumerate(keywords_list) if kw2 in doc]
    
    if docs_with_kw1 and docs_with_kw2:
        # è®¡ç®—åŒ…å«kw1å’Œkw2çš„æ–‡æ¡£å‘é‡çš„å¹³å‡ç›¸ä¼¼åº¦
        similarities = []
        for i in docs_with_kw1:
            for j in docs_with_kw2:
                sim = cosine_similarity(tfidf_vectors[i], tfidf_vectors[j])
                similarities.append(sim)
        
        if similarities:
            semantic_sim = sum(similarities) / len(similarities)
    
    # 3. å…±ç°å…³ç³»
    cooccurrence_sim = 0.0
    pair = tuple(sorted([kw1, kw2]))
    if pair in cooccurrence:
        # å½’ä¸€åŒ–å…±ç°æ¬¡æ•°ï¼ˆä½¿ç”¨å¯¹æ•°ç¼©æ”¾ï¼‰
        cooccurrence_sim = min(1.0, math.log(cooccurrence[pair] + 1) / math.log(10))
    
    # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æƒé‡æœ€é«˜ï¼ˆå¤„ç†ç¼©å†™å’Œæ‹¼å†™å˜ä½“ï¼‰
    # è¯­ä¹‰ç›¸ä¼¼åº¦æ¬¡ä¹‹ï¼ˆç†è§£è¯­ä¹‰å…³ç³»ï¼‰
    # å…±ç°å…³ç³»æƒé‡è¾ƒä½ï¼ˆä½œä¸ºè¡¥å……ï¼‰
    combined_sim = (0.5 * str_sim + 
                    0.3 * semantic_sim + 
                    COOCCURRENCE_WEIGHT * cooccurrence_sim)
    
    return combined_sim

def hierarchical_clustering(keywords_list: List[List[str]],
                            all_keywords: Set[str],
                            vocab: Dict[str, int],
                            tfidf_vectors: List[Dict[str, float]],
                            cooccurrence: Dict[Tuple[str, str], int]) -> Dict[str, List[str]]:
    """
    å±‚æ¬¡èšç±»ç®—æ³•ï¼šå°†ç›¸ä¼¼çš„å…³é”®è¯å½’ä¸ºä¸€ç±»
    
    è¿”å›ï¼š{category_name: [keyword1, keyword2, ...], ...}
    """
    # è®¡ç®—æ‰€æœ‰å…³é”®è¯å¯¹çš„ç›¸ä¼¼åº¦
    keyword_pairs = []
    keyword_list = list(all_keywords)
    
    print(f"   ğŸ”„ è®¡ç®— {len(keyword_list)} ä¸ªå…³é”®è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦...")
    for i, kw1 in enumerate(keyword_list):
        for kw2 in keyword_list[i+1:]:
            sim = calculate_keyword_similarity(kw1, kw2, vocab, tfidf_vectors, 
                                              keywords_list, cooccurrence)
            if sim >= SEMANTIC_SIMILARITY_THRESHOLD or string_similarity(kw1, kw2) >= STRING_SIMILARITY_THRESHOLD:
                keyword_pairs.append((sim, kw1, kw2))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    keyword_pairs.sort(reverse=True, key=lambda x: x[0])
    
    # èšç±»ï¼šä½¿ç”¨å¹¶æŸ¥é›†æ€æƒ³
    keyword_to_cluster = {kw: i for i, kw in enumerate(keyword_list)}
    clusters = {i: [kw] for i, kw in enumerate(keyword_list)}
    
    for sim, kw1, kw2 in keyword_pairs:
        cluster1 = keyword_to_cluster[kw1]
        cluster2 = keyword_to_cluster[kw2]
        
        if cluster1 != cluster2:
            # åˆå¹¶ä¸¤ä¸ªèšç±»
            # å°†cluster2ä¸­çš„æ‰€æœ‰å…³é”®è¯ç§»åˆ°cluster1
            for kw in clusters[cluster2]:
                keyword_to_cluster[kw] = cluster1
                clusters[cluster1].append(kw)
            del clusters[cluster2]
    
    # ç”Ÿæˆç±»åˆ«åç§°å’Œç»“æœ
    categories = {}
    for cluster_id, keywords in clusters.items():
        if len(keywords) >= MIN_CLUSTER_SIZE:
            # é€‰æ‹©æœ€é•¿çš„å…³é”®è¯ä½œä¸ºç±»åˆ«åç§°ï¼ˆæˆ–å‡ºç°é¢‘ç‡æœ€é«˜çš„ï¼‰
            category_name = max(keywords, key=lambda x: (len(x), keywords.count(x)))
            categories[category_name] = sorted(set(keywords))
    
    return categories

def assign_multi_category(keywords_list: List[List[str]],
                          categories: Dict[str, List[str]],
                          all_keywords: Set[str]) -> Dict[str, List[str]]:
    """
    å¤šæ ‡ç­¾åˆ†é…ï¼šå…è®¸ä¸€ä¸ªå…³é”®è¯å±äºå¤šä¸ªç±»åˆ«
    åŸºäºå…³é”®è¯åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„å…±ç°æ¨¡å¼
    """
    keyword_categories = defaultdict(set)
    
    # é¦–å…ˆï¼Œå°†å…³é”®è¯åˆ†é…åˆ°å®ƒä»¬æ‰€åœ¨çš„èšç±»ç±»åˆ«
    for category, keywords in categories.items():
        for kw in keywords:
            keyword_categories[kw].add(category)
    
    # ç„¶åï¼ŒåŸºäºå…±ç°å…³ç³»ï¼Œæ‰©å±•å¤šæ ‡ç­¾
    # å¦‚æœä¸€ä¸ªå…³é”®è¯ç»å¸¸ä¸æŸä¸ªç±»åˆ«çš„å…¶ä»–å…³é”®è¯å…±åŒå‡ºç°ï¼Œä¹ŸåŠ å…¥è¯¥ç±»åˆ«
    for doc_keywords in keywords_list:
        unique_doc_kws = set(doc_keywords)
        
        # æ‰¾åˆ°æ–‡æ¡£ä¸­å·²æœ‰çš„ç±»åˆ«
        doc_categories = set()
        for kw in unique_doc_kws:
            doc_categories.update(keyword_categories[kw])
        
        # å°†è¯¥æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…³é”®è¯éƒ½å…³è”åˆ°è¿™äº›ç±»åˆ«
        for kw in unique_doc_kws:
            keyword_categories[kw].update(doc_categories)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    result = {kw: sorted(list(cats)) for kw, cats in keyword_categories.items()}
    return result

# ================= ä¸»å¤„ç†å‡½æ•° =================

def is_target_note(note_title: str, note_content: str = "") -> bool:
    """
    åˆ¤æ–­æ˜¯å¦æ˜¯ç›®æ ‡ç¬”è®°ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
    ä¸ä»…æ£€æŸ¥æ ‡é¢˜ï¼Œä¹Ÿæ£€æŸ¥å†…å®¹
    """
    # æ£€æŸ¥æ ‡é¢˜
    if note_title:
        note_title_lower = note_title.lower()
        
        # å®Œå…¨åŒ¹é…
        if NOTE_TITLE in note_title or note_title == NOTE_TITLE:
            return True
        
        # æ¨¡ç³ŠåŒ¹é…ï¼šåŒ…å«å…³é”®è¯ä¸­çš„è‡³å°‘ä¸€ä¸ª
        for keyword in NOTE_TITLE_KEYWORDS:
            if keyword.lower() in note_title_lower:
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«"AI"å’Œ"é˜…è¯»"æˆ–"æŠ¥å‘Š"
        if "ai" in note_title_lower and ("é˜…è¯»" in note_title or "æŠ¥å‘Š" in note_title):
            return True
    
    # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–ä¸åŒ¹é…ï¼Œæ£€æŸ¥å†…å®¹
    if note_content:
        note_content_lower = note_content.lower()
        # æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦åŒ…å«"AI æ·±åº¦é˜…è¯»æŠ¥å‘Š"æˆ–ç›¸å…³å…³é”®è¯
        if "ai æ·±åº¦é˜…è¯»æŠ¥å‘Š" in note_content_lower or "aiæ·±åº¦é˜…è¯»æŠ¥å‘Š" in note_content_lower:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯ç»„åˆ
        has_ai = "ai" in note_content_lower or "ğŸ¤–" in note_content
        has_reading = "æ·±åº¦é˜…è¯»" in note_content or "é˜…è¯»æŠ¥å‘Š" in note_content
        if has_ai and has_reading:
            return True
        
        # æ£€æŸ¥HTMLæ ¼å¼çš„æ ‡é¢˜
        if "<h1>" in note_content and "ai" in note_content_lower and "é˜…è¯»" in note_content:
            return True
    
    return False

def fetch_items_with_retry(zot, limit, start, max_retries=3):
    """
    å¸¦é‡è¯•æœºåˆ¶çš„è·å–æ–‡çŒ®é¡¹
    """
    for attempt in range(max_retries):
        try:
            items = zot.items(limit=limit, start=start)
            return items
        except Exception as e:
            error_str = str(e)
            # å¦‚æœæ˜¯502é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
            if "502" in error_str or "Bad Gateway" in error_str:
                wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š2s, 4s, 6s
                print(f"\n   âš ï¸  é‡åˆ°502é”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})...")
                time.sleep(wait_time)
                continue
            else:
                # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                raise
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    raise Exception(f"è·å–æ–‡çŒ®å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")

def is_item_type_supported(item):
    """
    æ£€æŸ¥æ–‡çŒ®ç±»å‹æ˜¯å¦æ”¯æŒchildrenè°ƒç”¨
    åªæœ‰ä¸»è¦æ–‡çŒ®ç±»å‹ï¼ˆjournalArticle, conferencePaperç­‰ï¼‰æ‰æ”¯æŒï¼Œé™„ä»¶ä¸æ”¯æŒ
    """
    item_type = item['data'].get('itemType', '')
    # æ”¯æŒçš„ç±»å‹ï¼šä¸»è¦æ–‡çŒ®ç±»å‹
    supported_types = {
        'journalArticle', 'conferencePaper', 'book', 'bookSection',
        'thesis', 'report', 'presentation', 'document', 'manuscript',
        'preprint', 'patent', 'dataset', 'webpage', 'blogPost'
    }
    return item_type in supported_types

def fetch_all_items_with_keywords(zot):
    """
    ä»Zoteroè·å–æ‰€æœ‰æ–‡çŒ®åŠå…¶å…³é”®è¯ï¼ˆæ£€ç´¢æ‰€æœ‰æ–‡çŒ®ï¼Œæ— æ•°é‡é™åˆ¶ï¼‰
    """
    print("\nğŸ“š æ­£åœ¨æ£€ç´¢Zoteroåº“ä¸­çš„æ‰€æœ‰æ–‡çŒ®...")
    
    # è·å–æ‰€æœ‰æ–‡çŒ®é¡¹ï¼ˆåˆ†é¡µè·å–ï¼Œç¡®ä¿è·å–å…¨éƒ¨ï¼‰
    all_items = []
    start = 0
    batch_size = 100  # æ¯æ‰¹è·å–100ä¸ª
    
    try:
        while True:
            # ä½¿ç”¨é‡è¯•æœºåˆ¶è·å–
            items = fetch_items_with_retry(zot, batch_size, start)
            
            if not items:
                break
            
            # è¿‡æ»¤å‡ºæ”¯æŒçš„ç±»å‹
            valid_items = [item for item in items if is_item_type_supported(item)]
            all_items.extend(valid_items)
            
            print(f"   ğŸ“„ å·²è·å– {len(all_items)} ä¸ªæœ‰æ•ˆæ–‡çŒ®é¡¹ (æœ¬æ‰¹: {len(items)}ä¸ªï¼Œæœ‰æ•ˆ: {len(valid_items)}ä¸ª)...", end='\r')
            
            # å¦‚æœè¿”å›çš„æ•°é‡å°‘äºbatch_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰é¡¹
            if len(items) < batch_size:
                break
            
            start += batch_size
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
            time.sleep(0.5)
            
            # å®‰å…¨é™åˆ¶ï¼šæœ€å¤šè·å–10000ç¯‡ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            if len(all_items) >= 10000:
                print(f"\n   âš ï¸  å·²è¾¾åˆ°æœ€å¤§é™åˆ¶ï¼ˆ10000ç¯‡ï¼‰ï¼Œåœæ­¢æ£€ç´¢")
                break
        
        print(f"\n   âœ… å…±æ‰¾åˆ° {len(all_items)} ä¸ªæœ‰æ•ˆæ–‡çŒ®é¡¹")
    except Exception as e:
        print(f"\n   âŒ è·å–æ–‡çŒ®å¤±è´¥: {e}")
        if all_items:
            print(f"   âš ï¸  å·²è·å– {len(all_items)} ä¸ªæ–‡çŒ®é¡¹ï¼Œå°†ç»§ç»­å¤„ç†å·²è·å–çš„é¡¹")
        else:
            return []
    
    print(f"\nğŸ” æ­£åœ¨æŸ¥æ‰¾åŒ…å« '{NOTE_TITLE}' çš„ç¬”è®°å¹¶æå–å…³é”®è¯...")
    print(f"   ğŸ” åŒ¹é…æ¨¡å¼: å®Œå…¨åŒ¹é…æˆ–åŒ…å«å…³é”®è¯ {NOTE_TITLE_KEYWORDS}")
    
    items_with_keywords = []
    notes_found = 0
    notes_checked = 0
    notes_with_target_title = 0
    errors_count = 0
    skipped_count = 0
    
    for i, item in enumerate(all_items):
        if (i + 1) % 50 == 0:
            print(f"   è¿›åº¦: {i + 1}/{len(all_items)} (ç›®æ ‡ç¬”è®°: {notes_with_target_title}, æå–æˆåŠŸ: {notes_found}, é”™è¯¯: {errors_count})...")
        
        try:
            # è·å–å­é¡¹ï¼ˆç¬”è®°ï¼‰- æ·»åŠ é‡è¯•æœºåˆ¶
            children = None
            max_retries = 2
            for retry in range(max_retries):
                try:
                    children = zot.children(item['key'])
                    break
                except Exception as e:
                    error_str = str(e)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯"can only be called on PDF, EPUB, and snapshot attachments"é”™è¯¯
                    if "can only be called on" in error_str or "PDF, EPUB" in error_str:
                        # è¿™ç§æƒ…å†µæ˜¯æ­£å¸¸çš„ï¼ŒæŸäº›é¡¹ç›®ç±»å‹ä¸æ”¯æŒchildren
                        skipped_count += 1
                        if DEBUG_MODE and skipped_count <= 3:
                            item_title = item['data'].get('title', 'Untitled')[:50]
                            item_type = item['data'].get('itemType', 'unknown')
                            print(f"      â­ï¸  è·³è¿‡é¡¹ç›® (ç±»å‹: {item_type}): {item_title}...")
                        children = []
                        break
                    # 502é”™è¯¯ï¼Œç­‰å¾…åé‡è¯•
                    elif "502" in error_str or "Bad Gateway" in error_str:
                        if retry < max_retries - 1:
                            wait_time = (retry + 1) * 1
                            print(f"      âš ï¸  APIé”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            raise
                    else:
                        raise
            
            if children is None:
                errors_count += 1
                if DEBUG_MODE and errors_count <= 3:
                    print(f"      âŒ æ— æ³•è·å–å­é¡¹: {item['data'].get('title', 'Untitled')[:50]}...")
                continue
            
            found_keywords = False
            for child in children:
                if child['data']['itemType'] == 'note':
                    notes_checked += 1
                    note_title = child['data'].get('title', '')
                    note_content = child['data'].get('note', '')
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ç¬”è®°ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼ŒåŒ…æ‹¬æ ‡é¢˜å’Œå†…å®¹ï¼‰
                    if is_target_note(note_title, note_content):
                        notes_with_target_title += 1
                        display_title = note_title if note_title else "(æ— æ ‡é¢˜ï¼Œä»å†…å®¹è¯†åˆ«)"
                        if DEBUG_MODE:
                            print(f"\n      âœ… [{notes_with_target_title}] æ‰¾åˆ°ç›®æ ‡ç¬”è®°: '{display_title}'")
                            print(f"         æ–‡çŒ®: {item['data'].get('title', 'Untitled')[:60]}...")
                        
                        keywords = extract_keywords_from_note(note_content)
                        
                        if keywords:
                            items_with_keywords.append({
                                'key': item['key'],
                                'title': item['data'].get('title', 'Untitled'),
                                'keywords': keywords,
                                'note_title': note_title
                            })
                            notes_found += 1
                            found_keywords = True
                            if DEBUG_MODE:
                                print(f"         âœ¨ æˆåŠŸæå– {len(keywords)} ä¸ªå…³é”®è¯")
                        elif DEBUG_MODE:
                            print(f"         âš ï¸  æœªæå–åˆ°å…³é”®è¯")
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°å…³é”®è¯ï¼Œè·³è¿‡è¯¥æ–‡çŒ®çš„å…¶ä»–ç¬”è®°
                    if found_keywords:
                        break
            
            # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
            if (i + 1) % 10 == 0:
                time.sleep(0.1)
            
        except Exception as e:
            errors_count += 1
            # è·³è¿‡é”™è¯¯é¡¹ï¼Œç»§ç»­å¤„ç†
            if DEBUG_MODE and errors_count <= 5:
                item_title = item['data'].get('title', 'Untitled')[:50]
                print(f"      âŒ å¤„ç†æ–‡çŒ®æ—¶å‡ºé”™: {item_title}... - {str(e)[:100]}")
            continue
    
    print(f"\n   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"      - å¤„ç†äº† {len(all_items)} ä¸ªæœ‰æ•ˆæ–‡çŒ®é¡¹")
    print(f"      - æ£€æŸ¥äº† {notes_checked} ä¸ªç¬”è®°")
    print(f"      - æ‰¾åˆ° {notes_with_target_title} ä¸ªç›®æ ‡ç¬”è®°")
    print(f"      - æˆåŠŸæå– {notes_found} ç¯‡æ–‡çŒ®çš„å…³é”®è¯")
    if skipped_count > 0:
        print(f"      - è·³è¿‡äº† {skipped_count} ä¸ªä¸æ”¯æŒçš„é¡¹ç›®")
    if errors_count > 0:
        print(f"      - é‡åˆ° {errors_count} ä¸ªé”™è¯¯ï¼ˆå·²è·³è¿‡ï¼‰")
    
    return items_with_keywords

def analyze_and_classify_keywords(items_with_keywords: List[Dict]) -> Dict:
    """
    åˆ†æå…³é”®è¯å¹¶è¿›è¡Œåˆ†ç±»
    """
    if not items_with_keywords:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„æ–‡çŒ®ï¼")
        return {}
    
    print(f"\nğŸ“Š æ­£åœ¨åˆ†æ {len(items_with_keywords)} ç¯‡æ–‡çŒ®çš„å…³é”®è¯...")
    
    # æå–æ‰€æœ‰å…³é”®è¯åˆ—è¡¨
    keywords_list = [item['keywords'] for item in items_with_keywords]
    all_keywords = set()
    for kws in keywords_list:
        all_keywords.update(kws)
    
    print(f"   ğŸ“ å…±å‘ç° {len(all_keywords)} ä¸ªå”¯ä¸€å…³é”®è¯")
    
    # æ„å»ºTF-IDFå‘é‡
    print("   ğŸ”„ æ„å»ºTF-IDFå‘é‡...")
    vocab, tfidf_vectors = tfidf_vectorize(keywords_list)
    
    # æ„å»ºå…±ç°çŸ©é˜µ
    print("   ğŸ”„ æ„å»ºå…³é”®è¯å…±ç°çŸ©é˜µ...")
    cooccurrence = build_cooccurrence_matrix(keywords_list)
    print(f"   âœ… å‘ç° {len(cooccurrence)} å¯¹å…±ç°å…³é”®è¯")
    
    # å±‚æ¬¡èšç±»
    print("\nğŸ¯ æ­£åœ¨è¿›è¡Œå…³é”®è¯èšç±»åˆ†æ...")
    categories = hierarchical_clustering(keywords_list, all_keywords, vocab, 
                                        tfidf_vectors, cooccurrence)
    print(f"   âœ… ç”Ÿæˆ {len(categories)} ä¸ªç±»åˆ«")
    
    # å¤šæ ‡ç­¾åˆ†é…
    print("\nğŸ·ï¸  æ­£åœ¨åˆ†é…å¤šæ ‡ç­¾åˆ†ç±»...")
    keyword_categories = assign_multi_category(keywords_list, categories, all_keywords)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_items': len(items_with_keywords),
        'total_unique_keywords': len(all_keywords),
        'total_categories': len(categories),
        'keywords_per_category': {cat: len(kws) for cat, kws in categories.items()},
        'multi_category_keywords': sum(1 for cats in keyword_categories.values() if len(cats) > 1),
        'total_cooccurrences': len(cooccurrence)
    }
    
    return {
        'categories': categories,
        'keyword_assignments': keyword_categories,
        'statistics': stats,
        'raw_data': items_with_keywords
    }

def save_results(results: Dict):
    """
    ä¿å­˜åˆ†æç»“æœ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜JSONæ ¼å¼çš„åˆ†ç±»ç»“æœ
    output_data = {
        'categories': results['categories'],
        'keyword_assignments': results['keyword_assignments'],
        'statistics': results['statistics']
    }
    
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… åˆ†ç±»ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_JSON}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(OUTPUT_STATS, 'w', encoding='utf-8') as f:
        json.dump(results['statistics'], f, indent=2, ensure_ascii=False)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {OUTPUT_STATS}")
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("å…³é”®è¯åˆ†ç±»åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 70 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = results['statistics']
        f.write("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n")
        f.write("-" * 70 + "\n")
        f.write(f"æ€»æ–‡çŒ®æ•°: {stats['total_items']}\n")
        f.write(f"å”¯ä¸€å…³é”®è¯æ•°: {stats['total_unique_keywords']}\n")
        f.write(f"åˆ†ç±»ç±»åˆ«æ•°: {stats['total_categories']}\n")
        f.write(f"å¤šæ ‡ç­¾å…³é”®è¯æ•°: {stats['multi_category_keywords']}\n")
        f.write(f"å…³é”®è¯å…±ç°å¯¹æ•°: {stats['total_cooccurrences']}\n")
        f.write("\n")
        
        # ç±»åˆ«è¯¦æƒ…
        f.write("=" * 70 + "\n")
        f.write("ğŸ“‚ åˆ†ç±»ç±»åˆ«è¯¦æƒ…\n")
        f.write("=" * 70 + "\n\n")
        
        categories = results['categories']
        for i, (category, keywords) in enumerate(sorted(categories.items(), 
                                                        key=lambda x: len(x[1]), 
                                                        reverse=True), 1):
            f.write(f"{i}. {category} ({len(keywords)} ä¸ªå…³é”®è¯)\n")
            f.write("-" * 70 + "\n")
            for kw in keywords:
                assignments = results['keyword_assignments'].get(kw, [])
                if len(assignments) > 1:
                    f.write(f"   â€¢ {kw} [å¤šæ ‡ç­¾: {', '.join(assignments)}]\n")
                else:
                    f.write(f"   â€¢ {kw}\n")
            f.write("\n")
        
        # å¤šæ ‡ç­¾å…³é”®è¯åˆ—è¡¨
        f.write("=" * 70 + "\n")
        f.write("ğŸ·ï¸  å¤šæ ‡ç­¾å…³é”®è¯åˆ—è¡¨\n")
        f.write("=" * 70 + "\n\n")
        
        multi_label_kws = [(kw, cats) for kw, cats in results['keyword_assignments'].items() 
                          if len(cats) > 1]
        multi_label_kws.sort(key=lambda x: len(x[1]), reverse=True)
        
        for kw, cats in multi_label_kws:
            f.write(f"â€¢ {kw}\n")
            f.write(f"  æ‰€å±ç±»åˆ«: {', '.join(cats)}\n\n")
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_REPORT}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 70)
    print("ğŸ” Zotero å…³é”®è¯åˆ†ç±»åˆ†æå·¥å…·")
    print("=" * 70)
    
    # åˆå§‹åŒ–Zoteroè¿æ¥
    try:
        zot = zotero.Zotero(LIBRARY_ID, LIBRARY_TYPE, API_KEY)
        print(f"âœ… å·²è¿æ¥åˆ°Zoteroåº“ (ID: {LIBRARY_ID})")
    except Exception as e:
        print(f"âŒ è¿æ¥Zoteroå¤±è´¥: {e}")
        return
    
    # è·å–æ‰€æœ‰æ–‡çŒ®åŠå…¶å…³é”®è¯
    items_with_keywords = fetch_all_items_with_keywords(zot)
    
    if not items_with_keywords:
        print("\nâš ï¸  æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥ï¼š")
        print(f"   1. ç¬”è®°æ ‡é¢˜æ˜¯å¦åŒ…å« '{NOTE_TITLE}'")
        print(f"   2. ç¬”è®°ä¸­æ˜¯å¦åŒ…å«Keywordséƒ¨åˆ†")
        return
    
    # åˆ†æå¹¶åˆ†ç±»å…³é”®è¯
    results = analyze_and_classify_keywords(items_with_keywords)
    
    if not results:
        return
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("=" * 70)
    stats = results['statistics']
    print(f"æ€»æ–‡çŒ®æ•°: {stats['total_items']}")
    print(f"å”¯ä¸€å…³é”®è¯æ•°: {stats['total_unique_keywords']}")
    print(f"åˆ†ç±»ç±»åˆ«æ•°: {stats['total_categories']}")
    print(f"å¤šæ ‡ç­¾å…³é”®è¯æ•°: {stats['multi_category_keywords']}")
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}/")

if __name__ == "__main__":
    main()

