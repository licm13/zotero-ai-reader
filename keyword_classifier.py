#!/usr/bin/env python3
"""
Zotero å…³é”®è¯åˆ†ç±»åˆ†æå·¥å…·
============================

ä»Zoteroåº“ä¸­æå–æ‰€æœ‰æ–‡çŒ®çš„AIç¬”è®°å…³é”®è¯ï¼Œè¿›è¡Œæ™ºèƒ½èšç±»å’Œåˆ†ç±»åˆ†æã€‚

åŠŸèƒ½ç‰¹æ€§ï¼š
- è‡ªåŠ¨æ£€ç´¢æ‰€æœ‰æ–‡çŒ®çš„"AI æ·±åº¦é˜…è¯»æŠ¥å‘Š"ç¬”è®°
- æå–Keywordså…³é”®è¯éƒ¨åˆ†
- å…³é”®è¯å½’ä¸€åŒ–ï¼ˆå»é‡ã€åŒä¹‰è¯è¯†åˆ«ã€ç¼©å†™å±•å¼€ï¼‰
- åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦å’Œå…±ç°å…³ç³»çš„æ™ºèƒ½èšç±»
- æ”¯æŒå¤šæ ‡ç­¾åˆ†ç±»ï¼ˆä¸€ä¸ªå…³é”®è¯å¯å±äºå¤šä¸ªç±»ç›®ï¼‰
- ç”Ÿæˆå±‚æ¬¡åŒ–åˆ†ç±»ç»“æ„å’Œç»Ÿè®¡æŠ¥å‘Š

ä½œè€…ï¼šProf. Chengming Li (SCUT)
"""

import os
import re
import json
import math
from collections import defaultdict, Counter
from typing import Dict, List, Set, Tuple
from pyzotero import zotero

try:
    import config
    LIBRARY_ID = config.LIBRARY_ID
    API_KEY = config.API_KEY
    LIBRARY_TYPE = config.LIBRARY_TYPE
    print("âœ… å·²ä» config.py åŠ è½½é…ç½®")
except ImportError:
    print("âš ï¸  æœªæ‰¾åˆ° config.py æ–‡ä»¶ï¼")
    print("ğŸ“‹ è¯·å¤åˆ¶ config.example.py ä¸º config.py å¹¶å¡«å…¥æ‚¨çš„é…ç½®ä¿¡æ¯")
    exit(1)

# ================= é…ç½®å‚æ•° =================

NOTE_TITLE = "AI æ·±åº¦é˜…è¯»æŠ¥å‘Š"  # ç›®æ ‡ç¬”è®°æ ‡é¢˜
OUTPUT_DIR = "keyword_analysis"  # è¾“å‡ºç›®å½•
OUTPUT_JSON = os.path.join(OUTPUT_DIR, "keyword_categories.json")  # JSONè¾“å‡º
OUTPUT_REPORT = os.path.join(OUTPUT_DIR, "keyword_report.txt")  # æ–‡æœ¬æŠ¥å‘Š
OUTPUT_STATS = os.path.join(OUTPUT_DIR, "keyword_statistics.json")  # ç»Ÿè®¡ä¿¡æ¯

# ç›¸ä¼¼åº¦é˜ˆå€¼
SEMANTIC_SIMILARITY_THRESHOLD = 0.6  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
STRING_SIMILARITY_THRESHOLD = 0.8    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
COOCCURRENCE_WEIGHT = 0.3             # å…±ç°å…³ç³»æƒé‡
MIN_CLUSTER_SIZE = 2                  # æœ€å°èšç±»å¤§å°

# ================= è¾…åŠ©å‡½æ•° =================

def normalize_keyword(keyword: str) -> str:
    """
    å…³é”®è¯å½’ä¸€åŒ–ï¼š
    - å»é™¤é¦–å°¾ç©ºæ ¼
    - å°å†™åŒ–
    - å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ï¼‰
    """
    # å»é™¤HTMLæ ‡ç­¾
    keyword = re.sub(r'<[^>]+>', '', keyword)
    # è½¬æ¢ä¸ºå°å†™
    keyword = keyword.lower().strip()
    # ä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€è¿å­—ç¬¦ã€ä¸­æ–‡
    keyword = re.sub(r'[^\w\s\-ä¸€-é¾¥]', '', keyword)
    # è§„èŒƒåŒ–ç©ºæ ¼
    keyword = re.sub(r'\s+', ' ', keyword).strip()
    return keyword

def split_keywords(keywords_text: str) -> List[str]:
    """
    ä»å…³é”®è¯æ–‡æœ¬ä¸­æå–å•ä¸ªå…³é”®è¯åˆ—è¡¨
    æ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼šé€—å·ã€åˆ†å·ã€æ¢è¡Œã€ç«–çº¿ç­‰
    """
    # æ›¿æ¢å„ç§åˆ†éš”ç¬¦ä¸ºç»Ÿä¸€çš„åˆ†éš”ç¬¦
    text = re.sub(r'[ï¼Œï¼›ã€\n\r|]', ',', keywords_text)
    # æŒ‰é€—å·åˆ†å‰²
    keywords = [normalize_keyword(kw.strip()) for kw in text.split(',')]
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œå¤ªçŸ­çš„å…³é”®è¯
    keywords = [kw for kw in keywords if kw and len(kw) >= 2]
    return keywords

def extract_keywords_from_note(note_content: str) -> List[str]:
    """
    ä»ç¬”è®°å†…å®¹ä¸­æå–Keywordséƒ¨åˆ†
    """
    # å»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', note_content)
    
    # æŸ¥æ‰¾Keywordséƒ¨åˆ†ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
    patterns = [
        r'(?:Keywords|å…³é”®è¯|è®ºæ–‡å…³é”®è¯|å…³é”®è¯ï¼š|Keywords:)[ï¼š:\s]*\n?\s*(.+?)(?:\n\n|\n[A-Z]|$)',
        r'(?:Keywords|å…³é”®è¯|è®ºæ–‡å…³é”®è¯)[ï¼š:\s]+(.+?)(?:\n\n|\n[A-Z]|$)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
        if match:
            keywords_text = match.group(1).strip()
            # å»é™¤å¯èƒ½çš„åç»­æ ‡é¢˜ï¼ˆå¦‚"Summary"ã€"Abstract"ç­‰ï¼‰
            keywords_text = re.split(r'\n(?:Summary|æ€»ç»“|Abstract|æ‘˜è¦)', keywords_text, flags=re.IGNORECASE)[0]
            keywords = split_keywords(keywords_text)
            if keywords:
                return keywords
    
    return []

def levenshtein_distance(s1: str, s2: str) -> int:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç¼–è¾‘è·ç¦»ï¼ˆLevenshteinè·ç¦»ï¼‰
    """
    if len(s1) < len(s2):
        return levenshtein_distance(s2, s1)
    
    if len(s2) == 0:
        return len(s1)
    
    previous_row = range(len(s2) + 1)
    for i, c1 in enumerate(s1):
        current_row = [i + 1]
        for j, c2 in enumerate(s2):
            insertions = previous_row[j + 1] + 1
            deletions = current_row[j] + 1
            substitutions = previous_row[j] + (c1 != c2)
            current_row.append(min(insertions, deletions, substitutions))
        previous_row = current_row
    
    return previous_row[-1]

def string_similarity(s1: str, s2: str) -> float:
    """
    è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰
    è¿”å›å€¼ï¼š0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºå®Œå…¨ç›¸åŒ
    """
    if not s1 or not s2:
        return 0.0
    if s1 == s2:
        return 1.0
    
    max_len = max(len(s1), len(s2))
    if max_len == 0:
        return 1.0
    
    distance = levenshtein_distance(s1, s2)
    return 1.0 - (distance / max_len)

def jaccard_similarity(set1: Set, set2: Set) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªé›†åˆçš„Jaccardç›¸ä¼¼åº¦
    """
    intersection = len(set1 & set2)
    union = len(set1 | set2)
    if union == 0:
        return 0.0
    return intersection / union

def tfidf_vectorize(keywords_list: List[List[str]]) -> Tuple[Dict[str, int], List[Dict[str, float]]]:
    """
    å°†å…³é”®è¯åˆ—è¡¨è½¬æ¢ä¸ºTF-IDFå‘é‡
    è¿”å›ï¼š(è¯æ±‡è¡¨å­—å…¸, TF-IDFå‘é‡åˆ—è¡¨)
    """
    # æ„å»ºè¯æ±‡è¡¨
    vocab = {}
    doc_freq = defaultdict(int)  # æ–‡æ¡£é¢‘ç‡ï¼ˆåŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°ï¼‰
    
    for doc_keywords in keywords_list:
        unique_keywords = set(doc_keywords)
        for kw in unique_keywords:
            if kw not in vocab:
                vocab[kw] = len(vocab)
            doc_freq[kw] += 1
    
    total_docs = len(keywords_list)
    vocab_size = len(vocab)
    
    # è®¡ç®—TF-IDFå‘é‡
    tfidf_vectors = []
    for doc_keywords in keywords_list:
        vector = {}
        kw_count = Counter(doc_keywords)
        max_count = max(kw_count.values()) if kw_count else 1
        
        for kw, count in kw_count.items():
            # TF (Term Frequency)
            tf = count / max_count
            # IDF (Inverse Document Frequency)
            idf = math.log(total_docs / (doc_freq[kw] + 1))
            # TF-IDF
            vector[kw] = tf * idf
        
        tfidf_vectors.append(vector)
    
    return vocab, tfidf_vectors

def cosine_similarity(vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªTF-IDFå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    """
    # è·å–æ‰€æœ‰å”¯ä¸€çš„é”®
    all_keys = set(vec1.keys()) | set(vec2.keys())
    
    if not all_keys:
        return 0.0
    
    dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in all_keys)
    norm1 = math.sqrt(sum(v * v for v in vec1.values()))
    norm2 = math.sqrt(sum(v * v for v in vec2.values()))
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def build_cooccurrence_matrix(keywords_list: List[List[str]]) -> Dict[Tuple[str, str], int]:
    """
    æ„å»ºå…³é”®è¯å…±ç°çŸ©é˜µ
    è¿”å›ï¼š{(keyword1, keyword2): count, ...}
    """
    cooccurrence = defaultdict(int)
    
    for doc_keywords in keywords_list:
        unique_keywords = list(set(doc_keywords))
        # æ¯å¯¹å…³é”®è¯åœ¨åŒä¸€æ–‡æ¡£ä¸­å‡ºç°ï¼Œå…±ç°æ¬¡æ•°+1
        for i, kw1 in enumerate(unique_keywords):
            for kw2 in unique_keywords[i+1:]:
                # ç¡®ä¿æœ‰åºï¼ˆé¿å…é‡å¤ï¼‰
                pair = tuple(sorted([kw1, kw2]))
                cooccurrence[pair] += 1
    
    return dict(cooccurrence)

def calculate_keyword_similarity(kw1: str, kw2: str, 
                                  vocab: Dict[str, int],
                                  tfidf_vectors: List[Dict[str, float]],
                                  keywords_list: List[List[str]],
                                  cooccurrence: Dict[Tuple[str, str], int]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå…³é”®è¯çš„ç»¼åˆç›¸ä¼¼åº¦
    ç»“åˆï¼šå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ã€TF-IDFè¯­ä¹‰ç›¸ä¼¼åº¦ã€å…±ç°å…³ç³»
    """
    # 1. å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
    str_sim = string_similarity(kw1, kw2)
    
    # 2. è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåŸºäºTF-IDFï¼‰
    # æ‰¾åˆ°åŒ…å«è¿™ä¸¤ä¸ªå…³é”®è¯çš„æ–‡æ¡£ï¼Œè®¡ç®—å®ƒä»¬çš„å‘é‡ç›¸ä¼¼åº¦
    semantic_sim = 0.0
    docs_with_kw1 = [i for i, doc in enumerate(keywords_list) if kw1 in doc]
    docs_with_kw2 = [i for i, doc in enumerate(keywords_list) if kw2 in doc]
    
    if docs_with_kw1 and docs_with_kw2:
        # è®¡ç®—åŒ…å«kw1å’Œkw2çš„æ–‡æ¡£å‘é‡çš„å¹³å‡ç›¸ä¼¼åº¦
        similarities = []
        for i in docs_with_kw1:
            for j in docs_with_kw2:
                sim = cosine_similarity(tfidf_vectors[i], tfidf_vectors[j])
                similarities.append(sim)
        
        if similarities:
            semantic_sim = sum(similarities) / len(similarities)
    
    # 3. å…±ç°å…³ç³»
    cooccurrence_sim = 0.0
    pair = tuple(sorted([kw1, kw2]))
    if pair in cooccurrence:
        # å½’ä¸€åŒ–å…±ç°æ¬¡æ•°ï¼ˆä½¿ç”¨å¯¹æ•°ç¼©æ”¾ï¼‰
        cooccurrence_sim = min(1.0, math.log(cooccurrence[pair] + 1) / math.log(10))
    
    # ç»¼åˆç›¸ä¼¼åº¦ï¼ˆåŠ æƒå¹³å‡ï¼‰
    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦æƒé‡æœ€é«˜ï¼ˆå¤„ç†ç¼©å†™å’Œæ‹¼å†™å˜ä½“ï¼‰
    # è¯­ä¹‰ç›¸ä¼¼åº¦æ¬¡ä¹‹ï¼ˆç†è§£è¯­ä¹‰å…³ç³»ï¼‰
    # å…±ç°å…³ç³»æƒé‡è¾ƒä½ï¼ˆä½œä¸ºè¡¥å……ï¼‰
    combined_sim = (0.5 * str_sim + 
                    0.3 * semantic_sim + 
                    COOCCURRENCE_WEIGHT * cooccurrence_sim)
    
    return combined_sim

def hierarchical_clustering(keywords_list: List[List[str]],
                            all_keywords: Set[str],
                            vocab: Dict[str, int],
                            tfidf_vectors: List[Dict[str, float]],
                            cooccurrence: Dict[Tuple[str, str], int]) -> Dict[str, List[str]]:
    """
    å±‚æ¬¡èšç±»ç®—æ³•ï¼šå°†ç›¸ä¼¼çš„å…³é”®è¯å½’ä¸ºä¸€ç±»
    
    è¿”å›ï¼š{category_name: [keyword1, keyword2, ...], ...}
    """
    # è®¡ç®—æ‰€æœ‰å…³é”®è¯å¯¹çš„ç›¸ä¼¼åº¦
    keyword_pairs = []
    keyword_list = list(all_keywords)
    
    print(f"   ğŸ”„ è®¡ç®— {len(keyword_list)} ä¸ªå…³é”®è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦...")
    for i, kw1 in enumerate(keyword_list):
        for kw2 in keyword_list[i+1:]:
            sim = calculate_keyword_similarity(kw1, kw2, vocab, tfidf_vectors, 
                                              keywords_list, cooccurrence)
            if sim >= SEMANTIC_SIMILARITY_THRESHOLD or string_similarity(kw1, kw2) >= STRING_SIMILARITY_THRESHOLD:
                keyword_pairs.append((sim, kw1, kw2))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    keyword_pairs.sort(reverse=True, key=lambda x: x[0])
    
    # èšç±»ï¼šä½¿ç”¨å¹¶æŸ¥é›†æ€æƒ³
    keyword_to_cluster = {kw: i for i, kw in enumerate(keyword_list)}
    clusters = {i: [kw] for i, kw in enumerate(keyword_list)}
    
    for sim, kw1, kw2 in keyword_pairs:
        cluster1 = keyword_to_cluster[kw1]
        cluster2 = keyword_to_cluster[kw2]
        
        if cluster1 != cluster2:
            # åˆå¹¶ä¸¤ä¸ªèšç±»
            # å°†cluster2ä¸­çš„æ‰€æœ‰å…³é”®è¯ç§»åˆ°cluster1
            for kw in clusters[cluster2]:
                keyword_to_cluster[kw] = cluster1
                clusters[cluster1].append(kw)
            del clusters[cluster2]
    
    # ç”Ÿæˆç±»åˆ«åç§°å’Œç»“æœ
    categories = {}
    for cluster_id, keywords in clusters.items():
        if len(keywords) >= MIN_CLUSTER_SIZE:
            # é€‰æ‹©æœ€é•¿çš„å…³é”®è¯ä½œä¸ºç±»åˆ«åç§°ï¼ˆæˆ–å‡ºç°é¢‘ç‡æœ€é«˜çš„ï¼‰
            category_name = max(keywords, key=lambda x: (len(x), keywords.count(x)))
            categories[category_name] = sorted(set(keywords))
    
    return categories

def assign_multi_category(keywords_list: List[List[str]],
                          categories: Dict[str, List[str]],
                          all_keywords: Set[str]) -> Dict[str, List[str]]:
    """
    å¤šæ ‡ç­¾åˆ†é…ï¼šå…è®¸ä¸€ä¸ªå…³é”®è¯å±äºå¤šä¸ªç±»åˆ«
    åŸºäºå…³é”®è¯åœ¨ä¸åŒæ–‡æ¡£ä¸­çš„å…±ç°æ¨¡å¼
    """
    keyword_categories = defaultdict(set)
    
    # é¦–å…ˆï¼Œå°†å…³é”®è¯åˆ†é…åˆ°å®ƒä»¬æ‰€åœ¨çš„èšç±»ç±»åˆ«
    for category, keywords in categories.items():
        for kw in keywords:
            keyword_categories[kw].add(category)
    
    # ç„¶åï¼ŒåŸºäºå…±ç°å…³ç³»ï¼Œæ‰©å±•å¤šæ ‡ç­¾
    # å¦‚æœä¸€ä¸ªå…³é”®è¯ç»å¸¸ä¸æŸä¸ªç±»åˆ«çš„å…¶ä»–å…³é”®è¯å…±åŒå‡ºç°ï¼Œä¹ŸåŠ å…¥è¯¥ç±»åˆ«
    for doc_keywords in keywords_list:
        unique_doc_kws = set(doc_keywords)
        
        # æ‰¾åˆ°æ–‡æ¡£ä¸­å·²æœ‰çš„ç±»åˆ«
        doc_categories = set()
        for kw in unique_doc_kws:
            doc_categories.update(keyword_categories[kw])
        
        # å°†è¯¥æ–‡æ¡£ä¸­çš„æ‰€æœ‰å…³é”®è¯éƒ½å…³è”åˆ°è¿™äº›ç±»åˆ«
        for kw in unique_doc_kws:
            keyword_categories[kw].update(doc_categories)
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    result = {kw: sorted(list(cats)) for kw, cats in keyword_categories.items()}
    return result

# ================= ä¸»å¤„ç†å‡½æ•° =================

def fetch_all_items_with_keywords(zot):
    """
    ä»Zoteroè·å–æ‰€æœ‰æ–‡çŒ®åŠå…¶å…³é”®è¯
    """
    print("\nğŸ“š æ­£åœ¨æ£€ç´¢Zoteroåº“ä¸­çš„æ‰€æœ‰æ–‡çŒ®...")
    
    # è·å–æ‰€æœ‰æ–‡çŒ®é¡¹ï¼ˆé™åˆ¶1000ç¯‡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    try:
        items = zot.items(limit=1000)
        print(f"   âœ… æ‰¾åˆ° {len(items)} ä¸ªæ–‡çŒ®é¡¹")
    except Exception as e:
        print(f"   âŒ è·å–æ–‡çŒ®å¤±è´¥: {e}")
        return []
    
    print(f"\nğŸ” æ­£åœ¨æŸ¥æ‰¾ '{NOTE_TITLE}' ç¬”è®°å¹¶æå–å…³é”®è¯...")
    
    items_with_keywords = []
    notes_found = 0
    
    for i, item in enumerate(items):
        if (i + 1) % 50 == 0:
            print(f"   è¿›åº¦: {i + 1}/{len(items)}...")
        
        try:
            # è·å–å­é¡¹ï¼ˆç¬”è®°ï¼‰
            children = zot.children(item['key'])
            
            for child in children:
                if child['data']['itemType'] == 'note':
                    note_title = child['data'].get('title', '')
                    note_content = child['data'].get('note', '')
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ç¬”è®°
                    if NOTE_TITLE in note_title or note_title == NOTE_TITLE:
                        keywords = extract_keywords_from_note(note_content)
                        
                        if keywords:
                            items_with_keywords.append({
                                'key': item['key'],
                                'title': item['data'].get('title', 'Untitled'),
                                'keywords': keywords
                            })
                            notes_found += 1
                            break
            
        except Exception as e:
            # è·³è¿‡é”™è¯¯é¡¹ï¼Œç»§ç»­å¤„ç†
            continue
    
    print(f"   âœ… æˆåŠŸæå– {notes_found} ç¯‡æ–‡çŒ®çš„å…³é”®è¯")
    return items_with_keywords

def analyze_and_classify_keywords(items_with_keywords: List[Dict]) -> Dict:
    """
    åˆ†æå…³é”®è¯å¹¶è¿›è¡Œåˆ†ç±»
    """
    if not items_with_keywords:
        print("\nâš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ…å«å…³é”®è¯çš„æ–‡çŒ®ï¼")
        return {}
    
    print(f"\nğŸ“Š æ­£åœ¨åˆ†æ {len(items_with_keywords)} ç¯‡æ–‡çŒ®çš„å…³é”®è¯...")
    
    # æå–æ‰€æœ‰å…³é”®è¯åˆ—è¡¨
    keywords_list = [item['keywords'] for item in items_with_keywords]
    all_keywords = set()
    for kws in keywords_list:
        all_keywords.update(kws)
    
    print(f"   ğŸ“ å…±å‘ç° {len(all_keywords)} ä¸ªå”¯ä¸€å…³é”®è¯")
    
    # æ„å»ºTF-IDFå‘é‡
    print("   ğŸ”„ æ„å»ºTF-IDFå‘é‡...")
    vocab, tfidf_vectors = tfidf_vectorize(keywords_list)
    
    # æ„å»ºå…±ç°çŸ©é˜µ
    print("   ğŸ”„ æ„å»ºå…³é”®è¯å…±ç°çŸ©é˜µ...")
    cooccurrence = build_cooccurrence_matrix(keywords_list)
    print(f"   âœ… å‘ç° {len(cooccurrence)} å¯¹å…±ç°å…³é”®è¯")
    
    # å±‚æ¬¡èšç±»
    print("\nğŸ¯ æ­£åœ¨è¿›è¡Œå…³é”®è¯èšç±»åˆ†æ...")
    categories = hierarchical_clustering(keywords_list, all_keywords, vocab, 
                                        tfidf_vectors, cooccurrence)
    print(f"   âœ… ç”Ÿæˆ {len(categories)} ä¸ªç±»åˆ«")
    
    # å¤šæ ‡ç­¾åˆ†é…
    print("\nğŸ·ï¸  æ­£åœ¨åˆ†é…å¤šæ ‡ç­¾åˆ†ç±»...")
    keyword_categories = assign_multi_category(keywords_list, categories, all_keywords)
    
    # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_items': len(items_with_keywords),
        'total_unique_keywords': len(all_keywords),
        'total_categories': len(categories),
        'keywords_per_category': {cat: len(kws) for cat, kws in categories.items()},
        'multi_category_keywords': sum(1 for cats in keyword_categories.values() if len(cats) > 1),
        'total_cooccurrences': len(cooccurrence)
    }
    
    return {
        'categories': categories,
        'keyword_assignments': keyword_categories,
        'statistics': stats,
        'raw_data': items_with_keywords
    }

def save_results(results: Dict):
    """
    ä¿å­˜åˆ†æç»“æœ
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ä¿å­˜JSONæ ¼å¼çš„åˆ†ç±»ç»“æœ
    output_data = {
        'categories': results['categories'],
        'keyword_assignments': results['keyword_assignments'],
        'statistics': results['statistics']
    }
    
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… åˆ†ç±»ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_JSON}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
    with open(OUTPUT_STATS, 'w', encoding='utf-8') as f:
        json.dump(results['statistics'], f, indent=2, ensure_ascii=False)
    print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜è‡³: {OUTPUT_STATS}")
    
    # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    with open(OUTPUT_REPORT, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("å…³é”®è¯åˆ†ç±»åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 70 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = results['statistics']
        f.write("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n")
        f.write("-" * 70 + "\n")
        f.write(f"æ€»æ–‡çŒ®æ•°: {stats['total_items']}\n")
        f.write(f"å”¯ä¸€å…³é”®è¯æ•°: {stats['total_unique_keywords']}\n")
        f.write(f"åˆ†ç±»ç±»åˆ«æ•°: {stats['total_categories']}\n")
        f.write(f"å¤šæ ‡ç­¾å…³é”®è¯æ•°: {stats['multi_category_keywords']}\n")
        f.write(f"å…³é”®è¯å…±ç°å¯¹æ•°: {stats['total_cooccurrences']}\n")
        f.write("\n")
        
        # ç±»åˆ«è¯¦æƒ…
        f.write("=" * 70 + "\n")
        f.write("ğŸ“‚ åˆ†ç±»ç±»åˆ«è¯¦æƒ…\n")
        f.write("=" * 70 + "\n\n")
        
        categories = results['categories']
        for i, (category, keywords) in enumerate(sorted(categories.items(), 
                                                        key=lambda x: len(x[1]), 
                                                        reverse=True), 1):
            f.write(f"{i}. {category} ({len(keywords)} ä¸ªå…³é”®è¯)\n")
            f.write("-" * 70 + "\n")
            for kw in keywords:
                assignments = results['keyword_assignments'].get(kw, [])
                if len(assignments) > 1:
                    f.write(f"   â€¢ {kw} [å¤šæ ‡ç­¾: {', '.join(assignments)}]\n")
                else:
                    f.write(f"   â€¢ {kw}\n")
            f.write("\n")
        
        # å¤šæ ‡ç­¾å…³é”®è¯åˆ—è¡¨
        f.write("=" * 70 + "\n")
        f.write("ğŸ·ï¸  å¤šæ ‡ç­¾å…³é”®è¯åˆ—è¡¨\n")
        f.write("=" * 70 + "\n\n")
        
        multi_label_kws = [(kw, cats) for kw, cats in results['keyword_assignments'].items() 
                          if len(cats) > 1]
        multi_label_kws.sort(key=lambda x: len(x[1]), reverse=True)
        
        for kw, cats in multi_label_kws:
            f.write(f"â€¢ {kw}\n")
            f.write(f"  æ‰€å±ç±»åˆ«: {', '.join(cats)}\n\n")
    
    print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_REPORT}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("=" * 70)
    print("ğŸ” Zotero å…³é”®è¯åˆ†ç±»åˆ†æå·¥å…·")
    print("=" * 70)
    
    # åˆå§‹åŒ–Zoteroè¿æ¥
    try:
        zot = zotero.Zotero(LIBRARY_ID, LIBRARY_TYPE, API_KEY)
        print(f"âœ… å·²è¿æ¥åˆ°Zoteroåº“ (ID: {LIBRARY_ID})")
    except Exception as e:
        print(f"âŒ è¿æ¥Zoteroå¤±è´¥: {e}")
        return
    
    # è·å–æ‰€æœ‰æ–‡çŒ®åŠå…¶å…³é”®è¯
    items_with_keywords = fetch_all_items_with_keywords(zot)
    
    if not items_with_keywords:
        print("\nâš ï¸  æœªæ‰¾åˆ°åŒ…å«å…³é”®è¯çš„æ–‡çŒ®ï¼Œè¯·æ£€æŸ¥ï¼š")
        print(f"   1. ç¬”è®°æ ‡é¢˜æ˜¯å¦åŒ…å« '{NOTE_TITLE}'")
        print(f"   2. ç¬”è®°ä¸­æ˜¯å¦åŒ…å«Keywordséƒ¨åˆ†")
        return
    
    # åˆ†æå¹¶åˆ†ç±»å…³é”®è¯
    results = analyze_and_classify_keywords(items_with_keywords)
    
    if not results:
        return
    
    # ä¿å­˜ç»“æœ
    save_results(results)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("=" * 70)
    stats = results['statistics']
    print(f"æ€»æ–‡çŒ®æ•°: {stats['total_items']}")
    print(f"å”¯ä¸€å…³é”®è¯æ•°: {stats['total_unique_keywords']}")
    print(f"åˆ†ç±»ç±»åˆ«æ•°: {stats['total_categories']}")
    print(f"å¤šæ ‡ç­¾å…³é”®è¯æ•°: {stats['multi_category_keywords']}")
    print("\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {OUTPUT_DIR}/")

if __name__ == "__main__":
    main()

