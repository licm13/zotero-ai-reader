#!/usr/bin/env python3
"""
å…³é”®è¯Top20åˆ†æå·¥å…·
====================

ä»keyword_categories.jsonå’Œkeyword_statistics.jsonä¸­æå–å…³é”®è¯ï¼Œ
ä½¿ç”¨Gemini AIè¿›è¡Œæ™ºèƒ½åˆå¹¶å’Œè§„èŒƒåŒ–ï¼Œç”ŸæˆTop20å…³é”®è¯åˆ—è¡¨ã€‚

åŠŸèƒ½ï¼š
1. æå–å¹¶æ‹†åˆ†å…³é”®è¯
2. ç»Ÿè®¡é¢‘æ¬¡
3. ä½¿ç”¨Geminiè¯†åˆ«åŒä¹‰è¯å¹¶åˆå¹¶
4. ç”ŸæˆTop20å…³é”®è¯æŠ¥å‘Š
"""

import os
import json
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple
from google import genai

# ================= é…ç½® =================

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

API_KEY = "AIzaSyCq8R1HDwiU8dEQDFxpLo-JVYKeIYAWDog"
MODEL = "gemini-3-pro-preview"

KEYWORD_CATEGORIES_FILE = os.path.join(SCRIPT_DIR, "keyword_categories.json")
KEYWORD_STATISTICS_FILE = os.path.join(SCRIPT_DIR, "keyword_statistics.json")
OUTPUT_TOP20_FILE = os.path.join(SCRIPT_DIR, "keyword_top20.json")
OUTPUT_TOP20_REPORT = os.path.join(SCRIPT_DIR, "keyword_top20_report.txt")

# åœç”¨è¯ï¼ˆå¸¸è§çš„æ— æ„ä¹‰è¯ï¼‰
STOP_WORDS = {
    'tags', 'tag', 'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 
    'at', 'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'are',
    'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do',
    'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
    'must', 'can', 'this', 'that', 'these', 'those', 'data', 'model',
    'analysis', 'study', 'research', 'method', 'approach'
}

# ================= æ•°æ®é¢„å¤„ç†å‡½æ•° =================

def load_json_file(filepath: str) -> dict:
    """åŠ è½½JSONæ–‡ä»¶"""
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)

def remove_tags_prefix(keyword: str) -> str:
    """ç§»é™¤'tags'å‰ç¼€"""
    if keyword.startswith('tags'):
        return keyword[4:].strip()
    return keyword.strip()

def split_keywords(keyword_string: str) -> List[str]:
    """
    å°†å…³é”®è¯å­—ç¬¦ä¸²æ‹†åˆ†æˆå•ä¸ªå…³é”®è¯
    å¤„ç†è¿å­—ç¬¦ã€æ–œæ ç­‰åˆ†éš”ç¬¦
    """
    # ç§»é™¤tagså‰ç¼€
    keyword_string = remove_tags_prefix(keyword_string)
    
    # å…ˆå¤„ç†å¸¸è§çš„åˆ†éš”ç¬¦ï¼Œç»Ÿä¸€ä¸ºç©ºæ ¼
    keyword_string = re.sub(r'[-/|]', ' ', keyword_string)
    
    # æŒ‰ç©ºæ ¼åˆ†å‰²
    words = keyword_string.split()
    
    # è¿‡æ»¤å’Œæ¸…ç†
    keywords = []
    for word in words:
        # å»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€è¿å­—ç¬¦ï¼‰
        word = re.sub(r'[^\w-]', '', word)
        word = word.strip()
        
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²ã€å¤ªçŸ­çš„è¯ã€åœç”¨è¯
        if word and len(word) >= 3 and word.lower() not in STOP_WORDS:
            keywords.append(word.lower())
    
    return keywords

def extract_and_count_keywords(categories_data: dict) -> Counter:
    """
    ä»keyword_assignmentsä¸­æå–æ‰€æœ‰å…³é”®è¯å¹¶ç»Ÿè®¡é¢‘æ¬¡
    """
    keyword_counter = Counter()
    keyword_assignments = categories_data.get('keyword_assignments', {})
    
    print("ğŸ“Š æ­£åœ¨æå–å’Œæ‹†åˆ†å…³é”®è¯...")
    
    total_phrases = len(keyword_assignments)
    for i, (keyword_phrase, _) in enumerate(keyword_assignments.items()):
        if (i + 1) % 500 == 0:
            print(f"   è¿›åº¦: {i + 1}/{total_phrases}...", end='\r')
        
        # æ‹†åˆ†å…³é”®è¯çŸ­è¯­
        keywords = split_keywords(keyword_phrase)
        
        # ç»Ÿè®¡æ¯ä¸ªå…³é”®è¯çš„é¢‘æ¬¡
        for kw in keywords:
            keyword_counter[kw] += 1
    
    print(f"\n   âœ… æå–å®Œæˆï¼Œå…± {len(keyword_counter)} ä¸ªå”¯ä¸€å…³é”®è¯")
    return keyword_counter

# ================= Gemini AIåˆ†æå‡½æ•° =================

def prepare_keywords_for_gemini(keyword_counter: Counter, top_n: int = 100) -> List[Dict]:
    """
    å‡†å¤‡Top Nå…³é”®è¯ä¾›Geminiåˆ†æ
    """
    # è·å–é¢‘æ¬¡æœ€é«˜çš„å‰Nä¸ªå…³é”®è¯
    top_keywords = keyword_counter.most_common(top_n)
    
    keywords_data = []
    for keyword, frequency in top_keywords:
        keywords_data.append({
            "keyword": keyword,
            "frequency": frequency
        })
    
    return keywords_data

def merge_keywords_with_gemini(keywords_data: List[Dict]) -> Dict:
    """
    ä½¿ç”¨Gemini APIè¯†åˆ«åŒä¹‰è¯å¹¶åˆå¹¶å…³é”®è¯
    """
    print("\nğŸ¤– æ­£åœ¨ä½¿ç”¨Gemini AIè¿›è¡Œå…³é”®è¯åˆå¹¶åˆ†æ...")
    
    client = genai.Client(api_key=API_KEY)
    
    # å‡†å¤‡ç´§å‡‘æ ¼å¼çš„å…³é”®è¯åˆ—è¡¨ï¼ˆå‚è€ƒorganizer.pyçš„æ ¼å¼ï¼‰
    keywords_text = "\n".join([
        f"{i+1}|{item['keyword']}|{item['frequency']}"
        for i, item in enumerate(keywords_data)
    ])
    
    prompt = f"""You are a Research Assistant for Prof. Chengming Li (Hydrology/Hydro-climatology/Remote Sensing).

TASK: Analyze keyword list and identify synonyms/similar terms for intelligent merging.

CONTEXT:
- Domain: Hydrology, Remote Sensing, Climate Science, Water Resources
- Focus Areas: Evapotranspiration (ET), Soil Moisture, Drought, Data Fusion, Triple Collocation, Machine Learning

KEYWORDS (Format: ID|Keyword|Frequency):
{keywords_text}

MERGE RULES:
1. **Synonyms**: Merge (e.g., evapotranspiration â†” transpiration, soil moisture â†” soil-moisture)
2. **Hierarchy**: Merge specific to general (e.g., "satellite remote sensing" â†’ "remote sensing")
3. **Format Variants**: Unify format (e.g., "climate-change" â†’ "climate change")
4. **Related but Different**: DO NOT merge (e.g., climate change â‰  climate warming, drought â‰  flood)

EXAMPLES:
- âœ… MERGE: evapotranspiration (150) + transpiration (80) â†’ evapotranspiration (230)
- âœ… MERGE: remote sensing (200) + satellite remote sensing (50) â†’ remote sensing (250)
- âœ… MERGE: soil moisture (120) + soil-moisture (30) â†’ soil moisture (150)
- âŒ DO NOT MERGE: climate change (100) vs climate warming (60) [related but distinct concepts]
- âŒ DO NOT MERGE: drought (80) vs flood (70) [opposite concepts]

OUTPUT REQUIREMENTS:
- Return top 20 most important keywords after merging
- Merged frequency = sum of all variants' frequencies
- Sort by merged frequency (descending)
- Use standardized keyword forms

OUTPUT: JSON format:
{{
    "normalized_keywords": [
        {{
            "keyword": "evapotranspiration",
            "frequency": 230,
            "variants": ["evapotranspiration", "transpiration"],
            "description": "Merged synonyms: evapotranspiration includes transpiration"
        }},
        ...
    ],
    "merge_rules": [
        {{
            "from": "transpiration",
            "to": "evapotranspiration",
            "reason": "synonym: transpiration is a component of evapotranspiration"
        }},
        ...
    ]
}}

JSON:"""

    try:
        print("   æ­£åœ¨è°ƒç”¨Gemini API...")
        response = client.models.generate_content(
            model=MODEL,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        
        result = json.loads(response.text)
        print("   âœ… Geminiåˆ†æå®Œæˆ")
        return result
        
    except Exception as e:
        print(f"   âŒ Gemini APIè°ƒç”¨å¤±è´¥: {e}")
        # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®çš„å‰20ä¸ª
        print("   âš ï¸  ä½¿ç”¨åŸå§‹é¢‘æ¬¡æ’åº...")
        return {
            "normalized_keywords": [
                {
                    "keyword": item['keyword'],
                    "frequency": item['frequency'],
                    "variants": [item['keyword']],
                    "description": "åŸå§‹å…³é”®è¯"
                }
                for item in keywords_data[:20]
            ],
            "merge_rules": []
        }

def apply_merge_to_all_keywords(keyword_counter: Counter, merge_result: Dict) -> Counter:
    """
    å°†Geminiçš„åˆå¹¶è§„åˆ™åº”ç”¨åˆ°æ‰€æœ‰å…³é”®è¯
    """
    merge_rules = merge_result.get('merge_rules', [])
    normalized_keywords = merge_result.get('normalized_keywords', [])
    
    # æ„å»ºæ˜ å°„ï¼šä»å˜ä½“åˆ°æ ‡å‡†å…³é”®è¯
    variant_to_normalized = {}
    for item in normalized_keywords:
        normalized = item['keyword']
        for variant in item.get('variants', [normalized]):
            variant_to_normalized[variant.lower()] = normalized
    
    # åº”ç”¨åˆ°æ‰€æœ‰åˆå¹¶è§„åˆ™
    for rule in merge_rules:
        from_kw = rule.get('from', '').lower()
        to_kw = rule.get('to', '').lower()
        if from_kw and to_kw:
            variant_to_normalized[from_kw] = to_kw
    
    # åº”ç”¨åˆå¹¶
    merged_counter = Counter()
    for keyword, frequency in keyword_counter.items():
        normalized = variant_to_normalized.get(keyword.lower(), keyword)
        merged_counter[normalized] += frequency
    
    return merged_counter

# ================= ä¸»å‡½æ•° =================

def main():
    """
    ä¸»åˆ†ææµç¨‹
    """
    print("=" * 70)
    print("ğŸ” å…³é”®è¯Top20åˆ†æå·¥å…·")
    print("=" * 70)
    
    # 1. åŠ è½½æ•°æ®
    print("\nğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
    try:
        categories_data = load_json_file(KEYWORD_CATEGORIES_FILE)
        statistics_data = load_json_file(KEYWORD_STATISTICS_FILE)
        print(f"   âœ… å·²åŠ è½½ {KEYWORD_CATEGORIES_FILE}")
        print(f"   âœ… å·²åŠ è½½ {KEYWORD_STATISTICS_FILE}")
        print(f"   ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ï¼š")
        print(f"      - æ€»æ–‡çŒ®æ•°: {statistics_data.get('total_items', 0)}")
        print(f"      - å”¯ä¸€å…³é”®è¯æ•°: {statistics_data.get('total_unique_keywords', 0)}")
        print(f"      - åˆ†ç±»æ•°: {statistics_data.get('total_categories', 0)}")
    except Exception as e:
        print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. æå–å’Œç»Ÿè®¡å…³é”®è¯
    keyword_counter = extract_and_count_keywords(categories_data)
    
    # 3. å‡†å¤‡Top 100ä¾›Geminiåˆ†æ
    print(f"\nğŸ“‹ å‡†å¤‡Top 100å…³é”®è¯ä¾›AIåˆ†æ...")
    top_keywords = prepare_keywords_for_gemini(keyword_counter, top_n=100)
    print(f"   âœ… å·²å‡†å¤‡ {len(top_keywords)} ä¸ªå…³é”®è¯")
    
    # 4. ä½¿ç”¨Geminiè¿›è¡Œæ™ºèƒ½åˆå¹¶
    merge_result = merge_keywords_with_gemini(top_keywords)
    
    # 5. åº”ç”¨åˆå¹¶è§„åˆ™åˆ°æ‰€æœ‰å…³é”®è¯
    print("\nğŸ”„ æ­£åœ¨åº”ç”¨åˆå¹¶è§„åˆ™åˆ°æ‰€æœ‰å…³é”®è¯...")
    merged_counter = apply_merge_to_all_keywords(keyword_counter, merge_result)
    
    # 6. è·å–Top20
    top20 = merged_counter.most_common(20)
    
    # 7. æ„å»ºè¯¦ç»†ç»“æœ
    result = {
        "top20_keywords": [
            {
                "rank": i + 1,
                "keyword": keyword,
                "frequency": frequency,
                "percentage": round(frequency / len(keyword_counter) * 100, 2) if len(keyword_counter) > 0 else 0,
                "gemini_info": next(
                    (item for item in merge_result.get('normalized_keywords', []) 
                     if item['keyword'].lower() == keyword.lower()),
                    None
                )
            }
            for i, (keyword, frequency) in enumerate(top20)
        ],
        "statistics": {
            "total_keywords_before_merge": len(keyword_counter),
            "total_keywords_after_merge": len(merged_counter),
            "merge_rules_count": len(merge_result.get('merge_rules', [])),
            "total_frequency": sum(keyword_counter.values())
        },
        "merge_details": merge_result
    }
    
    # 8. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    with open(OUTPUT_TOP20_FILE, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    print(f"   âœ… å·²ä¿å­˜åˆ°: {OUTPUT_TOP20_FILE}")
    
    # 9. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    with open(OUTPUT_TOP20_REPORT, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("å…³é”®è¯Top20åˆ†ææŠ¥å‘Š\n")
        f.write("=" * 70 + "\n\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = result['statistics']
        f.write("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n")
        f.write("-" * 70 + "\n")
        f.write(f"åˆå¹¶å‰å…³é”®è¯æ•°: {stats['total_keywords_before_merge']}\n")
        f.write(f"åˆå¹¶åå…³é”®è¯æ•°: {stats['total_keywords_after_merge']}\n")
        f.write(f"åˆå¹¶è§„åˆ™æ•°: {stats['merge_rules_count']}\n")
        f.write(f"æ€»é¢‘æ¬¡: {stats['total_frequency']}\n")
        f.write("\n")
        
        # Top20åˆ—è¡¨
        f.write("=" * 70 + "\n")
        f.write("ğŸ† Top20å…³é”®è¯\n")
        f.write("=" * 70 + "\n\n")
        
        for item in result['top20_keywords']:
            rank = item['rank']
            keyword = item['keyword']
            frequency = item['frequency']
            percentage = item['percentage']
            gemini_info = item.get('gemini_info')
            
            f.write(f"{rank:2d}. {keyword}\n")
            f.write(f"    é¢‘æ¬¡: {frequency} ({percentage}%)\n")
            
            if gemini_info:
                variants = gemini_info.get('variants', [])
                if len(variants) > 1:
                    f.write(f"    åˆå¹¶çš„å˜ä½“: {', '.join(variants)}\n")
                desc = gemini_info.get('description', '')
                if desc:
                    f.write(f"    è¯´æ˜: {desc}\n")
            f.write("\n")
        
        # åˆå¹¶è§„åˆ™è¯¦æƒ…
        merge_rules = merge_result.get('merge_rules', [])
        if merge_rules:
            f.write("=" * 70 + "\n")
            f.write("ğŸ”€ åˆå¹¶è§„åˆ™è¯¦æƒ…\n")
            f.write("=" * 70 + "\n\n")
            for i, rule in enumerate(merge_rules, 1):
                f.write(f"{i}. {rule.get('from', '')} â†’ {rule.get('to', '')}\n")
                f.write(f"   åŸå› : {rule.get('reason', 'N/A')}\n\n")
    
    print(f"   âœ… å·²ä¿å­˜åˆ°: {OUTPUT_TOP20_REPORT}")
    
    # 10. æ‰“å°æ‘˜è¦
    print("\n" + "=" * 70)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("=" * 70)
    print(f"åˆå¹¶å‰å…³é”®è¯æ•°: {stats['total_keywords_before_merge']}")
    print(f"åˆå¹¶åå…³é”®è¯æ•°: {stats['total_keywords_after_merge']}")
    print(f"åˆå¹¶è§„åˆ™æ•°: {stats['merge_rules_count']}")
    print("\nğŸ† Top10å…³é”®è¯:")
    for item in result['top20_keywords'][:10]:
        print(f"  {item['rank']:2d}. {item['keyword']:30s} (é¢‘æ¬¡: {item['frequency']})")
    print("\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()

