import time
import json
import math
import os
from pyzotero import zotero
from google import genai
import config  # å¤ç”¨æ‚¨çš„é…ç½®æ–‡ä»¶

# ================= 1. é…ç½®åŒºåŸŸ =================

# --- å…³é”®è®¾ç½® ---
# å¦‚æœä¸ä¸º Noneï¼Œè„šæœ¬åªä¼šåœ¨è¿™ä¸ªé›†åˆé‡Œæ‰¾æ–‡çŒ®è¿›è¡Œåˆ†ç±»
# ç¤ºä¾‹: TARGET_COLLECTION_PATH = "00_Inbox" æˆ– "2025/Pending"
TARGET_COLLECTION_PATH = getattr(config, 'TARGET_COLLECTION_PATH', None) 
# æ‚¨ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œå¼ºåˆ¶æŒ‡å®šï¼Œè¦†ç›– config.py
# TARGET_COLLECTION_PATH = "2025_New_Papers"

DRY_RUN = True          # True=ä»…æµ‹è¯•ï¼ŒFalse=çœŸç§»åŠ¨
BATCH_SIZE = 5          # æ‰¹å¤„ç†å¤§å°
AUTO_TAG_NAME = "auto_organized" # é˜²æ­¢é‡å¤å¤„ç†çš„æ ‡ç­¾

# --- å­¦æœ¯ç”»åƒ (ä¿æŒä¸å˜) ---
USER_PROFILE_CONTEXT = """
The user is a Professor in Hydrology (Chengming Li, SCUT/Tsinghua), specializing in:
1. Evapotranspiration (ET), Transpiration, and Global Water Cycle.
2. Hydrological Extremes: Specifically "Flash Drought", "Flood", and "Drought-Flood Abrupt Alternation" (æ—±æ¶æ€¥è½¬).
3. Data Methods: Triple Collocation, Data Fusion, Uncertainty Analysis, and Deep Learning in Hydrology.

PREFERRED CATEGORY STRUCTURE (Hierarchy):
- Hydrological Extremes
  - Drought & Flash Drought
  - Flood & Inundation
  - Drought-Flood Transitions (For 'Abrupt Alternation' papers)
- Water Cycle Processes
  - Evapotranspiration & GPP (Focus on ET products, physiology)
  - Runoff & Streamflow
  - Snow & Glaciers (Cryosphere)
  - Soil Moisture
- Methodology
  - Data Fusion & Uncertainty (For Triple Collocation, Merging)
  - Remote Sensing Retrieval (For algorithm development)
  - AI & Deep Learning (For LSTM, CNN applications)
- Climate Change & Attribution
"""

# ================= 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def find_collection_by_path(zot, collection_path):
    """(å¤ç”¨è‡ª reader.py) æ ¹æ®è·¯å¾„æŸ¥æ‰¾é›†åˆ Key"""
    if not collection_path: return None
    path_parts = [p.strip() for p in collection_path.split('/') if p.strip()]
    if not path_parts: return None
    
    # è·å–æ‰€æœ‰é›†åˆå»ºç«‹æ˜ å°„ (ä¸ºäº†æ•ˆç‡ï¼Œåªåšç®€å•åç§°åŒ¹é…ï¼Œä¸¥è°¨ç‰ˆéœ€é€’å½’)
    try:
        all_colls = zot.collections()
    except Exception as e:
        print(f"âŒ è·å–é›†åˆåˆ—è¡¨å¤±è´¥: {e}")
        return None
        
    # ç®€å•æŸ¥æ‰¾é€»è¾‘ï¼šæ‰¾åˆ°åŒ¹é…è·¯å¾„æœ«å°¾åç§°çš„é›†åˆ
    # æ³¨æ„ï¼šå¦‚æœæœ‰åŒåé›†åˆï¼Œè¿™é‡Œå¯èƒ½ä¼šæ··æ·†ï¼Œå»ºè®®ä½¿ç”¨ç‹¬ç‰¹åç§°
    target_name = path_parts[-1]
    for c in all_colls:
        if c['data']['name'] == target_name:
            # å¯ä»¥åœ¨è¿™é‡Œå¢åŠ å¯¹çˆ¶é›†åˆçš„æ ¡éªŒé€»è¾‘
            return c['key']
    
    print(f"âš ï¸ æœªæ‰¾åˆ°é›†åˆ: {collection_path}")
    return None

def get_all_collections_map(zot):
    """è·å–ç°æœ‰é›†åˆæ˜ å°„ {name: key} ç”¨äºAIå‚è€ƒ"""
    # ä»…è·å–é¡¶å±‚å’ŒäºŒçº§ï¼Œé¿å…Tokenè¿‡å¤š
    raw_colls = zot.collections()
    return {c['data']['name']: c['key'] for c in raw_colls}

def extract_tags_from_note(note_content):
    import re
    text = re.sub(r'<[^>]+>', '', note_content)
    match = re.search(r'(?:Keywords[â€“-]Tags|è®ºæ–‡åˆ†ç±»)[ï¼š:]\s*(.+)', text, re.IGNORECASE)
    if match: return match.group(1).strip()
    return ""

def ai_classify_batch(batch_items, existing_colls):
    client = genai.Client(api_key=config.AI_API_KEY)
    papers_desc = [f"ID {i}: Title='{item['title']}', Keywords='{item['tags']}'" for i, item in enumerate(batch_items)]
    papers_text = "\n".join(papers_desc)
    existing_list = ", ".join(list(existing_colls.keys())[:50])

    prompt = f"""
    {USER_PROFILE_CONTEXT}
    
    TASK: Classify these papers into collections.
    EXISTING COLLECTIONS: [{existing_list}]
    
    INSTRUCTIONS:
    1. Match papers to the "Preferred Category Structure" if possible.
    2. Return JSON with IDs as keys and "collection_path" as values.
    
    INPUT:
    {papers_text}
    
    OUTPUT JSON format: {{"0": "Path/To/Collection"}}
    """
    
    try:
        response = client.models.generate_content(
            model=config.AI_MODEL,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        return json.loads(response.text)
    except Exception as e:
        print(f"   âŒ Batch AI Error: {e}")
        return {}

def ensure_and_move(zot, item, path, cached_colls):
    """åˆ›å»ºè·¯å¾„å¹¶ç§»åŠ¨"""
    if not path or path == "Unclassified": return
    parts = [p.strip() for p in path.split('/') if p.strip()]
    parent_key = None
    
    # é€çº§åˆ›å»ºç›®å½•
    for part in parts:
        found_key = cached_colls.get(part) # ç®€å•æŸ¥æ‰¾
        
        if not found_key:
            if not DRY_RUN:
                print(f"      ğŸ”¨ åˆ›å»ºæ–°é›†åˆ: {part}")
                try:
                    payload = {'name': part}
                    if parent_key: payload['parentCollection'] = parent_key
                    res = zot.create_collections([payload])
                    if res and 'successful' in res:
                        found_key = list(res['successful'].values())[0]['key']
                        cached_colls[part] = found_key
                except Exception as e:
                    print(f"      âŒ åˆ›å»ºå¤±è´¥: {e}")
                    return
            else:
                print(f"      [Dry Run] æ‹Ÿåˆ›å»ºé›†åˆ: {part}")
                found_key = "fake_" + part
        
        parent_key = found_key

    # ç§»åŠ¨æ–‡çŒ®
    if parent_key and not parent_key.startswith("fake_"):
        if not DRY_RUN:
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨é›†åˆä¸­
            if parent_key not in item['data'].get('collections', []):
                try:
                    zot.add_to_collection(parent_key, item)
                    zot.add_tags(item, AUTO_TAG_NAME) # æ‰“æ ‡
                    print(f"      âœ… å·²ç§»å…¥: {path}")
                except Exception as e:
                    print(f"      âŒ ç§»åŠ¨å¤±è´¥: {e}")
            else:
                print(f"      â„¹ï¸  å·²åœ¨ç›®æ ‡é›†åˆä¸­")
        else:
            print(f"      [Dry Run] æ‹Ÿç§»å…¥: {path}")

# ================= 3. ä¸»æµç¨‹ =================

def main():
    print(f"ğŸš€ å¯åŠ¨æ™ºèƒ½å½’æ¡£ (Dry Run: {DRY_RUN})")
    zot = zotero.Zotero(config.LIBRARY_ID, config.LIBRARY_TYPE, config.API_KEY)
    colls_cache = get_all_collections_map(zot)

    # --- å…³é”®ä¿®æ”¹ï¼šæ”¯æŒæŒ‡å®šé›†åˆ ---
    target_coll_key = None
    if TARGET_COLLECTION_PATH:
        print(f"ğŸ“‚ æŒ‡å®šç›®æ ‡é›†åˆè·¯å¾„: {TARGET_COLLECTION_PATH}")
        target_coll_key = find_collection_by_path(zot, TARGET_COLLECTION_PATH)
        if not target_coll_key:
            print("âŒ æœªæ‰¾åˆ°æŒ‡å®šé›†åˆï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚é€€å‡ºã€‚")
            return
        print(f"âœ… é”å®šé›†åˆKey: {target_coll_key}")

    # è·å–å¾…å¤„ç†æ–‡çŒ®
    print("ğŸ” æ­£åœ¨è·å–æ–‡çŒ®åˆ—è¡¨...")
    if target_coll_key:
        # ä»…è·å–ç‰¹å®šé›†åˆä¸‹çš„æ–‡çŒ® (APIè¿‡æ»¤)
        # æ³¨æ„ï¼šcollection_items é»˜è®¤ä¸æ·±å±‚é€’å½’ï¼Œå¦‚éœ€é€’å½’éœ€åŠ å‚æ•°ï¼Œè¿™é‡Œæš‚åªå¤„ç†è¯¥å±‚çº§
        items = zot.collection_items(target_coll_key, tag='gemini_read', limit=50)
        print(f"   - èŒƒå›´: é›†åˆ '{TARGET_COLLECTION_PATH}'")
    else:
        # å…¨åº“æœç´¢
        items = zot.items(tag='gemini_read', limit=50)
        print(f"   - èŒƒå›´: æ•´ä¸ªæ–‡çŒ®åº“")

    # æœ¬åœ°è¿‡æ»¤å·²å¤„ç†çš„
    todo_items = []
    for it in items:
        tags = [t['tag'] for t in it['data'].get('tags', [])]
        if AUTO_TAG_NAME not in tags:
            # æå–ç¬”è®°
            children = zot.children(it['key'])
            note_tags = ""
            for child in children:
                if child['data']['itemType'] == 'note':
                    extracted = extract_tags_from_note(child['data']['note'])
                    if extracted: 
                        note_tags = extracted
                        break
            
            if note_tags:
                todo_items.append({
                    'key': it['key'],
                    'data': it['data'],
                    'title': it['data'].get('title', 'No Title'),
                    'tags': note_tags
                })
    
    print(f"âœ… å¾…å¤„ç†æ–‡çŒ®æ•°: {len(todo_items)}")
    if not todo_items: return

    # æ‰¹å¤„ç†å¾ªç¯
    total_batches = math.ceil(len(todo_items) / BATCH_SIZE)
    for i in range(total_batches):
        batch = todo_items[i*BATCH_SIZE : (i+1)*BATCH_SIZE]
        print(f"\nğŸ“¦ Batch {i+1}/{total_batches} ({len(batch)} items)...")
        
        results = ai_classify_batch(batch, colls_cache)
        
        for idx_str, path in results.items():
            try:
                idx = int(idx_str)
                if idx < len(batch):
                    ensure_and_move(zot, batch[idx]['data'], path, colls_cache)
            except Exception as e:
                print(f"   âš ï¸ Error: {e}")
        
        time.sleep(2)

    print("\nğŸ‰ å®Œæˆ")

if __name__ == "__main__":
    main()