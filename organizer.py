#!/usr/bin/env python3
"""
Zotero AI-Powered Paper Organizer - Dual-Track Classification System
=====================================================================

Automatically classifies papers into TWO simultaneous structures:
1. Track A (üìö Archive): Subject-based hierarchy for easy retrieval
2. Track B (üí° Idea Lab): Question-based hierarchy for scientific exploration

Key Features:
- Dual-track classification (one paper ‚Üí two collections)
- Profile-driven AI prompts (loads user_profile.json from profiler.py)
- Local caching to minimize API calls
- Batch processing for efficiency
- Smart tagging to prevent duplicate processing
- Targeted collection processing support

Author: Prof. Chengming Li (SCUT)
"""

import time
import json
import math
import os
import sys
from pyzotero import zotero
from google import genai

# ÈÖçÁΩÆÂä†ËΩΩ
from config_loader import get_config_from_args_or_interactive

config = get_config_from_args_or_interactive()
if config is None:
    print("‚ùå Êó†Ê≥ïÂä†ËΩΩÈÖçÁΩÆÊñá‰ª∂ÔºåÁ®ãÂ∫èÈÄÄÂá∫")
    sys.exit(1)

# ================= 1. Configuration =================

# Target collection to process (None = process entire library)
TARGET_COLLECTION_PATH = getattr(config, 'TARGET_COLLECTION_PATH', None)

# Processing settings
DRY_RUN = True                      # True = test mode, False = actually move items
BATCH_SIZE = 5                      # Number of papers to classify per API call
AUTO_TAG_NAME = "auto_organized"    # Tag to prevent duplicate processing
PROFILE_FILE = 'user_profile.json'  # User profile generated by profiler.py
CACHE_FILE = "collections_cache.json"  # Local cache for collection IDs

# ================= 2. Dual-Track Taxonomy Definition =================

# Comprehensive Dual-Track Taxonomy for Hydrology Research
PREFERRED_TAXONOMY = {
    "Track_A_Archive": {
        "description": "Standard disciplinary classification for systematic retrieval",
        "emoji": "üìö",
        "structure": {
            "Processes": [
                "Evapotranspiration (ET)",
                "Runoff & Streamflow",
                "Soil Moisture",
                "Groundwater",
                "Cryosphere (Snow & Glacier)"
            ],
            "Hazards": [
                "Drought/Flash Drought",
                "Flood",
                "Compound Events/DFA",
                "Heatwaves & Extremes"
            ],
            "Methodology": [
                "Remote Sensing/Retrieval",
                "Deep Learning (LSTM_CNN)",
                "Data Fusion",
                "Triple Collocation/Uncertainty",
                "Statistical Methods"
            ],
            "Applications": [
                "Water Resources Management",
                "Climate Change Impact",
                "Agricultural Systems",
                "Early Warning Systems"
            ]
        }
    },
    "Track_B_Idea_Lab": {
        "description": "Taste-driven classification based on scientific questions and mechanisms",
        "emoji": "üí°",
        "structure": {
            "Mechanism": [
                "Abrupt Transitions/Phase Change",
                "Land-Atmosphere Coupling",
                "Feedback Loops & Cascades",
                "Threshold Behavior"
            ],
            "Data Philosophy": [
                "Signal Purification/Uncertainty",
                "Scale Issues (Spatial_Temporal)",
                "Multi-Source Integration",
                "Data Quality & Validation"
            ],
            "Modeling": [
                "Physics-Informed AI",
                "Causal Inference",
                "Hybrid Modeling (Process+ML)",
                "Ensemble Methods"
            ],
            "Dynamics": [
                "Non-linearity & Complexity",
                "Tipping Points",
                "Drought-Flood Transitions",
                "Rapid Onset Events"
            ],
            "Coupling": [
                "Vegetation-Water Interaction",
                "Energy-Water Nexus",
                "Human-Nature Systems",
                "Multi-Sphere Coupling"
            ]
        }
    }
}

# ================= 3. Profile & Cache Management =================

def load_user_profile():
    """Load user profile generated by profiler.py"""
    if os.path.exists(PROFILE_FILE):
        try:
            with open(PROFILE_FILE, 'r', encoding='utf-8') as f:
                profile = json.load(f)
            print(f"‚úÖ Loaded user profile from {PROFILE_FILE}")
            return profile
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load profile: {e}")

    # Default profile if file doesn't exist
    print("‚ÑπÔ∏è  Using default hydrology profile")
    return {
        "base_info": "Default Hydrology Researcher Profile",
        "dynamic_analysis": {
            "summary": "Focus on hydrological extremes and AI applications",
            "focus_areas": ["ET", "Flash Drought", "DFA", "Deep Learning"]
        }
    }

def load_cache():
    """Load collection ID cache from disk"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  Failed to load cache: {e}")
    return {}

def save_cache(cache):
    """Save collection ID cache to disk"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to save cache: {e}")

def refresh_cache_from_zotero(zot):
    """Fetch all collections from Zotero and build cache"""
    print("üîÑ Refreshing collection cache from Zotero...")
    try:
        all_colls = zot.collections()
        cache = {}

        # Build hierarchical mapping
        for c in all_colls:
            name = c['data']['name']
            key = c['key']
            parent = c['data'].get('parentCollection', None)

            cache[name] = {
                'key': key,
                'parent': parent if parent else None
            }

        save_cache(cache)
        print(f"‚úÖ Cached {len(cache)} collections")
        return cache
    except Exception as e:
        print(f"‚ùå Failed to fetch collections: {e}")
        return {}

# ================= 4. Collection Management =================

def find_collection_by_path(zot, collection_path, cache):
    """
    Find collection key by path (e.g., "Parent/Child")
    Uses cache first, falls back to Zotero API if needed
    """
    if not collection_path:
        return None

    parts = [p.strip() for p in collection_path.split('/') if p.strip()]
    if not parts:
        return None

    # Try to find in cache
    current_parent = None
    for i, part in enumerate(parts):
        if part not in cache:
            print(f"   ‚ö†Ô∏è  Collection '{part}' not in cache, refreshing...")
            cache = refresh_cache_from_zotero(zot)
            if part not in cache:
                print(f"   ‚ùå Collection '{part}' not found")
                return None

        # Verify parent relationship (except for top level)
        if i > 0:
            expected_parent = cache[parts[i-1]]['key']
            actual_parent = cache[part].get('parent')
            if actual_parent != expected_parent:
                print(f"   ‚ö†Ô∏è  Warning: '{part}' has different parent than expected")

        current_parent = cache[part]['key']

    return current_parent

def ensure_collection_path(zot, path, cache):
    """
    Create collection path if it doesn't exist
    Returns the final collection key
    """
    if not path or path == "Unclassified":
        return None

    parts = [p.strip() for p in path.split('/') if p.strip()]
    parent_key = None

    for part in parts:
        # Check if collection exists in cache
        if part in cache:
            coll_info = cache[part]

            # Verify parent matches (except top level)
            if parent_key and coll_info.get('parent') != parent_key:
                # Collision: same name, different parent - need to create new
                found_key = None
            else:
                found_key = coll_info['key']
        else:
            found_key = None

        # Create if doesn't exist
        if not found_key:
            if DRY_RUN:
                print(f"      [Dry Run] Would create: {part}")
                found_key = f"fake_{part}_{parent_key or 'root'}"
            else:
                print(f"      üî® Creating collection: {part}")
                try:
                    payload = {'name': part}
                    if parent_key:
                        payload['parentCollection'] = parent_key

                    res = zot.create_collections([payload])
                    if res and 'successful' in res:
                        # Extract key from response
                        success_dict = res['successful']
                        found_key = list(success_dict.values())[0]['key']

                        # Update cache
                        cache[part] = {
                            'key': found_key,
                            'parent': parent_key
                        }
                        save_cache(cache)
                        print(f"      ‚úÖ Created: {part} (Key: {found_key})")
                    else:
                        print(f"      ‚ùå Creation failed: {res}")
                        return None
                except Exception as e:
                    print(f"      ‚ùå Error creating '{part}': {e}")
                    return None

        parent_key = found_key

    return parent_key

# ================= 5. AI Classification =================

def extract_keywords_from_note(note_content):
    """Extract keywords/tags from AI-generated note"""
    import re

    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', note_content)

    # Look for common keyword patterns
    patterns = [
        r'(?:Keywords|ÂÖ≥ÈîÆËØç|ËÆ∫ÊñáÂàÜÁ±ª|Tags)[Ôºö:‚Äì-]\s*(.+?)(?:\n|$)',
        r'(?:ÂàÜÁ±ª|Classification)[Ôºö:]\s*(.+?)(?:\n|$)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            return match.group(1).strip()

    # Fallback: extract first 200 chars after "Summary" or "ÊÄªÁªì"
    summary_match = re.search(r'(?:Summary|ÊÄªÁªì)[Ôºö:]\s*(.{1,200})', text, re.IGNORECASE)
    if summary_match:
        return summary_match.group(1).strip()

    return ""

def format_taxonomy_for_prompt():
    """Convert taxonomy to compact text format for AI prompt"""
    lines = []

    for track_name, track_data in PREFERRED_TAXONOMY.items():
        emoji = track_data.get('emoji', '')
        desc = track_data.get('description', '')

        track_label = track_name.replace('_', ' ').replace('Track A', 'üìö Archive').replace('Track B', 'üí° Idea Lab')
        lines.append(f"\n{track_label} ({desc}):")

        for category, subcategories in track_data['structure'].items():
            lines.append(f"\n  {category}:")
            for subcat in subcategories:
                lines.append(f"    - {subcat}")

    return "\n".join(lines)

def ai_dual_classify_batch(batch_items, user_profile, ai_model, ai_key):
    """
    Classify a batch of papers using AI with dual-track logic

    Returns: dict with format {"0": {"archive": "path1", "idea": "path2"}, ...}
    """
    client = genai.Client(api_key=ai_key)

    # Format papers (compact)
    papers_list = []
    for i, item in enumerate(batch_items):
        papers_list.append(f"{i}|{item['title'][:80]}|{item['keywords'][:150]}")

    papers_text = "\n".join(papers_list)

    # Extract profile summary
    profile_summary = user_profile.get('dynamic_analysis', {}).get('summary', 'General hydrology researcher')
    focus_areas = user_profile.get('dynamic_analysis', {}).get('focus_areas', [])

    # Build compact prompt
    taxonomy_text = format_taxonomy_for_prompt()

    prompt = f"""You are a Research Assistant for Prof. Chengming Li (Hydrology/Hydro-climatology).

USER PROFILE:
- Base: {user_profile.get('base_info', '')}
- Current Focus: {profile_summary}
- Key Areas: {', '.join(focus_areas)}

TASK: Classify each paper into TWO distinct tracks:
1. **Archive Track (üìö)**: Standard subject/method classification for retrieval
2. **Idea Lab Track (üí°)**: Scientific question/mechanism classification for exploration

TAXONOMY (Use these paths or suggest logical sub-paths):
{taxonomy_text}

PAPERS (Format: ID|Title|Keywords):
{papers_text}

OUTPUT: JSON with format {{"0": {{"archive": "üìö Archive/Category/Subcategory", "idea": "üí° Idea Lab/Category/Subcategory"}}, "1": ..., ...}}
Return "Unclassified" if unsure. Use exact emoji prefixes and path separators (/).

JSON:"""

    try:
        response = client.models.generate_content(
            model=ai_model,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )

        result = json.loads(response.text)
        print(f"   ü§ñ AI classified {len(result)} papers")
        return result

    except Exception as e:
        print(f"   ‚ùå AI classification error: {e}")
        return {}

# ================= 6. Item Processing =================

def add_item_to_collections(zot, item, collection_keys):
    """Add item to multiple collections and mark with tag"""
    if not collection_keys:
        return

    # Filter out fake keys
    valid_keys = [k for k in collection_keys if not k.startswith("fake_")]

    if not valid_keys:
        return

    if DRY_RUN:
        return

    try:
        # Check current collections
        current_colls = item.get('collections', [])

        # Add to each collection if not already there
        added_count = 0
        for target_key in valid_keys:
            if target_key not in current_colls:
                zot.addto_collection(target_key, item)
                added_count += 1

        if added_count > 0:
            print(f"      ‚úÖ Added to {added_count} collection(s)")

        # Add tag
        current_tags = item.get('tags', [])
        tag_names = [t.get('tag', '') for t in current_tags]

        if AUTO_TAG_NAME not in tag_names:
            current_tags.append({'tag': AUTO_TAG_NAME})
            item['tags'] = current_tags
            zot.update_item(item)
            print(f"      üè∑Ô∏è  Tagged: {AUTO_TAG_NAME}")

    except Exception as e:
        print(f"      ‚ùå Failed to add to collections: {e}")

# ================= 7. Main Processing =================

def main():
    print("=" * 70)
    print("ü§ñ Zotero AI Paper Organizer - Dual-Track Classification System")
    print("=" * 70)
    print(f"Mode: {'üß™ DRY RUN (Test Mode)' if DRY_RUN else 'üöÄ LIVE MODE'}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Target Collection: {TARGET_COLLECTION_PATH or 'Entire Library'}")
    print("=" * 70)

    # Initialize
    zot = zotero.Zotero(config.LIBRARY_ID, config.LIBRARY_TYPE, config.API_KEY)

    # Load user profile
    user_profile = load_user_profile()

    # Load or refresh cache
    cache = load_cache()
    if not cache:
        cache = refresh_cache_from_zotero(zot)
    else:
        print(f"‚úÖ Loaded {len(cache)} collections from cache")

    # Find target collection if specified
    target_coll_key = None
    if TARGET_COLLECTION_PATH:
        print(f"\nüìÇ Looking for collection: {TARGET_COLLECTION_PATH}")
        target_coll_key = find_collection_by_path(zot, TARGET_COLLECTION_PATH, cache)

        if not target_coll_key:
            print("‚ùå Target collection not found. Exiting.")
            return

        print(f"‚úÖ Found collection (Key: {target_coll_key})")

    # Fetch items to process
    print("\nüîç Fetching items...")

    if target_coll_key:
        # Get items from specific collection with gemini_read tag
        items = zot.collection_items(target_coll_key, tag='gemini_read', limit=100)
        print(f"   Scope: Collection '{TARGET_COLLECTION_PATH}'")
    else:
        # Get items from entire library with gemini_read tag
        items = zot.items(tag='gemini_read', limit=100)
        print(f"   Scope: Entire library")

    print(f"   Found: {len(items)} items with 'gemini_read' tag")

    # Filter items that haven't been organized yet
    todo_items = []

    for item in items:
        # Skip if already has auto_organized tag
        tags = [t.get('tag', '') for t in item['data'].get('tags', [])]
        if AUTO_TAG_NAME in tags:
            continue

        # Get AI-generated note
        children = zot.children(item['key'])
        keywords = ""

        for child in children:
            if child['data']['itemType'] == 'note':
                note_content = child['data'].get('note', '')
                extracted = extract_keywords_from_note(note_content)
                if extracted:
                    keywords = extracted
                    break

        if keywords:
            todo_items.append({
                'key': item['key'],
                'data': item['data'],
                'title': item['data'].get('title', 'Untitled'),
                'keywords': keywords
            })

    print(f"‚úÖ Items to organize: {len(todo_items)}")

    if not todo_items:
        print("\nüéâ No items to process!")
        return

    # Process in batches
    total_batches = math.ceil(len(todo_items) / BATCH_SIZE)
    organized_count = 0

    for batch_idx in range(total_batches):
        batch = todo_items[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]

        print(f"\nüì¶ Batch {batch_idx + 1}/{total_batches} ({len(batch)} items)")
        print("-" * 70)

        # AI dual classification
        print("   üß† Calling AI for dual-track classification...")
        classifications = ai_dual_classify_batch(batch, user_profile, config.AI_MODEL, config.AI_API_KEY)

        # Process each classification
        for idx_str, paths in classifications.items():
            try:
                idx = int(idx_str)
                if idx >= len(batch):
                    continue

                item_info = batch[idx]
                print(f"\n   [{idx}] {item_info['title'][:60]}...")

                # Extract paths
                archive_path = paths.get('archive', '')
                idea_path = paths.get('idea', '')

                print(f"      üìö Archive: {archive_path}")
                print(f"      üí° Idea Lab: {idea_path}")

                # Collect keys
                collection_keys = []

                # Ensure archive path exists
                if archive_path and archive_path != "Unclassified":
                    archive_key = ensure_collection_path(zot, archive_path, cache)
                    if archive_key:
                        collection_keys.append(archive_key)

                # Ensure idea lab path exists
                if idea_path and idea_path != "Unclassified":
                    idea_key = ensure_collection_path(zot, idea_path, cache)
                    if idea_key:
                        collection_keys.append(idea_key)

                # Add to collections
                if collection_keys:
                    if DRY_RUN:
                        print(f"      [Dry Run] Would add to {len(collection_keys)} collection(s)")
                    else:
                        add_item_to_collections(zot, item_info['data'], collection_keys)

                    organized_count += 1

            except Exception as e:
                print(f"   ‚ùå Error processing item {idx_str}: {e}")

        # Rate limiting
        if batch_idx < total_batches - 1:
            print("\n   ‚è∏Ô∏è  Waiting 3s before next batch...")
            time.sleep(3)

    # Summary
    print("\n" + "=" * 70)
    print("üìä Summary")
    print("=" * 70)
    print(f"Total items processed: {len(todo_items)}")
    print(f"Successfully organized: {organized_count}")
    print(f"Mode: {'DRY RUN (no changes made)' if DRY_RUN else 'LIVE MODE (changes saved)'}")
    print("=" * 70)
    print("\nüéâ Done!")

    if DRY_RUN:
        print("\nüí° Tip: Set DRY_RUN = False in organizer.py to actually organize items")

if __name__ == "__main__":
    main()
