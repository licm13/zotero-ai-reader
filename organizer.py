#!/usr/bin/env python3
"""
Zotero AI-Powered Paper Organizer - Dual-Track Classification System
=====================================================================

Automatically classifies papers into TWO simultaneous structures:
1. Track A (ğŸ“š Archive): Subject-based hierarchy for easy retrieval
2. Track B (ğŸ’¡ Idea Lab): Question-based hierarchy for scientific exploration

Key Features:
- Dual-track classification (one paper â†’ two collections)
- Profile-driven AI prompts (loads user_profile.json from profiler.py)
- Local caching to minimize API calls
- Batch processing for efficiency
- Smart tagging to prevent duplicate processing
- Targeted collection processing support

Author: Prof. Chengming Li (SCUT)
"""

import time
import json
import math
import os
import sys
from pyzotero import zotero
from google import genai

# é…ç½®åŠ è½½
from config_loader import get_config_from_args_or_interactive

config = get_config_from_args_or_interactive()
if config is None:
    print("âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
    sys.exit(1)

# ================= 1. Configuration =================

# Target collection to process (None = process entire library)
TARGET_COLLECTION_PATH = getattr(config, 'TARGET_COLLECTION_PATH', None)

# Processing settings
DRY_RUN = False                      # True = test mode, False = actually move items
BATCH_SIZE = 5                      # Number of papers to classify per API call
AUTO_TAG_NAME = "auto_organized"    # Tag to prevent duplicate processing
PROFILE_FILE = 'user_profile.json'  # User profile generated by profiler.py
CACHE_FILE = "collections_cache.json"  # Local cache for collection IDs

# ================= 2. Dual-Track Taxonomy Definition =================

# Comprehensive Dual-Track Taxonomy for Hydrology Research
PREFERRED_TAXONOMY = {
    "Track_A_Archive": {
        "description": "Standard disciplinary classification for systematic retrieval",
        "emoji": "ğŸ“š",
        "structure": {
            "Processes": [
                "Evapotranspiration (ET)",
                "Runoff & Streamflow",
                "Soil Moisture",
                "Groundwater",
                "Cryosphere (Snow & Glacier)"
            ],
            "Hazards": [
                "Drought/Flash Drought",
                "Flood",
                "Compound Events/DFA",
                "Heatwaves & Extremes"
            ],
            "Methodology": [
                "Remote Sensing/Retrieval",
                "Deep Learning (LSTM_CNN)",
                "Data Fusion",
                "Triple Collocation/Uncertainty",
                "Statistical Methods"
            ],
            "Applications": [
                "Water Resources Management",
                "Climate Change Impact",
                "Agricultural Systems",
                "Early Warning Systems"
            ]
        }
    },
    "Track_B_Idea_Lab": {
        "description": "Taste-driven classification based on scientific questions and mechanisms",
        "emoji": "ğŸ’¡",
        "structure": {
            "Mechanism": [
                "Abrupt Transitions/Phase Change",
                "Land-Atmosphere Coupling",
                "Feedback Loops & Cascades",
                "Threshold Behavior"
            ],
            "Data Philosophy": [
                "Signal Purification/Uncertainty",
                "Scale Issues (Spatial_Temporal)",
                "Multi-Source Integration",
                "Data Quality & Validation"
            ],
            "Modeling": [
                "Physics-Informed AI",
                "Causal Inference",
                "Hybrid Modeling (Process+ML)",
                "Ensemble Methods"
            ],
            "Dynamics": [
                "Non-linearity & Complexity",
                "Tipping Points",
                "Drought-Flood Transitions",
                "Rapid Onset Events"
            ],
            "Coupling": [
                "Vegetation-Water Interaction",
                "Energy-Water Nexus",
                "Human-Nature Systems",
                "Multi-Sphere Coupling"
            ]
        }
    }
}

# ================= 3. Profile & Cache Management =================

def load_user_profile():
    """Load user profile generated by profiler.py"""
    if os.path.exists(PROFILE_FILE):
        try:
            with open(PROFILE_FILE, 'r', encoding='utf-8') as f:
                profile = json.load(f)
            print(f"âœ… Loaded user profile from {PROFILE_FILE}")
            return profile
        except Exception as e:
            print(f"âš ï¸  Failed to load profile: {e}")

    # Default profile if file doesn't exist
    print("â„¹ï¸  Using default hydrology profile")
    return {
        "base_info": "Default Hydrology Researcher Profile",
        "dynamic_analysis": {
            "summary": "Focus on hydrological extremes and AI applications",
            "focus_areas": ["ET", "Flash Drought", "DFA", "Deep Learning"]
        }
    }

def load_cache():
    """Load collection ID cache from disk"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âš ï¸  Failed to load cache: {e}")
    return {}

def save_cache(cache):
    """Save collection ID cache to disk"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"âš ï¸  Failed to save cache: {e}")

def refresh_cache_from_zotero(zot):
    """Fetch all collections from Zotero and build cache"""
    print("ğŸ”„ Refreshing collection cache from Zotero...")
    try:
        all_colls = zot.collections()
        cache = {}

        # Build a mapping from key to collection data for path reconstruction
        key_to_coll = {c['key']: c for c in all_colls}

        def build_full_path(coll):
            path = [coll['data']['name']]
            parent = coll['data'].get('parentCollection', None)
            while parent:
                parent_coll = key_to_coll.get(parent)
                if not parent_coll:
                    break
                path.insert(0, parent_coll['data']['name'])
                parent = parent_coll['data'].get('parentCollection', None)
            return '/'.join(path)

        for c in all_colls:
            full_path = build_full_path(c)
            key = c['key']
            parent = c['data'].get('parentCollection', None)
            cache[full_path] = {
                'key': key,
                'parent': parent if parent else None
            }

        save_cache(cache)
        print(f"âœ… Cached {len(cache)} collections (by path)")
        return cache
    except Exception as e:
        print(f"âŒ Failed to fetch collections: {e}")
        return {}

# ================= 4. Collection Management =================

def find_collection_by_path(zot, collection_path, cache):
    """
    Find collection key by path (e.g., "Parent/Child")
    Uses cache first, falls back to Zotero API if needed
    """
    if not collection_path:
        return None

    parts = [p.strip() for p in collection_path.split('/') if p.strip()]
    if not parts:
        return None

    # Build full path step by step
    current_path = ''
    current_parent = None
    for i, part in enumerate(parts):
        current_path = '/'.join(parts[:i+1])
        if current_path not in cache:
            print(f"   âš ï¸  Collection path '{current_path}' not in cache, refreshing...")
            cache = refresh_cache_from_zotero(zot)
            if current_path not in cache:
                print(f"   âŒ Collection path '{current_path}' not found")
                return None
        current_parent = cache[current_path]['key']
    return current_parent

def ensure_collection_path(zot, path, cache):
    """
    Create collection path if it doesn't exist
    Returns the final collection key
    """
    if not path or path == "Unclassified":
        return None

    parts = [p.strip() for p in path.split('/') if p.strip()]
    parent_key = None
    current_path = ''

    for i, part in enumerate(parts):
        current_path = '/'.join(parts[:i+1])
        # Check if collection exists in cache by full path
        if current_path in cache:
            coll_info = cache[current_path]
            found_key = coll_info['key']
        else:
            found_key = None

        # Create if doesn't exist
        if not found_key:
            if DRY_RUN:
                print(f"      [Dry Run] Would create: {current_path}")
                found_key = f"fake_{part}_{parent_key or 'root'}"
            else:
                print(f"      ğŸ”¨ Creating collection: {current_path}")
                try:
                    payload = {'name': part}
                    if parent_key:
                        payload['parentCollection'] = parent_key

                    res = zot.create_collections([payload])

                    # Validate response exists
                    if not res:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: APIè¿”å›ç©ºå“åº”")
                        return None

                    # Validate response structure
                    if not isinstance(res, dict):
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: APIå“åº”æ ¼å¼ä¸æ­£ç¡® ({type(res)})")
                        return None

                    # Check for successful creation
                    if 'successful' not in res:
                        if 'failed' in res:
                            failed_info = res['failed']
                            print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: {failed_info}")
                        else:
                            print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: å“åº”ä¸­ç¼ºå°‘'successful'å­—æ®µ")
                        return None

                    # Extract key from response
                    success_dict = res['successful']

                    # Validate success_dict is not empty
                    if not success_dict:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: 'successful'å­—æ®µä¸ºç©º")
                        return None

                    # Validate success_dict is a dictionary
                    if not isinstance(success_dict, dict):
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: 'successful'å­—æ®µæ ¼å¼ä¸æ­£ç¡® ({type(success_dict)})")
                        return None

                    # Extract the key from the first successful item
                    try:
                        first_value = list(success_dict.values())[0]
                        if not isinstance(first_value, dict):
                            print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: æˆåŠŸé¡¹æ ¼å¼ä¸æ­£ç¡®")
                            return None

                        if 'key' not in first_value:
                            print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: æˆåŠŸé¡¹ç¼ºå°‘'key'å­—æ®µ")
                            return None

                        found_key = first_value['key']

                        # Validate key is a non-empty string
                        if not found_key or not isinstance(found_key, str):
                            print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: è¿”å›çš„keyæ— æ•ˆ ({found_key})")
                            return None

                    except (IndexError, KeyError, TypeError) as extract_err:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: æ— æ³•æå–key - {extract_err}")
                        return None

                    # Update cache
                    cache[current_path] = {
                        'key': found_key,
                        'parent': parent_key
                    }
                    save_cache(cache)
                    print(f"      âœ… Created: {current_path} (Key: {found_key})")

                except Exception as e:
                    error_msg = str(e)
                    if '400' in error_msg:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥ (è¯·æ±‚æ ¼å¼é”™è¯¯): {e}")
                    elif '403' in error_msg or 'Forbidden' in error_msg:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥ (æƒé™ä¸è¶³): {e}")
                    elif 'timeout' in error_msg.lower() or 'timed out' in error_msg.lower():
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥ (è¯·æ±‚è¶…æ—¶): {e}")
                    else:
                        print(f"      âŒ åˆ›å»ºé›†åˆå¤±è´¥: {e}")
                    return None

        parent_key = found_key

    return parent_key

# ================= 5. AI Classification =================

def extract_keywords_from_note(note_content):
    """Extract keywords/tags from AI-generated note"""
    import re

    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', note_content)

    # Look for common keyword patterns
    patterns = [
        r'(?:Keywords|å…³é”®è¯|è®ºæ–‡åˆ†ç±»|Tags)[ï¼š:â€“-]\s*(.+?)(?:\n|$)',
        r'(?:åˆ†ç±»|Classification)[ï¼š:]\s*(.+?)(?:\n|$)',
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            return match.group(1).strip()

    # Fallback: extract first 200 chars after "Summary" or "æ€»ç»“"
    summary_match = re.search(r'(?:Summary|æ€»ç»“)[ï¼š:]\s*(.{1,200})', text, re.IGNORECASE)
    if summary_match:
        return summary_match.group(1).strip()

    return ""

def format_taxonomy_for_prompt():
    """Convert taxonomy to compact text format for AI prompt"""
    lines = []

    for track_name, track_data in PREFERRED_TAXONOMY.items():
        emoji = track_data.get('emoji', '')
        desc = track_data.get('description', '')

        track_label = track_name.replace('_', ' ').replace('Track A', 'ğŸ“š Archive').replace('Track B', 'ğŸ’¡ Idea Lab')
        lines.append(f"\n{track_label} ({desc}):")

        for category, subcategories in track_data['structure'].items():
            lines.append(f"\n  {category}:")
            for subcat in subcategories:
                lines.append(f"    - {subcat}")

    return "\n".join(lines)

def ai_dual_classify_batch(batch_items, user_profile, ai_model, ai_key):
    """
    Classify a batch of papers using AI with dual-track logic

    Returns: dict with format {"0": {"archive": "path1", "idea": "path2"}, ...}
    """
    client = genai.Client(api_key=ai_key)

    # Format papers (compact)
    papers_list = []
    for i, item in enumerate(batch_items):
        papers_list.append(f"{i}|{item['title'][:80]}|{item['keywords'][:150]}")

    papers_text = "\n".join(papers_list)

    # Extract profile summary
    profile_summary = user_profile.get('dynamic_analysis', {}).get('summary', 'General hydrology researcher')
    focus_areas = user_profile.get('dynamic_analysis', {}).get('focus_areas', [])

    # Build compact prompt
    taxonomy_text = format_taxonomy_for_prompt()

    prompt = f"""You are a Research Assistant for Prof. Chengming Li (Hydrology/Hydro-climatology).

USER PROFILE:
- Base: {user_profile.get('base_info', '')}
- Current Focus: {profile_summary}
- Key Areas: {', '.join(focus_areas)}

TASK: Classify each paper into TWO distinct tracks:
1. **Archive Track (ğŸ“š)**: Standard subject/method classification for retrieval
2. **Idea Lab Track (ğŸ’¡)**: Scientific question/mechanism classification for exploration

TAXONOMY (Use these paths or suggest logical sub-paths):
{taxonomy_text}

PAPERS (Format: ID|Title|Keywords):
{papers_text}

OUTPUT: JSON with format {{"0": {{"archive": "ğŸ“š Archive/Category/Subcategory", "idea": "ğŸ’¡ Idea Lab/Category/Subcategory"}}, "1": ..., ...}}
Return "Unclassified" if unsure. Use exact emoji prefixes and path separators (/).

JSON:"""

    try:
        response = client.models.generate_content(
            model=ai_model,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )

        # Validate response exists and has text attribute
        if not response:
            print(f"   âŒ AIè¿”å›ç©ºå“åº”")
            return {}

        if not hasattr(response, 'text') or not response.text:
            print(f"   âŒ AIå“åº”ç¼ºå°‘æ–‡æœ¬å†…å®¹")
            return {}

        # Validate JSON structure
        try:
            result = json.loads(response.text)
        except json.JSONDecodeError as json_err:
            print(f"   âŒ AIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON: {json_err}")
            print(f"   ğŸ“„ å“åº”å†…å®¹å‰100å­—ç¬¦: {response.text[:100]}")
            return {}

        # Validate result is a dictionary
        if not isinstance(result, dict):
            print(f"   âŒ AIè¿”å›çš„JSONä¸æ˜¯å­—å…¸æ ¼å¼: {type(result)}")
            return {}

        # Validate each entry has the expected structure
        valid_results = {}
        for key, value in result.items():
            if not isinstance(value, dict):
                print(f"   âš ï¸  è·³è¿‡æ— æ•ˆçš„åˆ†ç±»é¡¹ {key}: å€¼ä¸æ˜¯å­—å…¸")
                continue

            # Check for required fields
            if 'archive' not in value and 'idea' not in value:
                print(f"   âš ï¸  è·³è¿‡æ— æ•ˆçš„åˆ†ç±»é¡¹ {key}: ç¼ºå°‘'archive'æˆ–'idea'å­—æ®µ")
                continue

            # Validate archive and idea are strings
            archive = value.get('archive', '')
            idea = value.get('idea', '')

            if not isinstance(archive, str):
                print(f"   âš ï¸  åˆ†ç±»é¡¹ {key}: 'archive'ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                archive = "Unclassified"

            if not isinstance(idea, str):
                print(f"   âš ï¸  åˆ†ç±»é¡¹ {key}: 'idea'ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                idea = "Unclassified"

            valid_results[key] = {'archive': archive, 'idea': idea}

        if not valid_results:
            print(f"   âš ï¸  AIåˆ†ç±»ç»“æœæ²¡æœ‰æœ‰æ•ˆé¡¹ç›®")
            return {}

        print(f"   ğŸ¤– AIæˆåŠŸåˆ†ç±» {len(valid_results)} ç¯‡è®ºæ–‡")
        return valid_results

    except Exception as e:
        error_msg = str(e)
        if 'timeout' in error_msg.lower() or 'timed out' in error_msg.lower():
            print(f"   âŒ AIè¯·æ±‚è¶…æ—¶: {e}")
        elif 'rate limit' in error_msg.lower() or 'quota' in error_msg.lower():
            print(f"   âŒ AI APIé€Ÿç‡é™åˆ¶æˆ–é…é¢ä¸è¶³: {e}")
        elif 'model' in error_msg.lower() or 'not found' in error_msg.lower():
            print(f"   âŒ AIæ¨¡å‹é”™è¯¯: {e}")
        elif 'api key' in error_msg.lower() or 'authentication' in error_msg.lower():
            print(f"   âŒ AI APIå¯†é’¥é”™è¯¯: {e}")
        else:
            print(f"   âŒ AIåˆ†ç±»é”™è¯¯: {e}")
        return {}

# ================= 6. Item Processing =================

def add_item_to_collections(zot, item, collection_keys):
    """Add item to multiple collections and mark with tag"""
    if not collection_keys:
        return

    # Filter out fake keys
    valid_keys = [k for k in collection_keys if not k.startswith("fake_")]

    if not valid_keys:
        return

    if DRY_RUN:
        return

    try:
        # item åº”ä¸º key å­—ç¬¦ä¸²
        item_key = item if isinstance(item, str) else item.get('key')
        if not item_key:
            print(f"      âŒ Invalid item key")
            return

        # è·å–å½“å‰æ¡ç›®æ•°æ®ï¼ˆzot.item å¯èƒ½è¿”å› list æˆ– dictï¼‰
        item_data_raw = zot.item(item_key)
        if isinstance(item_data_raw, list):
            if not item_data_raw:
                print(f"      âŒ Could not fetch item data for key: {item_key}")
                return
            item_data = item_data_raw[0]
        elif isinstance(item_data_raw, dict):
            item_data = item_data_raw
        else:
            print(f"      âŒ Could not fetch item data for key: {item_key}")
            return
        current_colls = item_data.get('collections', [])

        # Add to each collection if not already there
        added_count = 0
        for target_key in valid_keys:
            if target_key not in current_colls:
                zot.addto_collection(target_key, item_key)
                added_count += 1

        if added_count > 0:
            print(f"      âœ… Added to {added_count} collection(s)")

        # Add tag
        current_tags = item_data.get('tags', [])
        tag_names = [t.get('tag', '') for t in current_tags]

        if AUTO_TAG_NAME not in tag_names:
            current_tags.append({'tag': AUTO_TAG_NAME})
            item_data['tags'] = current_tags
            # ä¿è¯ update_item ä¼ å…¥ dict
            zot.update_item(item_data)
            print(f"      ğŸ·ï¸  Tagged: {AUTO_TAG_NAME}")

    except Exception as e:
        print(f"      âŒ Failed to add to collections: {e}")

# ================= 7. Main Processing =================

def main():
    print("=" * 70)
    print("ğŸ¤– Zotero AI Paper Organizer - Dual-Track Classification System")
    print("=" * 70)
    print(f"Mode: {'ğŸ§ª DRY RUN (Test Mode)' if DRY_RUN else 'ğŸš€ LIVE MODE'}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Target Collection: {TARGET_COLLECTION_PATH or 'Entire Library'}")
    print("=" * 70)

    # Initialize
    zot = zotero.Zotero(config.LIBRARY_ID, config.LIBRARY_TYPE, config.API_KEY)

    # Load user profile
    user_profile = load_user_profile()

    # Load or refresh cache
    cache = load_cache()
    if not cache:
        cache = refresh_cache_from_zotero(zot)
    else:
        print(f"âœ… Loaded {len(cache)} collections from cache")

    # Find target collection if specified
    target_coll_key = None
    if TARGET_COLLECTION_PATH:
        print(f"\nğŸ“‚ Looking for collection: {TARGET_COLLECTION_PATH}")
        target_coll_key = find_collection_by_path(zot, TARGET_COLLECTION_PATH, cache)

        if not target_coll_key:
            print("âŒ Target collection not found. Exiting.")
            return

        print(f"âœ… Found collection (Key: {target_coll_key})")

    # Fetch items to process
    print("\nğŸ” Fetching items...")

    # åˆ†é¡µè¯»å–å…¨éƒ¨ items
    def fetch_all_items(zot, target_coll_key=None, tag=None):
        all_items = []
        start = 0
        page_size = 100
        while True:
            if target_coll_key:
                page = zot.collection_items(target_coll_key, tag=tag, limit=page_size, start=start)
            else:
                page = zot.items(tag=tag, limit=page_size, start=start)
            if not page:
                break
            all_items.extend(page)
            if len(page) < page_size:
                break
            start += page_size
        return all_items

    if target_coll_key:
        items = fetch_all_items(zot, target_coll_key, tag='gemini_read')
        print(f"   Scope: Collection '{TARGET_COLLECTION_PATH}'")
    else:
        items = fetch_all_items(zot, tag='gemini_read')
        print(f"   Scope: Entire library")

    print(f"   Found: {len(items)} items with 'gemini_read' tag")

    # Filter items that haven't been organized yet
    todo_items = []

    for item in items:
        # Skip if already has auto_organized tag
        tags = [t.get('tag', '') for t in item['data'].get('tags', [])]
        if AUTO_TAG_NAME in tags:
            continue

        # Get AI-generated note
        children = zot.children(item['key'])
        keywords = ""

        for child in children:
            if child['data']['itemType'] == 'note':
                note_content = child['data'].get('note', '')
                extracted = extract_keywords_from_note(note_content)
                if extracted:
                    keywords = extracted
                    break

        if keywords:
            todo_items.append({
                'key': item['key'],
                'data': item['data'],
                'title': item['data'].get('title', 'Untitled'),
                'keywords': keywords
            })


    print(f"âœ… Items to organize: {len(todo_items)}")

    # ================= ç»¼åˆåˆ†ææ‰€æœ‰ç¬”è®°å†…å®¹ =================
    if todo_items:
        print("\nğŸ” ç»¼åˆåˆ†ææ‰€æœ‰ AI ç¬”è®°å†…å®¹ï¼ˆå…³é”®è¯ç»Ÿè®¡ï¼‰...")
        from collections import Counter
        import re
        all_notes = []
        for item in items:
            children = zot.children(item['key'])
            for child in children:
                if child['data']['itemType'] == 'note':
                    note_content = child['data'].get('note', '')
                    # å»é™¤ HTML æ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬
                    text = re.sub(r'<[^>]+>', '', note_content)
                    all_notes.append(text)
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        all_text = '\n'.join(all_notes)
        # ç®€å•åˆ†è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼ŒæŒ‰éå­—æ¯æ•°å­—åˆ†å‰²ï¼‰
        words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]{2,}', all_text)
        # ç»Ÿè®¡é«˜é¢‘è¯
        counter = Counter(words)
        most_common = counter.most_common(30)
        print("\nğŸ“ˆ é«˜é¢‘å…³é”®è¯ç»Ÿè®¡ï¼ˆTop 30ï¼‰ï¼š")
        for word, count in most_common:
            print(f"   {word}: {count}")
        print("\nï¼ˆå¦‚éœ€æ›´å¤æ‚åˆ†æå¯è¿›ä¸€æ­¥æ‰©å±•ï¼‰")

    if not todo_items:
        print("\nğŸ‰ No items to process!")
        return

    # Process in batches
    total_batches = math.ceil(len(todo_items) / BATCH_SIZE)
    organized_count = 0

    for batch_idx in range(total_batches):
        batch = todo_items[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]

        print(f"\nğŸ“¦ Batch {batch_idx + 1}/{total_batches} ({len(batch)} items)")
        print("-" * 70)

        # AI dual classification
        print("   ğŸ§  Calling AI for dual-track classification...")
        classifications = ai_dual_classify_batch(batch, user_profile, config.AI_MODEL, config.AI_API_KEY)

        # Process each classification
        for idx_str, paths in classifications.items():
            try:
                idx = int(idx_str)
                if idx >= len(batch):
                    continue

                item_info = batch[idx]
                print(f"\n   [{idx}] {item_info['title'][:60]}...")

                # Extract paths
                archive_path = paths.get('archive', '')
                idea_path = paths.get('idea', '')

                print(f"      ğŸ“š Archive: {archive_path}")
                print(f"      ğŸ’¡ Idea Lab: {idea_path}")

                # Collect keys
                collection_keys = []

                # Ensure archive path exists
                if archive_path and archive_path != "Unclassified":
                    archive_key = ensure_collection_path(zot, archive_path, cache)
                    if archive_key:
                        collection_keys.append(archive_key)

                # Ensure idea lab path exists
                if idea_path and idea_path != "Unclassified":
                    idea_key = ensure_collection_path(zot, idea_path, cache)
                    if idea_key:
                        collection_keys.append(idea_key)

                # Add to collections
                if collection_keys:
                    if DRY_RUN:
                        print(f"      [Dry Run] Would add to {len(collection_keys)} collection(s)")
                    else:
                        add_item_to_collections(zot, item_info['key'], collection_keys)

                    organized_count += 1

            except Exception as e:
                print(f"   âŒ Error processing item {idx_str}: {e}")

        # Rate limiting
        if batch_idx < total_batches - 1:
            print("\n   â¸ï¸  Waiting 3s before next batch...")
            time.sleep(3)

    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š Summary")
    print("=" * 70)
    print(f"Total items processed: {len(todo_items)}")
    print(f"Successfully organized: {organized_count}")
    print(f"Mode: {'DRY RUN (no changes made)' if DRY_RUN else 'LIVE MODE (changes saved)'}")
    print("=" * 70)
    print("\nğŸ‰ Done!")

    if DRY_RUN:
        print("\nğŸ’¡ Tip: Set DRY_RUN = False in organizer.py to actually organize items")

if __name__ == "__main__":
    main()
