#!/usr/bin/env python3
"""
Zotero AI-Powered Paper Organizer - Dual-Track Classification System
=====================================================================

Automatically classifies papers into TWO simultaneous structures:
1. Track A (ğŸ“š Archive): Subject-based hierarchy for easy retrieval
2. Track B (ğŸ’¡ Idea Lab): Question-based hierarchy for scientific exploration

Key Features:
- Dual-track classification (one paper â†’ two collections)
- Profile-driven AI prompts (loads user_profile.json from profiler.py)
- Local caching to minimize API calls
- Batch processing for efficiency
- Smart tagging to prevent duplicate processing
- Targeted collection processing support

Author: Prof. Chengming Li (SCUT)
"""

import time
import json
import math
import os
import sys
from pyzotero import zotero
from google import genai

# é…ç½®åŠ è½½
from config_loader import get_config_from_args_or_interactive

config = get_config_from_args_or_interactive()
if config is None:
    print("[ERROR] æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œç¨‹åºé€€å‡º")
    sys.exit(1)

# ================= 1. Configuration =================

# Target collection to process (None = process entire library)
TARGET_COLLECTION_PATH = getattr(config, 'TARGET_COLLECTION_PATH', None)

# Processing settings
DRY_RUN = False                      # True = test mode, False = actually move items
BATCH_SIZE = 5                      # Number of papers to classify per API call
AUTO_TAG_NAME = "auto_organized"    # Tag to prevent duplicate processing
PROFILE_FILE = 'user_profile.json'  # User profile generated by profiler.py
CACHE_FILE = "collections_cache.json"  # Local cache for collection IDs

# ================= 2. Dual-Track Taxonomy Definition =================

# Comprehensive Dual-Track Taxonomy for Hydrology Research
PREFERRED_TAXONOMY = {
    "Track_A_Archive": {
        "description": "Standard disciplinary classification for systematic retrieval",
        "emoji": "ğŸ“š",
        "structure": {
            "Processes": [
                "Evapotranspiration (ET)",
                "Runoff & Streamflow",
                "Soil Moisture",
                "Groundwater",
                "Cryosphere (Snow & Glacier)"
            ],
            "Hazards": [
                "Drought/Flash Drought",
                "Flood",
                "Compound Events/DFA",
                "Heatwaves & Extremes"
            ],
            "Methodology": [
                "Remote Sensing/Retrieval",
                "Deep Learning (LSTM_CNN)",
                "Data Fusion",
                "Triple Collocation/Uncertainty",
                "Statistical Methods"
            ],
            "Applications": [
                "Water Resources Management",
                "Climate Change Impact",
                "Agricultural Systems",
                "Early Warning Systems"
            ]
        }
    },
    "Track_B_Idea_Lab": {
        "description": "Taste-driven classification based on scientific questions and mechanisms",
        "emoji": "ğŸ’¡",
        "structure": {
            "Mechanism": [
                "Abrupt Transitions/Phase Change",
                "Land-Atmosphere Coupling",
                "Feedback Loops & Cascades",
                "Threshold Behavior"
            ],
            "Data Philosophy": [
                "Signal Purification/Uncertainty",
                "Scale Issues (Spatial_Temporal)",
                "Multi-Source Integration",
                "Data Quality & Validation"
            ],
            "Modeling": [
                "Physics-Informed AI",
                "Causal Inference",
                "Hybrid Modeling (Process+ML)",
                "Ensemble Methods"
            ],
            "Dynamics": [
                "Non-linearity & Complexity",
                "Tipping Points",
                "Drought-Flood Transitions",
                "Rapid Onset Events"
            ],
            "Coupling": [
                "Vegetation-Water Interaction",
                "Energy-Water Nexus",
                "Human-Nature Systems",
                "Multi-Sphere Coupling"
            ]
        }
    }
}

# ================= 3. Profile & Cache Management =================

def load_user_profile():
    """Load user profile generated by profiler.py"""
    if os.path.exists(PROFILE_FILE):
        try:
            with open(PROFILE_FILE, 'r', encoding='utf-8') as f:
                profile = json.load(f)
            print(f"[OK] Loaded user profile from {PROFILE_FILE}")
            return profile
        except Exception as e:
            print(f"[WARN] Failed to load profile: {e}")

    # Default profile if file doesn't exist
    print("[INFO] Using default hydrology profile")
    return {
        "base_info": "Default Hydrology Researcher Profile",
        "dynamic_analysis": {
            "summary": "Focus on hydrological extremes and AI applications",
            "focus_areas": ["ET", "Flash Drought", "DFA", "Deep Learning"]
        }
    }

def load_cache():
    """Load collection ID cache from disk"""
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[WARN] Failed to load cache: {e}")
    return {}

def save_cache(cache):
    """Save collection ID cache to disk"""
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[WARN] Failed to save cache: {e}")

def get_analysis_results_file(collection_path=None):
    """
    æ ¹æ®collectionè·¯å¾„ç”Ÿæˆæ·±åº¦åˆ†æç»“æœæ–‡ä»¶å
    
    Args:
        collection_path: Collectionè·¯å¾„ï¼ˆå¦‚ "Precipitation"ï¼‰ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨ TARGET_COLLECTION_PATH
    
    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ "analysis_json/Precipitation_deep_analysis_results.json"ï¼‰
    """
    if collection_path is None:
        collection_path = TARGET_COLLECTION_PATH
    
    if collection_path is None:
        return "analysis_json/global_deep_analysis_results.json"
    
    # å°†è·¯å¾„è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ–‡ä»¶å
    # æ›¿æ¢ '/' ä¸º '_'ï¼Œç©ºæ ¼æ›¿æ¢ä¸º '_'
    safe_name = collection_path.replace('/', '_').replace(' ', '_')
    # ç§»é™¤å…¶ä»–å¯èƒ½çš„ä¸å®‰å…¨å­—ç¬¦
    safe_name = ''.join(c if c.isalnum() or c in ('_', '-') else '_' for c in safe_name)
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    while '__' in safe_name:
        safe_name = safe_name.replace('__', '_')
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
    safe_name = safe_name.strip('_')
    
    return f"analysis_json/{safe_name}_deep_analysis_results.json"

def load_analysis_results():
    """Load previous analysis results from disk"""
    file_path = get_analysis_results_file()
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[WARN] Failed to load analysis results: {e}")
    return {}

def save_analysis_results(results):
    """Save analysis results to disk"""
    file_path = get_analysis_results_file()
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"[WARN] Failed to save analysis results: {e}")

def refresh_cache_from_zotero(zot):
    """Fetch all collections from Zotero and build cache"""
    print("[REFRESH] Refreshing collection cache from Zotero...")
    try:
        # è·å–æ‰€æœ‰ collectionsï¼ˆåŒ…æ‹¬ç”¨æˆ·åˆ›å»ºçš„å’Œç³»ç»Ÿé»˜è®¤çš„ï¼‰
        # ä½¿ç”¨åˆ†é¡µæ–¹å¼ç¡®ä¿è·å–æ‰€æœ‰ collections
        all_colls = []
        start = 0
        page_size = 100
        
        while True:
            try:
                page = zot.collections(limit=page_size, start=start)
                if not page:
                    break
                all_colls.extend(page)
                
                # å¦‚æœè¿”å›çš„æ•°é‡å°‘äº page_sizeï¼Œè¯´æ˜å·²ç»è·å–å®Œæ‰€æœ‰æ•°æ®
                if len(page) < page_size:
                    break
                
                start += page_size
            except Exception as page_error:
                print(f"   [WARN] Error fetching collections page (start={start}): {page_error}")
                break
        
        if not all_colls:
            print("   [WARN] No collections returned from Zotero API")
            return {}
        
        print(f"   [FETCH] Fetched {len(all_colls)} collections from Zotero API (using pagination)")
        
        cache = {}

        # Build a mapping from key to collection data for path reconstruction
        key_to_coll = {c['key']: c for c in all_colls}

        def build_full_path(coll):
            path = [coll['data']['name']]
            parent = coll['data'].get('parentCollection', None)
            while parent:
                parent_coll = key_to_coll.get(parent)
                if not parent_coll:
                    break
                path.insert(0, parent_coll['data']['name'])
                parent = parent_coll['data'].get('parentCollection', None)
            return '/'.join(path)

        # æ”¶é›†æ‰€æœ‰é¡¶çº§ collection åç§°ç”¨äºè°ƒè¯•
        top_level_names = []
        all_collection_names = []  # æ‰€æœ‰ collection åç§°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        for c in all_colls:
            coll_name = c['data']['name']
            all_collection_names.append(coll_name)
            full_path = build_full_path(c)
            key = c['key']
            parent = c['data'].get('parentCollection', None)
            cache[full_path] = {
                'key': key,
                'parent': parent if parent else None
            }
            
            # æ”¶é›†é¡¶çº§ collectionï¼ˆæ²¡æœ‰çˆ¶ collection çš„ï¼‰
            if not parent:
                top_level_names.append(coll_name)

        save_cache(cache)
        print(f"[OK] Cached {len(cache)} collections (by path)")
        
        # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰é¡¶çº§ collection åç§°ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
        if top_level_names:
            print(f"   [SEARCH] Top-level collections ({len(top_level_names)}ä¸ª): {', '.join(sorted(top_level_names))}")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å« "0-New"
        if '0-New' in all_collection_names:
            print(f"   [OK] Found '0-New' in collections!")
            # æ‰¾åˆ°æ‰€æœ‰åŒ…å« "0-New" çš„è·¯å¾„
            matching_paths = [p for p in cache.keys() if '0-New' in p]
            if matching_paths:
                print(f"   [SEARCH] Paths containing '0-New': {matching_paths[:10]}")
        else:
            print(f"   [WARN] '0-New' not found in collection names")
            print(f"   [INFO] All collection names (å‰50ä¸ª): {', '.join(sorted(set(all_collection_names))[:50])}")
        
        return cache
    except Exception as e:
        print(f"[ERROR] Failed to fetch collections: {e}")
        import traceback
        print(f"[DEBUG] Error details: {traceback.format_exc()}")
        return {}

# ================= 4. Collection Management =================

def find_collection_by_path(zot, collection_path, cache):
    """
    Find collection key by path (e.g., "Parent/Child")
    Uses cache first, falls back to Zotero API if needed
    
    Handles two cases:
    1. Collection name itself contains '/' (e.g., "5 Runoff/Streamflow")
    2. Nested path (e.g., "Parent/Child")
    """
    if not collection_path:
        return None

    # é¦–å…ˆå°è¯•ç›´æ¥åŒ¹é…å®Œæ•´è·¯å¾„ï¼ˆæœ‰äº› collection åç§°æœ¬èº«å°±åŒ…å« '/'ï¼‰
    if collection_path in cache:
        result_key = cache[collection_path]['key']
        print(f"   [OK] Found exact match: '{collection_path}' -> Key: {result_key[:8]}...")
        return result_key
    
    # å¦‚æœç›´æ¥åŒ¹é…å¤±è´¥ï¼Œåˆ·æ–°ç¼“å­˜åå†è¯•ä¸€æ¬¡
    if collection_path not in cache:
        print(f"   [WARN] Collection path '{collection_path}' not in cache, refreshing...")
        cache = refresh_cache_from_zotero(zot)
        if collection_path in cache:
            result_key = cache[collection_path]['key']
            print(f"   [OK] Found exact match after refresh: '{collection_path}' -> Key: {result_key[:8]}...")
            return result_key

    # å¦‚æœç›´æ¥åŒ¹é…ä»ç„¶å¤±è´¥ï¼Œå°è¯•æŒ‰è·¯å¾„åˆ†å‰²çš„æ–¹å¼æŸ¥æ‰¾ï¼ˆç”¨äºåµŒå¥—è·¯å¾„ï¼‰
    parts = [p.strip() for p in collection_path.split('/') if p.strip()]
    if not parts:
        return None

    # è°ƒè¯•ï¼šæ‰“å°æŸ¥æ‰¾çš„è·¯å¾„éƒ¨åˆ†
    print(f"   [SEARCH] Trying nested path search, path parts: {parts}")

    # Build full path step by step
    current_path = ''
    current_parent = None
    for i, part in enumerate(parts):
        current_path = '/'.join(parts[:i+1])
        print(f"   [SEARCH] Checking path: '{current_path}'")
        
        if current_path not in cache:
            print(f"   [WARN] Collection path '{current_path}' not in cache, refreshing...")
            cache = refresh_cache_from_zotero(zot)
            
            # è°ƒè¯•ï¼šæ‰“å°æ‰€æœ‰ç¼“å­˜ä¸­çš„è·¯å¾„ï¼ˆå‰30ä¸ªï¼‰
            cached_paths = list(cache.keys())[:30]
            print(f"   [SEARCH] Available cached paths (å‰30ä¸ª): {cached_paths}")
            
            if current_path not in cache:
                # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼šæŸ¥æ‰¾åŒ…å«è¯¥éƒ¨åˆ†çš„è·¯å¾„
                matching_paths = [p for p in cache.keys() if part in p or p.endswith('/' + part) or p == part]
                if matching_paths:
                    print(f"   [INFO] Found similar paths: {matching_paths[:10]}")
                
                print(f"   [ERROR] Collection path '{current_path}' not found")
                print(f"   [INFO] Tip: è¯·æ£€æŸ¥ collection åç§°æ˜¯å¦æ­£ç¡®ï¼Œæ³¨æ„å¤§å°å†™å’Œç‰¹æ®Šå­—ç¬¦")
                return None
        current_parent = cache[current_path]['key']
        print(f"   [OK] Found: '{current_path}' -> Key: {current_parent[:8]}...")
    return current_parent

def ensure_collection_path(zot, path, cache):
    """
    Create collection path if it doesn't exist
    Returns the final collection key
    """
    if not path or path == "Unclassified":
        return None

    parts = [p.strip() for p in path.split('/') if p.strip()]
    parent_key = None
    current_path = ''

    for i, part in enumerate(parts):
        current_path = '/'.join(parts[:i+1])
        # Check if collection exists in cache by full path
        if current_path in cache:
            coll_info = cache[current_path]
            found_key = coll_info['key']
        else:
            found_key = None

        # Create if doesn't exist
        if not found_key:
            if DRY_RUN:
                print(f"      [Dry Run] Would create: {current_path}")
                found_key = f"fake_{part}_{parent_key or 'root'}"
            else:
                print(f"      [CREATE] Creating collection: {current_path}")
                try:
                    payload = {'name': part}
                    if parent_key:
                        payload['parentCollection'] = parent_key

                    res = zot.create_collections([payload])

                    # Validate response exists
                    if not res:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: APIè¿”å›ç©ºå“åº”")
                        return None

                    # Validate response structure
                    if not isinstance(res, dict):
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: APIå“åº”æ ¼å¼ä¸æ­£ç¡® ({type(res)})")
                        return None

                    # Check for successful creation
                    if 'successful' not in res:
                        if 'failed' in res:
                            failed_info = res['failed']
                            print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: {failed_info}")
                        else:
                            print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: å“åº”ä¸­ç¼ºå°‘'successful'å­—æ®µ")
                        return None

                    # Extract key from response
                    success_dict = res['successful']

                    # Validate success_dict is not empty
                    if not success_dict:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: 'successful'å­—æ®µä¸ºç©º")
                        return None

                    # Validate success_dict is a dictionary
                    if not isinstance(success_dict, dict):
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: 'successful'å­—æ®µæ ¼å¼ä¸æ­£ç¡® ({type(success_dict)})")
                        return None

                    # Extract the key from the first successful item
                    try:
                        first_value = list(success_dict.values())[0]
                        if not isinstance(first_value, dict):
                            print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: æˆåŠŸé¡¹æ ¼å¼ä¸æ­£ç¡®")
                            return None

                        if 'key' not in first_value:
                            print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: æˆåŠŸé¡¹ç¼ºå°‘'key'å­—æ®µ")
                            return None

                        found_key = first_value['key']

                        # Validate key is a non-empty string
                        if not found_key or not isinstance(found_key, str):
                            print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: è¿”å›çš„keyæ— æ•ˆ ({found_key})")
                            return None

                    except (IndexError, KeyError, TypeError) as extract_err:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: æ— æ³•æå–key - {extract_err}")
                        return None

                    # Update cache
                    cache[current_path] = {
                        'key': found_key,
                        'parent': parent_key
                    }
                    save_cache(cache)
                    print(f"      [OK] Created: {current_path} (Key: {found_key})")

                except Exception as e:
                    error_msg = str(e)
                    if '400' in error_msg:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥ (è¯·æ±‚æ ¼å¼é”™è¯¯): {e}")
                    elif '403' in error_msg or 'Forbidden' in error_msg:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥ (æƒé™ä¸è¶³): {e}")
                    elif 'timeout' in error_msg.lower() or 'timed out' in error_msg.lower():
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥ (è¯·æ±‚è¶…æ—¶): {e}")
                    else:
                        print(f"      [ERROR] åˆ›å»ºé›†åˆå¤±è´¥: {e}")
                    return None

        parent_key = found_key

    return parent_key

# ================= 5. AI Classification =================

def parse_note_content(note_content):
    """è§£æå®Œæ•´çš„AIé˜…è¯»æŠ¥å‘Šç¬”è®°å†…å®¹
    
    Returns:
        dict: åŒ…å«è§£æå‡ºçš„å„ä¸ªéƒ¨åˆ†çš„å­—å…¸
    """
    import re
    
    # Remove HTML tags
    text = re.sub(r'<[^>]+>', '', note_content)
    
    parsed = {
        'one_sentence_summary': '',
        'keywords': '',
        'title_zh': '',
        'title_en': '',
        'journal': '',
        'authors': '',
        'abstract': '',
        'research_question': '',
        'highlights': [],
        'key_literature': [],
        'data_methods': '',
        'weaknesses': [],
        'future_work': [],
        'full_text': text[:5000]  # ä¿ç•™å‰5000å­—ç¬¦ç”¨äºæ·±åº¦åˆ†æ
    }
    
    # æå–ä¸€å¥è¯æ€»ç»“
    summary_patterns = [
        r'ä¸€å¥è¯æ€»ç»“[^ï¼š:]*[ï¼š:]\s*([^\n]{5,100})',
        r'One-Sentence Summary[^ï¼š:]*[ï¼š:]\s*([^\n]{5,100})',
    ]
    for pattern in summary_patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            parsed['one_sentence_summary'] = match.group(1).strip()
            break
    
    # æå–å…³é”®è¯/è®ºæ–‡åˆ†ç±»
    keyword_patterns = [
        r'è®ºæ–‡åˆ†ç±»[^ï¼š:]*[ï¼š:]\s*([^\n]+)',
        r'Keywords[^ï¼š:]*[ï¼š:]\s*([^\n]+)',
        r'å…³é”®è¯[^ï¼š:]*[ï¼š:]\s*([^\n]+)',
    ]
    for pattern in keyword_patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            parsed['keywords'] = match.group(1).strip()
            break
    
    # æå–ç ”ç©¶é—®é¢˜
    rq_patterns = [
        r'ç ”ç©¶é—®é¢˜[^ï¼š:]*[ï¼š:]\s*([^\n]{10,200})',
        r'Research Question[^ï¼š:]*[ï¼š:]\s*([^\n]{10,200})',
    ]
    for pattern in rq_patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            parsed['research_question'] = match.group(1).strip()
            break
    
    # æå–æ ¸å¿ƒäº®ç‚¹ï¼ˆæœ€å¤š6ä¸ªï¼‰
    highlight_pattern = r'[Hh][1-6][ï¼š:]\s*([^\n]{5,50})'
    highlights = re.findall(highlight_pattern, text)
    parsed['highlights'] = highlights[:6]
    
    # æå–ä¸»è¦ä¸è¶³
    weakness_pattern = r'[1-6][ï¼‰)]\s*([^\n]{10,150})'
    weaknesses = re.findall(weakness_pattern, text)
    parsed['weaknesses'] = weaknesses[:6]
    
    # æå–æœªæ¥å·¥ä½œæ–¹å‘
    future_pattern = r'æ–¹å‘ [A-C][ï¼š:]\s*ç§‘å­¦é—®é¢˜[ï¼š:]\s*([^\n]{10,200})'
    future_works = re.findall(future_pattern, text)
    parsed['future_work'] = future_works[:3]
    
    return parsed

def extract_keywords_from_note(note_content):
    """Extract keywords/tags from AI-generated note (ä¿æŒå‘åå…¼å®¹)"""
    parsed = parse_note_content(note_content)
    if parsed['keywords']:
        return parsed['keywords']
    
    # Fallback: extract first 200 chars after "Summary" or "æ€»ç»“"
    import re
    text = re.sub(r'<[^>]+>', '', note_content)
    summary_match = re.search(r'(?:Summary|æ€»ç»“)[ï¼š:]\s*(.{1,200})', text, re.IGNORECASE)
    if summary_match:
        return summary_match.group(1).strip()
    
    return ""

def generate_report_filename(collection_path):
    """
    æ ¹æ®é›†åˆè·¯å¾„ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶å
    
    Args:
        collection_path: é›†åˆè·¯å¾„ï¼ˆå¦‚ "0 2025/12"ï¼‰ï¼ŒNone è¡¨ç¤ºæ•´ä¸ªåº“
    
    Returns:
        æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ "analysis_json/global_research_opportunities_report.json" æˆ– "analysis_json/0_2025_12_research_opportunities_report.json"ï¼‰
    """
    if collection_path is None:
        return "analysis_json/global_research_opportunities_report.json"
    
    # å°†è·¯å¾„è½¬æ¢ä¸ºæœ‰æ•ˆçš„æ–‡ä»¶å
    # æ›¿æ¢ '/' ä¸º '_'ï¼Œç©ºæ ¼ä¿ç•™æˆ–æ›¿æ¢ä¸º '_'
    safe_name = collection_path.replace('/', '_').replace(' ', '_')
    # ç§»é™¤å…¶ä»–å¯èƒ½çš„ä¸å®‰å…¨å­—ç¬¦
    safe_name = ''.join(c if c.isalnum() or c in ('_', '-') else '_' for c in safe_name)
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    while '__' in safe_name:
        safe_name = safe_name.replace('__', '_')
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ä¸‹åˆ’çº¿
    safe_name = safe_name.strip('_')
    
    return f"analysis_json/{safe_name}_research_opportunities_report.json"

def generate_global_analysis_report(analysis_results, user_profile, ai_model, ai_key):
    """
    ç”Ÿæˆå…¨å±€åˆ†ææŠ¥å‘Šï¼šæ±‡æ€»æ‰€æœ‰è®ºæ–‡çš„åˆ›æ–°ç‚¹å’Œç ”ç©¶æ–¹å‘
    
    Args:
        analysis_results: æ‰€æœ‰è®ºæ–‡çš„åˆ†æç»“æœå­—å…¸
        user_profile: ç”¨æˆ·ç”»åƒ
        ai_model: AIæ¨¡å‹åç§°
        ai_key: AI APIå¯†é’¥
    """
    if not analysis_results or len(analysis_results) == 0:
        print("   [WARN] No analysis results to summarize")
        return
    
    print(f"   [ANALYSIS] Analyzing {len(analysis_results)} papers for global insights...")
    
    try:
        client = genai.Client(api_key=ai_key)
        
        # æ”¶é›†æ‰€æœ‰è®ºæ–‡çš„å…³é”®ä¿¡æ¯
        papers_summary = []
        all_innovation_insights = []
        all_keywords = []
        all_archive_paths = []
        all_idea_paths = []
        
        for item_key, result in analysis_results.items():
            title = result.get('title', 'Unknown')
            innovation_insights = result.get('innovation_insights', [])
            keyword_assessment = result.get('keyword_assessment', {})
            archive_path = result.get('archive_path', '')
            idea_path = result.get('idea_path', '')
            
            papers_summary.append(f"- {title}")
            all_innovation_insights.extend(innovation_insights)
            all_keywords.append(keyword_assessment.get('suggested', keyword_assessment.get('current', '')))
            if archive_path:
                all_archive_paths.append(archive_path)
            if idea_path:
                all_idea_paths.append(idea_path)
        
        # æ„å»ºæç¤ºè¯
        profile_summary = user_profile.get('dynamic_analysis', {}).get('summary', 'General hydrology researcher')
        focus_areas = user_profile.get('dynamic_analysis', {}).get('focus_areas', [])
        
        prompt = f"""You are a Research Strategy Advisor for Prof. Chengming Li (Hydrology/Hydro-climatology).

USER PROFILE:
- Base: {user_profile.get('base_info', '')}
- Current Focus: {profile_summary}
- Key Areas: {', '.join(focus_areas)}

TASK: Analyze ALL {len(analysis_results)} papers that have been processed and provide:

1. **Research Landscape Overview**:
   - What are the main research themes across all papers?
   - What are the emerging research directions?
   - What are the common methodologies being used?

2. **Cross-Paper Research Opportunities**:
   - Identify 5-8 research opportunities that bridge multiple papers
   - For each opportunity, explain:
     * Which papers are related
     * Why this is a promising direction
     * What specific research questions could be addressed
     * Potential methodologies or approaches

3. **Research Gaps and Future Directions**:
   - What gaps exist in the current research landscape?
   - What are the most promising future research directions?
   - What interdisciplinary opportunities exist?

4. **Keyword Trends**:
   - What are the most common keywords/themes?
   - Are there any emerging keywords that appear frequently?
   - What keyword combinations suggest new research areas?

PAPERS PROCESSED ({len(analysis_results)} papers):
{chr(10).join(papers_summary[:50])}  # æœ€å¤šæ˜¾ç¤º50ç¯‡

INNOVATION INSIGHTS COLLECTED ({len(all_innovation_insights)} insights):
{chr(10).join([f"- {insight}" for insight in all_innovation_insights[:30]])}  # æœ€å¤šæ˜¾ç¤º30ä¸ª

ARCHIVE CLASSIFICATIONS:
{chr(10).join([f"- {path}" for path in list(set(all_archive_paths))[:20]])}  # å»é‡åæ˜¾ç¤ºå‰20ä¸ª

IDEA LAB CLASSIFICATIONS:
{chr(10).join([f"- {path}" for path in list(set(all_idea_paths))[:20]])}  # å»é‡åæ˜¾ç¤ºå‰20ä¸ª

OUTPUT: JSON format:
{{
  "research_landscape": {{
    "main_themes": ["theme1", "theme2", ...],
    "emerging_directions": ["direction1", "direction2", ...],
    "common_methodologies": ["method1", "method2", ...]
  }},
  "cross_paper_opportunities": [
    {{
      "title": "Opportunity title",
      "related_papers": ["paper title 1", "paper title 2"],
      "rationale": "Why this is promising",
      "research_questions": ["question1", "question2"],
      "methodologies": ["method1", "method2"]
    }},
    ...
  ],
  "research_gaps": [
    "gap description 1",
    "gap description 2",
    ...
  ],
  "future_directions": [
    "direction description 1",
    "direction description 2",
    ...
  ],
  "keyword_trends": {{
    "common_keywords": ["keyword1", "keyword2", ...],
    "emerging_keywords": ["keyword1", "keyword2", ...],
    "promising_combinations": ["combination1", "combination2", ...]
  }}
}}

JSON:"""
        
        print("   [AI] Calling AI for global analysis...")
        response = client.models.generate_content(
            model=ai_model,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        
        if not response or not hasattr(response, 'text') or not response.text:
            print("   [WARN] AIè¿”å›ç©ºå“åº”")
            return
        
        try:
            global_report = json.loads(response.text)
            
            # æ ¹æ® TARGET_COLLECTION_PATH ç”Ÿæˆæ–‡ä»¶å
            report_file = generate_report_filename(TARGET_COLLECTION_PATH)
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(global_report, f, indent=2, ensure_ascii=False)
            
            print(f"   [OK] Global analysis report saved to: {report_file}")
            
            # æ‰“å°æ‘˜è¦
            print("\n   [DEBUG] Global Analysis Summary:")
            print("   " + "-" * 68)
            
            if 'research_landscape' in global_report:
                landscape = global_report['research_landscape']
                if 'main_themes' in landscape:
                    print(f"\n   [THEMES] Main Research Themes ({len(landscape['main_themes'])}):")
                    for theme in landscape['main_themes'][:5]:
                        print(f"      - {theme}")
            
            if 'cross_paper_opportunities' in global_report:
                opportunities = global_report['cross_paper_opportunities']
                print(f"\n   [INFO] Cross-Paper Research Opportunities ({len(opportunities)}):")
                for i, opp in enumerate(opportunities[:3], 1):
                    print(f"      {i}. {opp.get('title', 'Unknown')}")
                    if 'related_papers' in opp:
                        print(f"         Related papers: {', '.join(opp['related_papers'][:2])}...")
            
            if 'future_directions' in global_report:
                directions = global_report['future_directions']
                print(f"\n   [DIRECTIONS] Future Research Directions ({len(directions)}):")
                for direction in directions[:3]:
                    print(f"      - {direction[:80]}...")
            
            print(f"\n   [SAVED] Full report saved to: {report_file}")
            
        except json.JSONDecodeError as e:
            print(f"   [WARN] Failed to parse global analysis JSON: {e}")
            print(f"   [PREVIEW] Response preview: {response.text[:200]}")
            
    except Exception as e:
        print(f"   [WARN] Failed to generate global analysis: {e}")
        import traceback
        print(f"   [DEBUG] Error details: {traceback.format_exc()}")

def format_taxonomy_for_prompt():
    """Convert taxonomy to compact text format for AI prompt"""
    lines = []

    for track_name, track_data in PREFERRED_TAXONOMY.items():
        emoji = track_data.get('emoji', '')
        desc = track_data.get('description', '')

        track_label = track_name.replace('_', ' ').replace('Track A', '[ARCHIVE] Archive').replace('Track B', '[IDEA] Idea Lab')
        lines.append(f"\n{track_label} ({desc}):")

        for category, subcategories in track_data['structure'].items():
            lines.append(f"\n  {category}:")
            for subcat in subcategories:
                lines.append(f"    - {subcat}")

    return "\n".join(lines)

def ai_dual_classify_batch(batch_items, user_profile, ai_model, ai_key):
    """
    æ·±åº¦åˆ†æå¹¶åˆ†ç±»è®ºæ–‡ï¼ŒåŒ…æ‹¬ï¼š
    1. åŒè½¨åˆ†ç±»ï¼ˆArchive + Idea Labï¼‰
    2. å…³é”®è¯è¯„ä¼°
    3. è®ºæ–‡å…³è”åˆ†æ
    4. åˆ›æ–°ç‚¹æ€è€ƒ

    Returns: dict with format {
        "0": {
            "archive": "path1", 
            "idea": "path2",
            "keyword_assessment": {"current": "...", "suggested": "...", "reason": "..."},
            "related_papers": ["title1", "title2"],
            "innovation_insights": ["insight1", "insight2"]
        }, 
        ...
    }
    """
    client = genai.Client(api_key=ai_key)

    # Format papers with full note content for deep analysis
    papers_list = []
    for i, item in enumerate(batch_items):
        note_content = item.get('note_content', '')
        parsed_note = parse_note_content(note_content) if note_content else {}
        
        paper_info = f"""Paper {i}:
Title: {item['title']}
Keywords (current): {item.get('keywords', '')}
One-sentence Summary: {parsed_note.get('one_sentence_summary', '')}
Research Question: {parsed_note.get('research_question', '')}
Highlights: {', '.join(parsed_note.get('highlights', [])[:3])}
Future Work: {', '.join(parsed_note.get('future_work', [])[:2])}
Note Content (excerpt): {parsed_note.get('full_text', '')[:1000]}
"""
        papers_list.append(paper_info)

    papers_text = "\n".join(papers_list)

    # Extract profile summary
    profile_summary = user_profile.get('dynamic_analysis', {}).get('summary', 'General hydrology researcher')
    focus_areas = user_profile.get('dynamic_analysis', {}).get('focus_areas', [])

    # Build compact prompt
    taxonomy_text = format_taxonomy_for_prompt()

    prompt = f"""You are a Research Assistant for Prof. Chengming Li (Hydrology/Hydro-climatology).

USER PROFILE:
- Base: {user_profile.get('base_info', '')}
- Current Focus: {profile_summary}
- Key Areas: {', '.join(focus_areas)}

TASK: For each paper, perform DEEP ANALYSIS and provide:
1. **Dual-Track Classification**:
   - Archive Track ([ARCHIVE]): Standard subject/method classification for retrieval
   - Idea Lab Track ([IDEA]): Scientific question/mechanism classification for exploration

2. **Keyword Assessment**:
   - Evaluate if current keywords are accurate and comprehensive
   - Suggest improvements if needed
   - Explain your reasoning

3. **Paper Relationships**:
   - Identify potential connections with other papers in the batch
   - Note: You can only see papers in this batch, so focus on relationships within the batch

4. **Innovation Insights**:
   - Based on the paper's content, research question, and future work suggestions
   - Think about: What new research directions or opportunities does this paper suggest?
   - What gaps or limitations could be addressed in future work?

TAXONOMY (Use these paths or suggest logical sub-paths):
{taxonomy_text}

PAPERS TO ANALYZE:
{papers_text}

OUTPUT: JSON with format:
{{
  "0": {{
    "archive": "[ARCHIVE] Archive/Category/Subcategory",
    "idea": "[IDEA] Idea Lab/Category/Subcategory",
    "keyword_assessment": {{
      "current": "current keywords from note",
      "suggested": "improved keywords if needed, or 'current keywords are good'",
      "reason": "brief explanation"
    }},
    "related_papers": ["Paper X title", "Paper Y title"],
    "innovation_insights": [
      "insight 1: what new research could be done",
      "insight 2: what gaps could be filled"
    ]
  }},
  "1": {{...}},
  ...
}}

Return "Unclassified" for classification if unsure. Use exact emoji prefixes and path separators (/).

JSON:"""

    try:
        response = client.models.generate_content(
            model=ai_model,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )

        # Validate response exists and has text attribute
        if not response:
            print(f"   [ERROR] AIè¿”å›ç©ºå“åº”")
            return {}

        if not hasattr(response, 'text') or not response.text:
            print(f"   [ERROR] AIå“åº”ç¼ºå°‘æ–‡æœ¬å†…å®¹")
            return {}

        # Validate JSON structure
        try:
            result = json.loads(response.text)
        except json.JSONDecodeError as json_err:
            print(f"   [ERROR] AIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON: {json_err}")
            print(f"   [PREVIEW] å“åº”å†…å®¹å‰100å­—ç¬¦: {response.text[:100]}")
            return {}

        # Validate result is a dictionary
        if not isinstance(result, dict):
            print(f"   [ERROR] AIè¿”å›çš„JSONä¸æ˜¯å­—å…¸æ ¼å¼: {type(result)}")
            return {}

        # Validate each entry has the expected structure
        valid_results = {}
        for key, value in result.items():
            if not isinstance(value, dict):
                print(f"   [WARN] è·³è¿‡æ— æ•ˆçš„åˆ†ç±»é¡¹ {key}: å€¼ä¸æ˜¯å­—å…¸")
                continue

            # Check for required fields
            if 'archive' not in value and 'idea' not in value:
                print(f"   [WARN] è·³è¿‡æ— æ•ˆçš„åˆ†ç±»é¡¹ {key}: ç¼ºå°‘'archive'æˆ–'idea'å­—æ®µ")
                continue

            # Validate archive and idea are strings
            archive = value.get('archive', '')
            idea = value.get('idea', '')

            if not isinstance(archive, str):
                print(f"   [WARN] åˆ†ç±»é¡¹ {key}: 'archive'ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                archive = "Unclassified"

            if not isinstance(idea, str):
                print(f"   [WARN] åˆ†ç±»é¡¹ {key}: 'idea'ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                idea = "Unclassified"

            # æå–æ·±åº¦åˆ†æç»“æœ
            keyword_assessment = value.get('keyword_assessment', {})
            related_papers = value.get('related_papers', [])
            innovation_insights = value.get('innovation_insights', [])

            valid_results[key] = {
                'archive': archive, 
                'idea': idea,
                'keyword_assessment': keyword_assessment if isinstance(keyword_assessment, dict) else {},
                'related_papers': related_papers if isinstance(related_papers, list) else [],
                'innovation_insights': innovation_insights if isinstance(innovation_insights, list) else []
            }

        if not valid_results:
            print(f"   [WARN] AIåˆ†ç±»ç»“æœæ²¡æœ‰æœ‰æ•ˆé¡¹ç›®")
            return {}

        print(f"   [AI] AIæˆåŠŸåˆ†ç±» {len(valid_results)} ç¯‡è®ºæ–‡")
        return valid_results

    except Exception as e:
        error_msg = str(e)
        if 'timeout' in error_msg.lower() or 'timed out' in error_msg.lower():
            print(f"   [ERROR] AIè¯·æ±‚è¶…æ—¶: {e}")
        elif 'rate limit' in error_msg.lower() or 'quota' in error_msg.lower():
            print(f"   [ERROR] AI APIé€Ÿç‡é™åˆ¶æˆ–é…é¢ä¸è¶³: {e}")
        elif 'model' in error_msg.lower() or 'not found' in error_msg.lower():
            print(f"   [ERROR] AIæ¨¡å‹é”™è¯¯: {e}")
        elif 'api key' in error_msg.lower() or 'authentication' in error_msg.lower():
            print(f"   [ERROR] AI APIå¯†é’¥é”™è¯¯: {e}")
        else:
            print(f"   [ERROR] AIåˆ†ç±»é”™è¯¯: {e}")
        return {}

# ================= 6. Item Fetching =================

def get_all_subcollection_keys(zot, parent_key, cache):
    """é€’å½’è·å–æ‰€æœ‰å­collectionçš„keysï¼ˆåŒ…æ‹¬å­å­collectionç­‰ï¼‰
    
    Args:
        zot: Zotero client instance
        parent_key: çˆ¶collectionçš„key
        cache: collectionç¼“å­˜å­—å…¸ï¼Œæ ¼å¼ä¸º {path: {'key': key, 'parent': parent_key}}
    
    Returns:
        List of collection keysï¼ˆåŒ…æ‹¬çˆ¶collectionæœ¬èº«å’Œæ‰€æœ‰å­collectionï¼‰
    """
    all_keys = [parent_key]  # åŒ…æ‹¬çˆ¶collectionæœ¬èº«
    
    # æ–¹æ³•1: ä»cacheä¸­æŸ¥æ‰¾æ‰€æœ‰parentå­—æ®µç­‰äºparent_keyçš„collectionï¼ˆç›´æ¥å­collectionï¼‰
    direct_children = []
    for path, info in cache.items():
        if info.get('parent') == parent_key:
            direct_children.append(info['key'])
    
    # é€’å½’å¤„ç†æ¯ä¸ªç›´æ¥å­collection
    for child_key in direct_children:
        if child_key not in all_keys:
            all_keys.append(child_key)
            # é€’å½’è·å–å­collectionçš„æ‰€æœ‰å­collection
            sub_keys = get_all_subcollection_keys(zot, child_key, cache)
            all_keys.extend([k for k in sub_keys if k not in all_keys])
    
    # æ–¹æ³•2: å¦‚æœcacheä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»Zotero APIè·å–æ‰€æœ‰collectionså¹¶æŸ¥æ‰¾
    if not direct_children:
        try:
            # è·å–æ‰€æœ‰collections
            all_colls = []
            start = 0
            page_size = 100
            while True:
                page = zot.collections(limit=page_size, start=start)
                if not page:
                    break
                all_colls.extend(page)
                if len(page) < page_size:
                    break
                start += page_size
            
            # æŸ¥æ‰¾æ‰€æœ‰parentCollectionç­‰äºparent_keyçš„collection
            for coll in all_colls:
                coll_parent = coll['data'].get('parentCollection')
                if coll_parent == parent_key:
                    child_key = coll['key']
                    if child_key not in all_keys:
                        all_keys.append(child_key)
                        # é€’å½’è·å–å­collectionçš„æ‰€æœ‰å­collection
                        sub_keys = get_all_subcollection_keys(zot, child_key, cache)
                        all_keys.extend([k for k in sub_keys if k not in all_keys])
        except Exception as e:
            print(f"   [WARN] Error fetching subcollections: {e}")
    
    return list(set(all_keys))  # å»é‡

def fetch_all_items(zot, target_coll_key=None, tag=None, cache=None, include_subcollections=True):
    """Fetch all items from Zotero with pagination support, optionally including subcollections
    
    Args:
        zot: Zotero client instance
        target_coll_key: Target collection key (None for entire library)
        tag: Filter by tag name (e.g., 'gemini_read')
        cache: Collection cache (required if include_subcollections=True)
        include_subcollections: If True, recursively fetch items from all subcollections
    
    Returns:
        List of items
    """
    # å¦‚æœæŒ‡å®šäº†collectionä¸”éœ€è¦åŒ…å«å­collectionï¼Œå…ˆè·å–æ‰€æœ‰ç›¸å…³collection keys
    collection_keys = [target_coll_key] if target_coll_key else None
    
    if target_coll_key and include_subcollections and cache:
        print(f"   [INFO] é€’å½’è·å–æ‰€æœ‰å­collectionä¸­çš„æ–‡çŒ®...")
        collection_keys = get_all_subcollection_keys(zot, target_coll_key, cache)
        print(f"   [INFO] æ‰¾åˆ° {len(collection_keys)} ä¸ªcollectionï¼ˆåŒ…æ‹¬ä¸»collectionå’Œæ‰€æœ‰å­collectionï¼‰")
    
    all_items = []
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šcollectionï¼Œä»æ•´ä¸ªåº“è·å–
    if not collection_keys:
        start = 0
        page_size = 100
        while True:
            try:
                page = zot.items(tag=tag, limit=page_size, start=start)
                if not page:
                    break
                all_items.extend(page)
                if len(page) < page_size:
                    break
                start += page_size
            except Exception as e:
                print(f"   [WARN] Error fetching items (start={start}): {e}")
                break
    else:
        # ä»æ¯ä¸ªcollectionä¸­è·å–items
        total_collections = len(collection_keys)
        for idx, coll_key in enumerate(collection_keys, 1):
            # æ¯10ä¸ªcollectionæˆ–å¼€å§‹æ—¶è¾“å‡ºè¿›åº¦
            if idx % 10 == 0 or idx == 1 or idx == total_collections:
                print(f"   [INFO] æ­£åœ¨è·å–items ({idx}/{total_collections} collections)...")
            
            start = 0
            page_size = 100
            while True:
                try:
                    page = zot.collection_items(coll_key, tag=tag, limit=page_size, start=start)
                    if not page:
                        break
                    all_items.extend(page)
                    if len(page) < page_size:
                        break
                    start += page_size
                except Exception as e:
                    print(f"   [WARN] Error fetching items from collection {coll_key[:8]}... (start={start}): {e}")
                    break
    
    # å»é‡ï¼ˆåŒä¸€ä¸ªitemå¯èƒ½åœ¨å¤šä¸ªcollectionä¸­ï¼‰
    seen_keys = set()
    unique_items = []
    for item in all_items:
        item_key = item.get('key', '')
        if item_key and item_key not in seen_keys:
            seen_keys.add(item_key)
            unique_items.append(item)
    
    return unique_items

# ================= 7. Item Processing =================

def validate_collection_keys(zot, collection_keys):
    """éªŒè¯collection keysæ˜¯å¦å­˜åœ¨ï¼Œè¿”å›æœ‰æ•ˆçš„keysåˆ—è¡¨"""
    if not collection_keys:
        return []
    
    valid_keys = []
    for key in collection_keys:
        # è·³è¿‡fake keys
        if key.startswith("fake_"):
            continue
        
        # å°è¯•è·å–collectionæ¥éªŒè¯å®ƒæ˜¯å¦å­˜åœ¨
        try:
            coll = zot.collection(key)
            if coll and isinstance(coll, dict) and coll.get('data'):
                valid_keys.append(key)
        except Exception:
            # Collectionä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ï¼Œè·³è¿‡
            pass
    
    return valid_keys

def add_item_to_collections(zot, item, collection_keys):
    """Add item to multiple collections and mark with tag"""
    if not collection_keys:
        return

    # Filter out fake keys
    new_keys = [k for k in collection_keys if not k.startswith("fake_")]

    if not new_keys:
        return

    if DRY_RUN:
        return

    try:
        # item åº”ä¸º key å­—ç¬¦ä¸²
        item_key = item if isinstance(item, str) else item.get('key')
        if not item_key:
            print(f"      [ERROR] Invalid item key")
            return

        # è·å–å½“å‰ item çš„å®Œæ•´æ•°æ®
        item_full = zot.item(item_key)
        if isinstance(item_full, list) and len(item_full) > 0:
            item_full = item_full[0]
        
        if not isinstance(item_full, dict):
            print(f"      [WARN] Could not get item: unexpected format")
            return
        
        # è·å– data å­—æ®µ
        item_data = item_full.get('data', {})
        if not isinstance(item_data, dict):
            print(f"      [WARN] Item data is not a dict: {type(item_data)}")
            return
        
        # è·å–å½“å‰çš„ collections
        current_collections = item_data.get('collections', [])
        if not isinstance(current_collections, list):
            current_collections = []
        
        # éªŒè¯æ‰€æœ‰collection keysï¼ˆåŒ…æ‹¬ç°æœ‰çš„å’Œæ–°çš„ï¼‰
        all_collection_keys = list(set(current_collections + new_keys))
        valid_collection_keys = validate_collection_keys(zot, all_collection_keys)
        
        # å¦‚æœæœ‰äº›keysæ— æ•ˆï¼Œè®°å½•æ¸…ç†ä¿¡æ¯
        invalid_keys = set(all_collection_keys) - set(valid_collection_keys)
        if invalid_keys:
            print(f"      [INFO] Removed {len(invalid_keys)} invalid collection key(s)")
        
        # åˆå¹¶æ–°çš„æœ‰æ•ˆ collectionsï¼ˆå»é‡ï¼‰
        updated_collections = list(set(valid_collection_keys))
        
        # æ£€æŸ¥å¹¶æ·»åŠ  tag
        current_tags = item_data.get('tags', [])
        if not isinstance(current_tags, list):
            current_tags = []
        
        # æ£€æŸ¥ tag æ˜¯å¦å·²å­˜åœ¨
        tag_exists = False
        for tag in current_tags:
            if isinstance(tag, dict):
                if tag.get('tag') == AUTO_TAG_NAME:
                    tag_exists = True
                    break
            elif isinstance(tag, str):
                if tag == AUTO_TAG_NAME:
                    tag_exists = True
                    break
        
        # å¦‚æœ tag ä¸å­˜åœ¨ï¼Œæ·»åŠ å®ƒ
        needs_update = False
        if not tag_exists:
            current_tags.append({'tag': AUTO_TAG_NAME})
            item_data['tags'] = current_tags
            needs_update = True
        
        # å¦‚æœ collections æœ‰å˜åŒ–ï¼Œä¹Ÿæ›´æ–°
        if set(updated_collections) != set(current_collections):
            item_data['collections'] = updated_collections
            needs_update = True
        
        # å¦‚æœæœ‰ä»»ä½•å˜åŒ–ï¼Œæ›´æ–° item
        if needs_update:
            item_full['data'] = item_data
            try:
                update_result = zot.update_item(item_full)
                if update_result:
                    collection_msg = f"Added to {len([k for k in new_keys if k in valid_collection_keys])} collection(s)" if set(updated_collections) != set(current_collections) else ""
                    tag_msg = f"Tagged: {AUTO_TAG_NAME}" if not tag_exists else ""
                    msgs = [m for m in [collection_msg, tag_msg] if m]
                    if msgs:
                        print(f"      [OK] {'; '.join(msgs)}")
                else:
                    print(f"      [WARN] Failed to update item")
            except Exception as update_error:
                import traceback
                error_msg = str(update_error)
                # æ£€æŸ¥æ˜¯å¦æ˜¯409é”™è¯¯ï¼ˆcollection not foundï¼‰
                if '409' in error_msg or 'not found' in error_msg.lower():
                    print(f"      [WARN] Collection not found, cleaning invalid keys and retrying...")
                    # å†æ¬¡éªŒè¯å¹¶æ¸…ç†
                    item_full_retry = zot.item(item_key)
                    if isinstance(item_full_retry, list) and len(item_full_retry) > 0:
                        item_full_retry = item_full_retry[0]
                    if isinstance(item_full_retry, dict):
                        item_data_retry = item_full_retry.get('data', {})
                        if isinstance(item_data_retry, dict):
                            current_collections_retry = item_data_retry.get('collections', [])
                            if isinstance(current_collections_retry, list):
                                valid_collections_retry = validate_collection_keys(zot, current_collections_retry)
                                item_data_retry['collections'] = valid_collections_retry
                                item_full_retry['data'] = item_data_retry
                                try:
                                    zot.update_item(item_full_retry)
                                    print(f"      [OK] Cleaned invalid collection keys")
                                except:
                                    print(f"      [ERROR] Failed to clean invalid keys")
                else:
                    print(f"      [ERROR] Failed to update item: {update_error}")
                    print(f"      [DEBUG] Error details: {traceback.format_exc()}")
        else:
            print(f"      [INFO] Item already in all target collections and has tag")

    except Exception as e:
        print(f"      [ERROR] Failed to add to collections: {e}")
        import traceback
        print(f"      [DEBUG] Error details: {traceback.format_exc()}")

# ================= 8. Main Processing =================

def main():
    print("=" * 70)
    print("[AI] Zotero AI Paper Organizer - Dual-Track Classification System")
    print("=" * 70)
    print(f"Mode: {'[DRY RUN] DRY RUN (Test Mode)' if DRY_RUN else '[LIVE] LIVE MODE'}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Target Collection: {TARGET_COLLECTION_PATH or 'Entire Library'}")
    print("=" * 70)

    # Initialize
    zot = zotero.Zotero(config.LIBRARY_ID, config.LIBRARY_TYPE, config.API_KEY)

    # Load user profile
    user_profile = load_user_profile()

    # Load or refresh cache
    cache = load_cache()
    if not cache:
        cache = refresh_cache_from_zotero(zot)
    else:
        print(f"[OK] Loaded {len(cache)} collections from cache")

    # Find target collection if specified
    target_coll_key = None
    if TARGET_COLLECTION_PATH:
        print(f"\n[SEARCH] Looking for collection: {TARGET_COLLECTION_PATH}")
        # å¦‚æœç¼“å­˜ä¸ºç©ºæˆ–å¾ˆå°ï¼Œå…ˆåˆ·æ–°ä¸€æ¬¡
        if not cache or len(cache) < 10:
            print("   [REFRESH] Cache is empty or small, refreshing...")
            cache = refresh_cache_from_zotero(zot)
        
        target_coll_key = find_collection_by_path(zot, TARGET_COLLECTION_PATH, cache)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ·æ–°ç¼“å­˜åå†è¯•ä¸€æ¬¡
        if not target_coll_key:
            print("   [REFRESH] Retrying after full cache refresh...")
            cache = refresh_cache_from_zotero(zot)
            target_coll_key = find_collection_by_path(zot, TARGET_COLLECTION_PATH, cache)

        if not target_coll_key:
            print("[ERROR] Target collection not found. Exiting.")
            print(f"[INFO] æç¤ºï¼šè¯·ç¡®è®¤ collection è·¯å¾„ '{TARGET_COLLECTION_PATH}' æ˜¯å¦æ­£ç¡®")
            print(f"[INFO] æ³¨æ„ï¼šè·¯å¾„åŒºåˆ†å¤§å°å†™ï¼Œè¯·æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ ¼æˆ–ç‰¹æ®Šå­—ç¬¦")
            return

        print(f"[OK] Found collection (Key: {target_coll_key})")

    # Fetch items to process
    print("\n[SEARCH] Fetching items...")

    if target_coll_key:
        items = fetch_all_items(zot, target_coll_key, tag='gemini_read', cache=cache, include_subcollections=True)
        print(f"   Scope: Collection '{TARGET_COLLECTION_PATH}' (åŒ…æ‹¬æ‰€æœ‰å­ç›®å½•)")
    else:
        items = fetch_all_items(zot, tag='gemini_read', include_subcollections=False)
        print(f"   Scope: Entire library")

    print(f"   Found: {len(items)} items with 'gemini_read' tag")

    # Filter items that haven't been organized yet
    todo_items = []
    skipped_organized = 0
    skipped_no_note = 0
    found_notes = 0

    for item in items:
        # Skip note items (notes themselves, not papers)
        item_type = item.get('data', {}).get('itemType', '')
        if item_type == 'note':
            # è¿™æ˜¯ç¬”è®°æœ¬èº«ï¼Œä¸æ˜¯è®ºæ–‡ï¼Œè·³è¿‡
            skipped_no_note += 1
            continue
        
        # Skip if already has auto_organized tag
        tags = [t.get('tag', '') for t in item.get('data', {}).get('tags', [])]
        if AUTO_TAG_NAME in tags:
            skipped_organized += 1
            continue

        # è·å–ç¬”è®°ï¼ˆå®Œæ•´å†…å®¹ï¼‰
        # ç­–ç•¥ï¼šä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„é€»è¾‘ï¼Œç¡®ä¿èƒ½æ‰¾åˆ°ç¬”è®°
        note_content = ""
        keywords = ""
        note_count = 0
        
        # è·å–item keyï¼ˆä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„ç®€å•æ–¹å¼ï¼‰
        item_key = item.get('key', '')
        
        if not item_key:
            if skipped_no_note < 3:
                title = item.get('data', {}).get('title', 'Unknown')[:50]
                print(f"      [WARN] Cannot get item key for '{title}...'")
            skipped_no_note += 1
            continue
        
        # å°è¯•è·å–å­é¡¹ï¼ˆç¬”è®°ï¼‰- ä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„é€»è¾‘
        children = []
        try:
            children = zot.children(item_key)
            if not isinstance(children, list):
                children = []
        except Exception as children_error:
            # æŸäº›itemç±»å‹å¯èƒ½ä¸æ”¯æŒchildrenæ–¹æ³•
            error_str = str(children_error)
            if "can only be called on" in error_str or "PDF, EPUB" in error_str:
                children = []
            else:
                # å…¶ä»–é”™è¯¯ï¼Œæ‰“å°è°ƒè¯•ä¿¡æ¯
                if skipped_no_note < 3:
                    title = item.get('data', {}).get('title', 'Unknown')[:50]
                    print(f"      [WARN] Error getting children for '{title}...': {children_error}")
                    print(f"         Item key: {item_key}")
                children = []
        
        # éå†æ‰€æœ‰å­é¡¹ï¼ŒæŸ¥æ‰¾ç¬”è®°ï¼ˆä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„é€»è¾‘ï¼‰
        for child in children:
            # ä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„ç®€å•æ–¹å¼è·å–ç±»å‹
            child_type = child.get('data', {}).get('itemType', 'unknown')
            
            if child_type == 'note':
                note_count += 1
                # ä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„ç®€å•æ–¹å¼è·å–ç¬”è®°å†…å®¹
                current_note = child.get('data', {}).get('note', '')
                if current_note and current_note.strip():
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•ˆç¬”è®°å°±ä½¿ç”¨ï¼ˆä¸è¦æ±‚æ ¼å¼ï¼‰
                    if not note_content:
                        note_content = current_note
                        # å°è¯•è§£æå…³é”®è¯ï¼ˆå¯é€‰ï¼‰
                        try:
                            parsed = parse_note_content(note_content)
                            keywords = parsed.get('keywords', '')
                        except:
                            keywords = ""
                        found_notes += 1
                        break  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæœ‰æ•ˆç¬”è®°å³å¯
        
        # è°ƒè¯•ï¼šå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç¬”è®°
        if not note_content:
            skipped_no_note += 1
            if skipped_no_note <= 3:  # åªæ‰“å°å‰3ä¸ª
                title = item.get('data', {}).get('title', 'Unknown')[:50]
                item_type = item.get('data', {}).get('itemType', 'unknown')
                print(f"      [WARN] Item '{title}...' (type: {item_type}) has NO notes")
                print(f"         Item key: {item_key}")
                print(f"         Children count: {len(children)}")
                if children:
                    # ä½¿ç”¨ä¸test_find_note.pyç›¸åŒçš„ç®€å•æ–¹å¼è·å–ç±»å‹
                    child_types = [c.get('data', {}).get('itemType', 'unknown') for c in children]
                    print(f"         Child types: {child_types}")

        # åªè¦æœ‰ç¬”è®°å†…å®¹ï¼Œå°±æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨
        if note_content:
            item_title = item.get('data', {}).get('title', 'Untitled') if isinstance(item, dict) else 'Untitled'
            todo_items.append({
                'key': item_key,
                'data': item.get('data', {}) if isinstance(item, dict) else {},
                'title': item_title,
                'keywords': keywords,
                'note_content': note_content  # ä¿å­˜å®Œæ•´ç¬”è®°å†…å®¹ç”¨äºæ·±åº¦åˆ†æ
            })

    # æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\n[STATS] Processing Statistics:")
    print(f"   Total items with 'gemini_read' tag: {len(items)}")
    print(f"   Skipped (already organized): {skipped_organized}")
    print(f"   Skipped (no notes): {skipped_no_note}")
    print(f"   Found notes: {found_notes}")
    print(f"[OK] Items to organize: {len(todo_items)}")
    
    # å¦‚æœæ‰€æœ‰itemséƒ½è¢«è·³è¿‡äº†ï¼Œæä¾›æ›´å¤šä¿¡æ¯
    if len(todo_items) == 0 and len(items) > 0:
        print(f"\n[INFO] Debug Info:")
        print(f"   - {skipped_organized} items already have '{AUTO_TAG_NAME}' tag")
        print(f"   - {skipped_no_note} items don't have any notes")
        if skipped_no_note > 0:
            print(f"   [INFO] Tip: è¯·ç¡®ä¿è®ºæ–‡æœ‰ç¬”è®°ï¼ˆä»»ä½•æ ¼å¼çš„ç¬”è®°éƒ½å¯ä»¥ï¼‰")

    if not todo_items:
        print("\n[INFO] No items to process!")
        return

    # Load previous analysis results
    analysis_results = load_analysis_results()
    
    # Process in batches
    total_batches = math.ceil(len(todo_items) / BATCH_SIZE)
    organized_count = 0

    for batch_idx in range(total_batches):
        batch = todo_items[batch_idx * BATCH_SIZE : (batch_idx + 1) * BATCH_SIZE]

        print(f"\n[BATCH] Batch {batch_idx + 1}/{total_batches} ({len(batch)} items)")
        print("-" * 70)

        # AI dual classification
        print("   [AI] Calling AI for dual-track classification...")
        classifications = ai_dual_classify_batch(batch, user_profile, config.AI_MODEL, config.AI_API_KEY)

        # Process each classification
        for idx_str, paths in classifications.items():
            try:
                idx = int(idx_str)
                if idx >= len(batch):
                    continue

                item_info = batch[idx]
                print(f"\n   [{idx}] {item_info['title'][:60]}...")

                # Extract paths
                archive_path = paths.get('archive', '')
                idea_path = paths.get('idea', '')

                print(f"      [ARCHIVE] Archive: {archive_path}")
                print(f"      [IDEA] Idea Lab: {idea_path}")

                # æ˜¾ç¤ºå…³é”®è¯è¯„ä¼°
                keyword_assessment = paths.get('keyword_assessment', {})
                if keyword_assessment:
                    current_kw = keyword_assessment.get('current', '')
                    suggested_kw = keyword_assessment.get('suggested', '')
                    reason = keyword_assessment.get('reason', '')
                    if suggested_kw and suggested_kw != current_kw:
                        print(f"      [KEYWORDS] å…³é”®è¯è¯„ä¼°:")
                        print(f"         å½“å‰: {current_kw}")
                        print(f"         å»ºè®®: {suggested_kw}")
                        if reason:
                            print(f"         åŸå› : {reason}")

                # æ˜¾ç¤ºè®ºæ–‡å…³è”
                related_papers = paths.get('related_papers', [])
                if related_papers:
                    print(f"      [RELATED] è®ºæ–‡å…³è”: {len(related_papers)} ç¯‡")
                    for rp in related_papers[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                        print(f"         - {rp[:50]}...")

                # æ˜¾ç¤ºåˆ›æ–°ç‚¹æ´å¯Ÿ
                innovation_insights = paths.get('innovation_insights', [])
                if innovation_insights:
                    print(f"      [INSIGHT] åˆ›æ–°ç‚¹æ´å¯Ÿ: {len(innovation_insights)} æ¡")
                    for insight in innovation_insights[:2]:  # æœ€å¤šæ˜¾ç¤º2ä¸ª
                        print(f"         - {insight[:80]}...")

                # Collect keys
                collection_keys = []

                # Ensure archive path exists
                if archive_path and archive_path != "Unclassified":
                    archive_key = ensure_collection_path(zot, archive_path, cache)
                    if archive_key:
                        collection_keys.append(archive_key)

                # Ensure idea lab path exists
                if idea_path and idea_path != "Unclassified":
                    idea_key = ensure_collection_path(zot, idea_path, cache)
                    if idea_key:
                        collection_keys.append(idea_key)

                # Add to collections
                if collection_keys:
                    if DRY_RUN:
                        print(f"      [Dry Run] Would add to {len(collection_keys)} collection(s)")
                    else:
                        add_item_to_collections(zot, item_info['key'], collection_keys)

                    organized_count += 1
                    
                    # ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
                    item_key = item_info['key']
                    analysis_results[item_key] = {
                        'title': item_info['title'],
                        'archive_path': archive_path,
                        'idea_path': idea_path,
                        'keyword_assessment': paths.get('keyword_assessment', {}),
                        'related_papers': paths.get('related_papers', []),
                        'innovation_insights': paths.get('innovation_insights', []),
                        'processed_time': time.strftime('%Y-%m-%d %H:%M:%S')
                    }
                    save_analysis_results(analysis_results)

            except Exception as e:
                print(f"   [ERROR] Error processing item {idx_str}: {e}")

        # Rate limiting
        if batch_idx < total_batches - 1:
            print("\n   [WAIT] Waiting 3s before next batch...")
            time.sleep(3)

    # Summary
    print("\n" + "=" * 70)
    print("ğŸ“Š Summary")
    print("=" * 70)
    print(f"Total items processed: {len(todo_items)}")
    print(f"Successfully organized: {organized_count}")
    print(f"Mode: {'DRY RUN (no changes made)' if DRY_RUN else 'LIVE MODE (changes saved)'}")
    analysis_results_file = get_analysis_results_file()
    print(f"Analysis results saved to: {analysis_results_file}")
    print("=" * 70)
    
    # å…¨å±€åˆ†æï¼šç”Ÿæˆæ±‡æ€»æŠ¥å‘Šï¼ˆå¦‚æœæœ‰æ–°å¤„ç†çš„è®ºæ–‡ï¼‰
    if organized_count > 0:
        print("\n" + "=" * 70)
        print("ğŸ”¬ å…¨å±€åˆ†æï¼šç”Ÿæˆç ”ç©¶æœºä¼šæ±‡æ€»æŠ¥å‘Š")
        print("=" * 70)
        # é‡æ–°åŠ è½½æ‰€æœ‰åˆ†æç»“æœï¼ˆåŒ…æ‹¬ä¹‹å‰å¤„ç†çš„ï¼‰
        all_analysis_results = load_analysis_results()
        if len(all_analysis_results) >= 3:  # è‡³å°‘3ç¯‡è®ºæ–‡æ‰è¿›è¡Œå…¨å±€åˆ†æ
            generate_global_analysis_report(all_analysis_results, user_profile, config.AI_MODEL, config.AI_API_KEY)
        else:
            print(f"   [INFO] å½“å‰åªæœ‰ {len(all_analysis_results)} ç¯‡è®ºæ–‡ï¼Œå»ºè®®è‡³å°‘3ç¯‡åå†è¿›è¡Œå…¨å±€åˆ†æ")
    
    print("\nğŸ‰ Done!")

    if DRY_RUN:
        print("\n[INFO] Tip: Set DRY_RUN = False in organizer.py to actually organize items")
    
    analysis_results_file = get_analysis_results_file()
    print(f"\n[INFO] æ·±åº¦åˆ†æç»“æœå·²ä¿å­˜åˆ° {analysis_results_file}")
    print(f"   æ‚¨å¯ä»¥æŸ¥çœ‹è¯¥æ–‡ä»¶æ¥äº†è§£ï¼š")
    print(f"   - å…³é”®è¯è¯„ä¼°å»ºè®®")
    print(f"   - è®ºæ–‡ä¹‹é—´çš„å…³è”")
    print(f"   - åˆ›æ–°ç‚¹æ´å¯Ÿå’Œç ”ç©¶æ–¹å‘")
    report_filename = generate_report_filename(TARGET_COLLECTION_PATH)
    print(f"\n[INFO] å…¨å±€ç ”ç©¶æœºä¼šæŠ¥å‘Šï¼š{report_filename}")
    print(f"   - è·¨è®ºæ–‡ç ”ç©¶æœºä¼š")
    print(f"   - ç ”ç©¶ç©ºç™½å’Œæœªæ¥æ–¹å‘")
    print(f"   - å…³é”®è¯è¶‹åŠ¿åˆ†æ")

if __name__ == "__main__":
    main()
