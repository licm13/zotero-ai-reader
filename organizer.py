import time
import json
import math
import os
from pyzotero import zotero
from google import genai
import config

# ================= é…ç½® =================
# å¦‚æœä¸ä¸º Noneï¼Œåªå¤„ç†è¯¥é›†åˆå†…çš„æ–‡çŒ®
TARGET_COLLECTION_PATH = getattr(config, 'TARGET_COLLECTION_PATH', None)
DRY_RUN = True              # âš ï¸ å…ˆå¼€å¯æµ‹è¯•æ¨¡å¼ï¼Œç¡®è®¤æ— è¯¯åæ”¹ä¸º False
BATCH_SIZE = 5              # æ‰¹å¤„ç†å¤§å°
AUTO_TAG_NAME = "auto_organized"
PROFILE_FILE = 'user_profile.json'
CACHE_FILE = 'collections_cache.json'

# ================= åŒè½¨åˆ†ç±»ä½“ç³» (é»˜è®¤ä¸ºæ‚¨çš„ç”»åƒå®šåˆ¶) =================
DEFAULT_TAXONOMY = {
    "Track_A_Archive": {
        "description": "Standard disciplinary classification for retrieval.",
        "structure": [
            "ğŸ“š Archive/Processes/Evapotranspiration",
            "ğŸ“š Archive/Processes/Runoff & Streamflow",
            "ğŸ“š Archive/Processes/Cryosphere (Snow_Glacier)",
            "ğŸ“š Archive/Hazards/Drought (Flash_Drought)",
            "ğŸ“š Archive/Hazards/Flood",
            "ğŸ“š Archive/Hazards/Compound_Events",
            "ğŸ“š Archive/Methodology/Remote_Sensing (Retrieval)",
            "ğŸ“š Archive/Methodology/Deep_Learning",
            "ğŸ“š Archive/Methodology/Data_Fusion"
        ]
    },
    "Track_B_Idea_Lab": {
        "description": "Taste-driven classification based on scientific questions and physical structures.",
        "structure": [
            "ğŸ’¡ Idea Lab/Mechanism/Abrupt_Transitions (Phase_Change)",
            "ğŸ’¡ Idea Lab/Mechanism/Land_Atmosphere_Coupling",
            "ğŸ’¡ Idea Lab/Data_Philosophy/Signal_Purification (Uncertainty)",
            "ğŸ’¡ Idea Lab/Data_Philosophy/Scale_Issues",
            "ğŸ’¡ Idea Lab/Modeling/Physics_AI_Fusion",
            "ğŸ’¡ Idea Lab/Modeling/Causal_Inference"
        ]
    }
}

# ================= åŠŸèƒ½å‡½æ•° =================

def load_profile():
    """åŠ è½½ profiler.py ç”Ÿæˆçš„åŠ¨æ€ç”»åƒ"""
    if os.path.exists(PROFILE_FILE):
        try:
            with open(PROFILE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {"base_info": "Use default hydrology profile."}

def load_collection_cache(zot):
    """åŠ è½½é›†åˆç¼“å­˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä» Zotero è·å–"""
    cache = {}
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
            print(f"ğŸ“¦ å·²åŠ è½½æœ¬åœ°é›†åˆç¼“å­˜ ({len(cache)} ä¸ª)")
            return cache
        except:
            print("âš ï¸ ç¼“å­˜æ–‡ä»¶æŸåï¼Œé‡æ–°è·å–...")
    
    print("ğŸŒ æ­£åœ¨ä» Zotero è·å–é›†åˆåˆ—è¡¨ (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
    try:
        # è·å–æ‰€æœ‰é›†åˆ (ç®€å•æ˜ å°„ name -> key)
        # æ³¨æ„ï¼šå¦‚æœæœ‰åŒåé›†åˆï¼Œè¿™é‡Œä¼šè¦†ç›–ã€‚å»ºè®®ä¿æŒé›†åˆåç§°å”¯ä¸€ã€‚
        colls = zot.collections()
        for c in colls:
            cache[c['data']['name']] = c['key']
        
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2)
        return cache
    except Exception as e:
        print(f"âŒ è·å–é›†åˆå¤±è´¥: {e}")
        return {}

def update_cache(name, key):
    """æ›´æ–°ç¼“å­˜"""
    try:
        cache = {}
        if os.path.exists(CACHE_FILE):
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        cache[name] = key
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2)
    except:
        pass

def find_collection_by_path_simple(zot, path_str):
    """(è¾…åŠ©) ç”¨äºæŸ¥æ‰¾ TARGET_COLLECTION_PATH"""
    if not path_str: return None
    name = path_str.split('/')[-1].strip()
    colls = zot.collections(q=name) # æœç´¢
    for c in colls:
        if c['data']['name'] == name:
            return c['key']
    return None

def extract_tags_from_note(zot, item_key):
    """ä»ç¬”è®°ä¸­æå– Keywords"""
    children = zot.children(item_key)
    for child in children:
        if child['data']['itemType'] == 'note':
            note = child['data']['note']
            import re
            clean = re.sub(r'<[^>]+>', '', note)
            match = re.search(r'(?:Keywords[â€“-]Tags|è®ºæ–‡åˆ†ç±»)[ï¼š:]\s*(.+)', clean, re.IGNORECASE)
            if match:
                return match.group(1).strip()
    return ""

def ai_dual_classify(batch_items, user_profile):
    """è°ƒç”¨ AI è¿›è¡ŒåŒè½¨åˆ†ç±»"""
    client = genai.Client(api_key=config.AI_API_KEY)
    
    papers_desc = []
    for i, item in enumerate(batch_items):
        papers_desc.append(f"Paper ID {i}: Title='{item['title']}', Keywords='{item['keywords']}'")
    papers_text = "\n".join(papers_desc)
    
    # åŠ¨æ€æ„å»ºåˆ†ç±»æ ‘æè¿°
    profile_summary = user_profile.get('dynamic_analysis', {}).get('summary', '')
    
    prompt = f"""
    ROLE: You are an expert Research Assistant for Prof. Chengming Li (Hydrology/AI).
    
    USER PROFILE:
    {user_profile.get('base_info', '')}
    {profile_summary}
    
    TASK: Classify the following papers into TWO distinct tracks:
    1. **Track A (Archive)**: The standard disciplinary folder (Subject/Method).
    2. **Track B (Idea Lab)**: The scientific question or "taste-based" folder (Mechanism/Philosophy).
    
    AVAILABLE TAXONOMY (You strictly adhere to these paths, or suggest logical sub-paths):
    {json.dumps(DEFAULT_TAXONOMY, indent=2)}
    
    INPUT PAPERS:
    {papers_text}
    
    OUTPUT JSON FORMAT (Strictly JSON):
    {{
        "0": {{
            "archive_path": "ğŸ“š Archive/...", 
            "idea_path": "ğŸ’¡ Idea Lab/..."
        }},
        "1": ...
    }}
    """
    
    try:
        response = client.models.generate_content(
            model=config.AI_MODEL,
            contents=prompt,
            config={'response_mime_type': 'application/json'}
        )
        return json.loads(response.text)
    except Exception as e:
        print(f"âŒ AI Error: {e}")
        return {}

def ensure_path_and_get_key(zot, path, cache):
    """ç¡®ä¿è·¯å¾„å­˜åœ¨å¹¶è¿”å›æœ€åä¸€çº§çš„ Key"""
    if not path or "Unclassified" in path: return None
    
    parts = [p.strip() for p in path.split('/') if p.strip()]
    parent_key = None
    
    for part in parts:
        # æ£€æŸ¥ç¼“å­˜
        current_key = cache.get(part)
        
        # å¦‚æœç¼“å­˜æ²¡æœ‰ï¼Œå°è¯•åˆ›å»º (æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†é€»è¾‘ï¼Œå‡è®¾åå­—å”¯ä¸€)
        if not current_key:
            if not DRY_RUN:
                print(f"      ğŸ”¨ åˆ›å»ºæ–°é›†åˆ: {part}")
                try:
                    payload = {'name': part}
                    if parent_key: payload['parentCollection'] = parent_key
                    res = zot.create_collections([payload])
                    if res and 'successful' in res:
                        current_key = list(res['successful'].values())[0]['key']
                        update_cache(part, current_key)
                        cache[part] = current_key # æ›´æ–°å†…å­˜ç¼“å­˜
                except Exception as e:
                    print(f"      âŒ åˆ›å»ºå¤±è´¥: {e}")
                    return None
            else:
                print(f"      [Dry Run] æ‹Ÿåˆ›å»ºé›†åˆ: {part}")
                current_key = "fake_" + part
        
        parent_key = current_key
        
    return parent_key

# ================= ä¸»ç¨‹åº =================

def main():
    print(f"ğŸš€ å¯åŠ¨åŒè½¨åˆ†ç±»å¼•æ“ (Dry Run: {DRY_RUN})...")
    
    # åˆå§‹åŒ–
    zot = zotero.Zotero(config.LIBRARY_ID, config.LIBRARY_TYPE, config.API_KEY)
    profile = load_profile()
    colls_cache = load_collection_cache(zot)
    
    # ç¡®å®šå¤„ç†èŒƒå›´
    target_key = None
    if TARGET_COLLECTION_PATH:
        print(f"ğŸ¯ ç›®æ ‡é›†åˆ: {TARGET_COLLECTION_PATH}")
        target_key = find_collection_by_path_simple(zot, TARGET_COLLECTION_PATH)
        if not target_key:
            print("âŒ æ— æ³•æ‰¾åˆ°ç›®æ ‡é›†åˆï¼Œè¯·æ£€æŸ¥ config.py æˆ–è·¯å¾„åç§°ã€‚")
            return
    
    # è·å–å¾…å¤„ç†æ–‡çŒ®
    print("ğŸ” æœç´¢å¾…å¤„ç†æ–‡çŒ® (tag: gemini_read)...")
    if target_key:
        items = zot.collection_items(target_key, tag='gemini_read', limit=50)
    else:
        items = zot.items(tag='gemini_read', limit=50)
        
    # é¢„å¤„ç†ï¼šè¿‡æ»¤å·²å¤„ç†çš„ï¼Œæå– Keywords
    todo_list = []
    for item in items:
        tags = [t['tag'] for t in item['data'].get('tags', [])]
        if AUTO_TAG_NAME in tags: continue
        
        kw = extract_tags_from_note(zot, item['key'])
        if kw:
            todo_list.append({
                'key': item['key'],
                'data': item['data'],
                'title': item['data'].get('title', 'Untitled'),
                'keywords': kw
            })
            
    print(f"âœ… æ‰¾åˆ° {len(todo_list)} ç¯‡å¾…åˆ†ç±»æ–‡çŒ®")
    if not todo_list: return

    # æ‰¹å¤„ç†
    batches = math.ceil(len(todo_list) / BATCH_SIZE)
    for i in range(batches):
        batch = todo_list[i*BATCH_SIZE : (i+1)*BATCH_SIZE]
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i+1}/{batches}...")
        
        # AI å†³ç­–
        decisions = ai_dual_classify(batch, profile)
        
        # æ‰§è¡Œç§»åŠ¨
        for idx_str, paths in decisions.items():
            try:
                idx = int(idx_str)
                if idx >= len(batch): continue
                
                paper = batch[idx]
                print(f"   ğŸ“„ {paper['title'][:30]}...")
                
                # è·å–è·¯å¾„
                p_archive = paths.get('archive_path')
                p_idea = paths.get('idea_path')
                
                keys_to_add = []
                
                # å¤„ç† Archive è·¯å¾„
                k1 = ensure_path_and_get_key(zot, p_archive, colls_cache)
                if k1 and not k1.startswith("fake"): keys_to_add.append(k1)
                
                # å¤„ç† Idea Lab è·¯å¾„
                k2 = ensure_path_and_get_key(zot, p_idea, colls_cache)
                if k2 and not k2.startswith("fake"): keys_to_add.append(k2)
                
                # æ‰§è¡Œ Zotero æ“ä½œ
                if not DRY_RUN:
                    for k in keys_to_add:
                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        current_colls = paper['data'].get('collections', [])
                        if k not in current_colls:
                            zot.add_to_collection(k, paper['data'])
                            print(f"      âœ… æ·»åŠ åˆ°: {colls_cache.get(k, k) if k in colls_cache else 'New Collection'}")
                    
                    # æ‰“æ ‡ç­¾
                    zot.add_tags(paper['data'], AUTO_TAG_NAME)
                else:
                    print(f"      [Dry Run] è®¡åˆ’å½’å…¥: \n        1. {p_archive}\n        2. {p_idea}")
                    
            except Exception as e:
                print(f"   âš ï¸ å•æ¡å¤„ç†å¤±è´¥: {e}")
        
        time.sleep(2)

    print("\nğŸ‰ å®Œæˆæ‰€æœ‰åˆ†ç±»ä»»åŠ¡ï¼")

if __name__ == "__main__":
    main()